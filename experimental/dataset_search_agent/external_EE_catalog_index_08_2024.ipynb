{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAVpp1fVd0zp"
      },
      "source": [
        "# Earth Engine Data Catalog LLM Experiment\n",
        "Author: Renee Johnston (reneejohnston@google.com), Eliot Cowan (eliotc@google.com)\n",
        "\n",
        "Forked from [work](https://colab.sandbox.google.com/drive/1Tvdquc7gFj_bR5WVRZBol-cdfKZliP6p?resourcekey=0-B93UAfDiEcw5kzGgJLF85g) by Johnny Yip (jcyip@google.com)\n",
        "\n",
        "### Summary\n",
        "This Colab notebook summarizes and creates embeddings for the Earth Engine data catalog to be better suited for a downstream search task.\n",
        "\n",
        "**Required Setup**\n",
        "\n",
        "You need to get a Generative AI API key [here](https://aistudio.google.com/app/prompts/new_chat). Be aware that you might need to pay\n",
        "for use of the Generative AI API.\n",
        "\n",
        "To save this key in the notebook, click on the key icon in Colab on the left-hand side and add your key as a secret with the name GOOGLE_API_KEY. Make sure the value has no newlines\n",
        "\n",
        "**Key steps**\n",
        "\n",
        "*   Authenticates the user and installs required libraries.\n",
        "*   Sets the GCP Project ID and location.\n",
        "*   Indexing\n",
        "\n",
        "*   Downloads the data catalog from GitHub.\n",
        "*   Parses JSONNET files to extract dataset details (ID, title, spatial/temporal extents).\n",
        "*   Summarizes dataset descriptions using an LLM (Gemini-1.5-pro).\n",
        "*   Stores the summarized information.\n",
        "\n",
        "**Intended use**\n",
        "\n",
        "The summarized documents can be embedded and used in a semantic search. Published alongside this notebook is an example.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MQJDgFB0jEH0"
      },
      "outputs": [],
      "source": [
        "#@title Authenticate\n",
        "\n",
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "ohPUPez8imvE"
      },
      "outputs": [],
      "source": [
        "#@title Install Python Libraries\n",
        "\n",
        "%%capture\n",
        "%%bash\n",
        "pip install google_cloud_aiplatform\n",
        "pip install langchain-community\n",
        "pip install langchain_google_genai\n",
        "pip install python-dateutil\n",
        "pip install langchain\n",
        "pip install GitPython\n",
        "pip install jsonnet\n",
        "pip install retry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKlgLtvCERNI"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD3kl8VzxyO7"
      },
      "outputs": [],
      "source": [
        "#@title Set GCP Project ID\n",
        "\n",
        "project_id = \"your_gcp_project\" #@param {type:\"string\"}\n",
        "location = \"us-central1\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX7M50m7yU_K"
      },
      "outputs": [],
      "source": [
        "#@title LLM and embeddings for use with Langchain\n",
        "\n",
        "from vertexai.preview.language_models import TextEmbeddingModel\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=project_id, location=location)\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "embedding_model = TextEmbeddingModel.from_pretrained(\"google/text-embedding-004\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WrDmkWCebUG"
      },
      "source": [
        "## Choose save location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_6YSzmGEwFy"
      },
      "outputs": [],
      "source": [
        "# Cached file paths\n",
        "persistent_path = '.'\n",
        "catalog_summary_path = f'{persistent_path}/catalog_summaries.jsonl'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNZILSSe4RFz"
      },
      "source": [
        "## Index the Earth Engine data catalog.\n",
        "\n",
        "**Note: you do not need to run this section unless you are building the index from scratch**\n",
        "\n",
        "The original idea was to load the webpages on https://developers.google.com/earth-engine/datasets but it was quite time consuming to parse out relevant text from the layout. Turns out the entire data catalog is available on [Github](https://github.com/google/earthengine-catalog/tree/main) as jsonnet format.\n",
        "\n",
        "TODO(b/363343474): Load STAC JSON files from gs://earthengine-stac instead. They are updated daily and expected to be more stable. They also have some light post-processing which may be helpful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMPrHVzFPOt5"
      },
      "source": [
        "### Fetch and parse the catalog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxL7OKJRMumx"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "rm -rf ./earthengine-catalog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LGZ3yXBejy0"
      },
      "source": [
        "Download the Earth Engine data catalog from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxatJqr0Lb-b"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import GitLoader\n",
        "\n",
        "loader = GitLoader(\n",
        "    clone_url=\"https://github.com/google/earthengine-catalog\",\n",
        "    repo_path=\"./earthengine-catalog\",\n",
        "    branch=\"main\"\n",
        ")\n",
        "\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPV9o7hZe0Is"
      },
      "source": [
        "Create methods to parse JSONNET files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9w77QYL4A0H"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import _jsonnet\n",
        "\n",
        "#  Returns content if worked, None if file not found, or throws an exception\n",
        "def try_path(dir, rel):\n",
        "  if not rel:\n",
        "    raise RuntimeError('Got invalid filename (empty string).')\n",
        "  if rel[0] == '/':\n",
        "    full_path = rel\n",
        "  elif os.path.isfile(dir + rel):\n",
        "    full_path = dir + rel\n",
        "  elif rel == 'terms_of_use.md':\n",
        "    # This one is super weird\n",
        "    full_path = './earthengine-catalog/catalog/LANDSAT/' + rel\n",
        "  else:\n",
        "    full_path = './earthengine-catalog/catalog/' + rel\n",
        "  if full_path[-1] == '/':\n",
        "    raise RuntimeError('Attempted to import a directory')\n",
        "\n",
        "  if not os.path.isfile(full_path):\n",
        "    return full_path, None\n",
        "  with open(full_path, 'rb') as f:\n",
        "    return full_path, f.read()\n",
        "\n",
        "def import_callback(directory, rel):\n",
        "  full_path, content = try_path(directory, rel)\n",
        "  if content:\n",
        "    return full_path, content\n",
        "  raise RuntimeError('File not found')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwdNsVGQ55TG"
      },
      "source": [
        "### Summarize the dataset description before generating the embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEKGRmSA4EV0"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size = 1000,\n",
        "  chunk_overlap  = 200,\n",
        "  length_function = len,\n",
        ")\n",
        "\n",
        "def summarize_text(text: str) -\u003e str:\n",
        "  # Remove newlines in description\n",
        "  text = re.sub('\\n\\s*', ' ', text)\n",
        "\n",
        "  docs = text_splitter.create_documents([text])\n",
        "\n",
        "  chain = load_summarize_chain(\n",
        "    llm,\n",
        "    chain_type=\"map_reduce\",\n",
        "    # map_prompt=prompt,\n",
        "    # combine_prompt=prompt\n",
        "  )\n",
        "  return chain.run(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XApc2QuA8MWe"
      },
      "source": [
        "Helper method to store generated summaries as JSONL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EfMiLG48L71"
      },
      "outputs": [],
      "source": [
        "from  langchain.schema import Document\n",
        "import json\n",
        "\n",
        "def save_docs_to_jsonl(docs, file_path):\n",
        "  with open(file_path, 'w') as jsonl_file:\n",
        "    for doc in docs:\n",
        "      jsonl_file.write(doc.json() + '\\n')\n",
        "\n",
        "def load_docs_from_jsonl(file_path):\n",
        "  docs = []\n",
        "  with open(file_path, 'r') as jsonl_file:\n",
        "    for line in jsonl_file:\n",
        "      data = json.loads(line)\n",
        "      obj = Document(**data)\n",
        "      docs.append(obj)\n",
        "  return docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENRrQaGUTkWY"
      },
      "source": [
        "Parse the JSONNET files and extract relevant informations. The current iteration of this code generates the embeddings for the dataset and band description, and puts the dataset title and ID as metadata. To make this more useful in the future, you can also pull in properties, date range etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOqSgzGVL2Fy"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from langchain.schema import Document\n",
        "import tqdm\n",
        "import datetime\n",
        "import dateutil\n",
        "\n",
        "import concurrent\n",
        "import retry\n",
        "\n",
        "iso_to_datetime = lambda date_str: dateutil.parser.isoparse(date_str) if date_str is not None else None\n",
        "\n",
        "@retry.retry(BaseException, tries=5, delay=1)  # TODO: Make this exception more specific.\n",
        "def _summarize_catalog_target(f):\n",
        "  if re.search(\"^catalog/*/.+\\.jsonnet\", f.metadata['file_path']) and f.metadata['file_name'] != ('catalog.jsonnet'):\n",
        "      json_str = _jsonnet.evaluate_file(\n",
        "        f\"./earthengine-catalog/{f.metadata['file_path']}\",\n",
        "        import_callback=import_callback,\n",
        "      )\n",
        "      json_dict = json.loads(json_str)\n",
        "      if 'deprecated' in json_dict and json_dict['deprecated']:\n",
        "        return\n",
        "      # Ignore community datasets\n",
        "      if json_dict['id'].startswith('projects/'):\n",
        "        return\n",
        "      try:\n",
        "        assert len(json_dict['extent']['temporal']['interval']) == 1, f\"Non-singular interval in temporal json: {json_dict['extent']['temporal']}\"\n",
        "        assert set(json_dict['extent']['spatial'].keys()) == {'bbox'}, f\"Non-bounding box keys in spatial json {json_dict['spatial']}\"\n",
        "        metadata = {\n",
        "          'file_path': f.metadata['file_path'],\n",
        "          'id': json_dict['id'],\n",
        "          'title': json_dict['title'],\n",
        "          'spatial': json_dict['extent']['spatial']['bbox'],\n",
        "          'temporal': [iso_to_datetime(datetime_string) for datetime_string in json_dict['extent']['temporal']['interval'][0]],\n",
        "        }\n",
        "      except Exception as e:\n",
        "        print(json_dict)\n",
        "        # print(e.with_traceback())\n",
        "        raise e\n",
        "      description = summarize_text(json_dict['description'])\n",
        "      # Format band descriptions and add them to the summary\n",
        "      band_string = \"\"\n",
        "      if 'summaries' in json_dict:\n",
        "        if 'eo:bands' in json_dict['summaries']:\n",
        "          for band in json_dict['summaries']['eo:bands']:\n",
        "            band_string += f'\"{band[\"name\"]}\" represents {band[\"description\"]}\\n'\n",
        "            if 'gee:classes' in band:\n",
        "              band_string += \"    Classes:\\n\"\n",
        "              for cls in band['gee:classes']:\n",
        "                band_string += f'    {cls[\"description\"]}\\n'\n",
        "      description = description + \"\\n\\n\" + band_string\n",
        "      doc = Document(\n",
        "          page_content=description,\n",
        "          metadata=metadata\n",
        "      )\n",
        "      return doc\n",
        "\n",
        "def summarize_catalog(threads=1, start_index=0):\n",
        "  global data\n",
        "  documents = []\n",
        "  with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as pool:\n",
        "    documents = list(tqdm.tqdm(pool.map(_summarize_catalog_target, data[start_index:]), total=len(data)-start_index))\n",
        "  return [doc for doc in documents if doc is not None]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK0Z2fuvKORi"
      },
      "source": [
        "Write summary texts to file, this process can take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0s4Otg-c7PkH"
      },
      "outputs": [],
      "source": [
        "if os.path.isfile(catalog_summary_path):\n",
        "  documents = load_docs_from_jsonl(catalog_summary_path)\n",
        "else:\n",
        "  # In theory this should be able to go higher than 8, but in practice we hit gemini 500 errors.\n",
        "  # Sometimes it works with 12.\n",
        "  documents = summarize_catalog(threads=8)\n",
        "  save_docs_to_jsonl(documents, catalog_summary_path)\n",
        "\n",
        "for doc in documents[:10]:\n",
        "  display(doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyzoC4WieCH2"
      },
      "source": [
        "# Save file\n",
        "You can instead download this file to place wherever is most helpful. The jsonl file can be used in any downstream application that requires the summarized catalog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrNdl9HBb5p7"
      },
      "outputs": [],
      "source": [
        "import google\n",
        "from google.cloud import storage\n",
        "\n",
        "gcp_location = \"gs://your_bucket/catalog_summaries.jsonl\" #@param {type:\"string\"}\n",
        "client = storage.Client(project=project_id)\n",
        "ee_client_bucket = google.cloud.storage.bucket.Bucket(client, name=gcp_location.split('/')[2],user_project=project_id)\n",
        "blob = ee_client_bucket.blob('/'.join(gcp_location.split('/')[3:]))\n",
        "blob.upload_from_filename('catalog_summaries.jsonl')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PKlgLtvCERNI",
        "4WrDmkWCebUG",
        "RMPrHVzFPOt5"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
