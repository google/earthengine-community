{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Functionsmith\n",
        "<a target='_blank' href='https://colab.research.google.com/github/google/earthengine-community/blob/master/experimental/functionsmith/functionsmith.ipynb'>   <img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/> </a>",
        "\n\n",
        "Functionsmith is a general-purpose problem-solving agent using *dynamic\n",
        "function calling*.\n",
        "\n",
        "**USING THIS AGENT IS UNSAFE**. It directly runs LLM-produced code, and thus\n",
        "should only be used for demonstration purposes. However, Colab serves as\n",
        "a moderately effective sandbox - the damage would be limited to whatever\n",
        "this notebook has access to.\n",
        "\n",
        "See [a sample session output](https://github.com/google/earthengine-community/blob/master/experimental/functionsmith/sample_session.txt).\n",
        "\n",
        "## Configuration\n",
        "\n",
        "To run with the default task investigating a CSV file with airport data,\n",
        "[obtain a Gemini API key](https://ai.google.dev/gemini-api/docs/api-key)\n",
        "and save it into a [Colab secret](https://colab.sandbox.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) named \"GOOGLE_API_KEY\".\n",
        "\n",
        "Run the notebook, then scroll to the end. You will see an empty text area\n",
        "and the task definition under it. Hit Enter in the task defintion input box\n",
        "to start processing.\n",
        "\n",
        "By default, the Gemini API is used. You can switch to Claude, ChatGPT,\n",
        "or DeepSeek APIs by uncommenting the corresponding LLM class\n",
        "in the last cell. You will also need to save ANTHROPIC_API_KEY,\n",
        "OPENAI_API_KEY, or DEEPSEEK_API_KEY secrets.\n",
        "\n",
        "## Approach\n",
        "\n",
        "This agent uses dynamic function calling, which means that instead of\n",
        "relying on a fixed set of tools predefined in the agent\n",
        "[in normal LLM function calling](https://ai.google.dev/gemini-api/docs/function-calling),\n",
        "we let the agent itself write with all the functions it needs.\n",
        "\n",
        "The functionsmith system prompt asks the agent to first write any low-level\n",
        "function it needs, as well as tests for them. The agent loop will try\n",
        "to run these functions and ask the LLM to make corrections if necessary.\n",
        "Once all the functions are ready, the agent will write and run the code\n",
        "to solve the actual user task.\n",
        "\n",
        "The agent does not use function calling features of LLM clients. Instead,\n",
        "it simply tries to parse all the 'python' or 'tool_use' sections\n",
        "present in the raw LLM output. It keeps all function definitions as well\n",
        "as their source code in memory. Each call to the LLM is preceded\n",
        "by the function definitions to let the LLM know what functions are available\n",
        "locally.\n",
        "\n",
        "The functions are not saved permanently, though this feature can be added.\n",
        "\n",
        "## Alternatives\n",
        "\n",
        "To run a (VERY UNSAFE) command-line version of this agent, run \n",
        "`pip install functionsmith`, then run `functionsmith_cli`.\n",
        "\n",
        "## Attribution\n",
        "\n",
        "Functionsmith was written by Simon Ilyushchenko (simonf@google.com).\n",
        "I am grateful to Renee Johnston and other Googlers for implementation advice,\n",
        "as well as to Earth Engine expert advisors Jeffrey Cardille, Erin Trochim,\n",
        "Morgan Crowley, and Samapriya Roy, who helped me choose the right training\n",
        "tasks."
      ],
      "metadata": {
        "id": "hqHymKLj5clL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install functionsmith"
      ],
      "metadata": {
        "id": "35c4WBO-KvoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/davidmegginson/ourairports-data/refs/heads/main/airports.csv"
      ],
      "metadata": {
        "id": "fWOseFwKORKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "nZ5OBZ5xypYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import copy\n",
        "import enum\n",
        "import inspect\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import time\n",
        "\n",
        "import google.colab\n",
        "import ipywidgets as widgets\n",
        "from google.colab import userdata\n",
        "from IPython.display import display, clear_output, HTML, Javascript\n",
        "\n",
        "from functionsmith import code_parser\n",
        "from functionsmith import executor\n",
        "from functionsmith import llm\n"
      ],
      "metadata": {
        "id": "OGciaLzqyqs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# System prompt"
      ],
      "metadata": {
        "id": "UteXTs9zyy82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_instruction=\"\"\"\n",
        "To solve the task given below, write first low-level python functions with\n",
        "tests for each of them in a ```python block. Include all the necessary imports.\n",
        "The tests should be as simple as possible and not rely on anything external.\n",
        "All asserts in tests should have an error message to make sure their failure is\n",
        "easy to detect. Do not check for __main__ - just write the top-level code\n",
        "directly in the output.\n",
        "\n",
        "In later responses, never omit parts of the code referring to earlier output -\n",
        "if you need to do this, define a function and then call it later.\n",
        "\n",
        "I will save the functions locally, and you can write higher-level code that\n",
        "will invoke them later. I will pass you the output from the code or any error\n",
        "messages.\n",
        "\n",
        "Call the task_done() function when you consider the task done.\n",
        "Ask the user questions if you need additional input.\n",
        "\n",
        "If I ask you to compute factorial of 10 and then prompt the user if they want\n",
        "more factorials computed, your responses should be like this (return one\n",
        "response at a time): Example chat session (each response should be returned in\n",
        "a separate answer):\n",
        "\n",
        "    Question 1:\n",
        "    Please compute the factorial of 10\n",
        "\n",
        "    Response 1:\n",
        "    Let's define the requested function and test it.\n",
        "    ```python\n",
        "    import math\n",
        "    def factorial(x):\n",
        "      return math.factorial(x)\n",
        "    def test_factorial():\n",
        "      assert factorial(3) == 6\n",
        "      assert factorial(4) == 24\n",
        "      print('success')\n",
        "\n",
        "    test_factorial()\n",
        "    ```\n",
        "\n",
        "    Question 2:\n",
        "    The code output was \"success\"\n",
        "\n",
        "    Response 2:\n",
        "\n",
        "    Now let's call the previously defined function to solve the user task.\n",
        "    ```python\n",
        "      print(factorial(10))\n",
        "    ```\n",
        "\n",
        "    Question 3:\n",
        "    The code output was \"3628800\"\n",
        "\n",
        "    Response 3:\n",
        "\n",
        "    The computed answer looks reasonable. Please enter a number if you want\n",
        "    another factorial to be computed, or instruct me to exit.\n",
        "\n",
        "    Question 4:\n",
        "    You can exit here\n",
        "\n",
        "    Response 4:\n",
        "\n",
        "    ```python\n",
        "      task_done('We can exit')\n",
        "    ```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5pw2YYOcy4GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task"
      ],
      "metadata": {
        "id": "8iUVE9nty8r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The header of the CSV file\n",
        "schema = \"\"\"\n",
        "\"id\",\"ident\",\"type\",\"name\",\"latitude_deg\",\"longitude_deg\",\"elevation_ft\",\"continent\",\"iso_country\",\"iso_region\",\"municipality\",\"scheduled_service\",\"gps_code\",\"iata_code\",\"local_code\",\"home_link\",\"wikipedia_link\",\"keywords\"\n",
        "\"\"\"\n",
        "\n",
        "task = f\"\"\"Please explore a local file airports.csv. First, make some \n",
        "hypotheses about the data, and then write code to test them to learn something \n",
        "interesting about the data. By 'interesting', I mean something you wouldn't \n",
        "have guessed from first principles - eg, finding that the largest countries \n",
        "have the most airports is not interesting. Explain why what you discovered \n",
        "seems interesting. When done, ask the user if they want to find out something \n",
        "else about this file. \n",
        "\n",
        "The file has the following schema: {schema}\"\"\"\n",
        "\n",
        "# If you are having problems with the above task, use this simple task defintion\n",
        "# for debugging the agent.\n",
        "if False:\n",
        "  task = \"\"\"\n",
        "  Compute the factorial of 20. When done, return a chat message asking the user\n",
        "  if they want to compute another factorial and compute it if they give you\n",
        "  a new value.\n",
        "  \"\"\"\n"
      ],
      "metadata": {
        "id": "090IIglOy8VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper classes for IO and code execution"
      ],
      "metadata": {
        "id": "R0jQbFJfzBy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The agent needs three helper classes:\n",
        "# * ColabIOManager knows how to interact with Colab\n",
        "# * Supervisor keeps track of agent state and helps it terminate\n",
        "# * CustomLoggingHandler captures logs from code parsing and execution\n",
        "\n",
        "STARS = '*' * 20 + '\\n'\n",
        "\n",
        "class IOState(enum.StrEnum):\n",
        "  THINKING = 'THINKING'\n",
        "  RUNNING_CODE = 'RUNNING_CODE'\n",
        "  WAITING_FOR_USER_INPUT = 'WAITING_FOR_USER_INPUT'\n",
        "  DONE = 'DONE'\n",
        "\n",
        "class IOManager:\n",
        "  \"\"\"Base class for I/O strategies.\"\"\"\n",
        "\n",
        "  def task_done(self, done_message: str='') -> None:\n",
        "    self.display(f\"Agent said: {done_message}\\n\\nTask Done!\")\n",
        "\n",
        "  def display(self, text: str) -> None:\n",
        "      raise NotImplementedError\n",
        "\n",
        "  def set_state(self, state: IOState):\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "class Supervisor:\n",
        "  \"\"\"Class responsible for controlling the agent.\"\"\"\n",
        "\n",
        "  # A public property indicating whether the agent is running\n",
        "  running: bool\n",
        "  _io_manager: IOManager\n",
        "\n",
        "  def __init__(self, io_manager):\n",
        "    self.running = False\n",
        "    self._io_manager = io_manager\n",
        "\n",
        "  def syscalls(self):\n",
        "    return [self.task_done]\n",
        "\n",
        "  def task_done(self, done_message: str='') -> None:\n",
        "    \"\"\"Signals the agent that the task is done to terminate execution.\"\"\"\n",
        "    self.running = False\n",
        "    self._io_manager.task_done(done_message)\n",
        "\n",
        "\n",
        "class ColabIOManager(IOManager):\n",
        "  \"\"\"I/O Manager for Google Colab execution.\n",
        "\n",
        "  This class exists to connect logging output from code parsing and execution\n",
        "  to the Colab UI.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self._user_input_handler = None\n",
        "\n",
        "    # Create output area with a unique ID\n",
        "    self._output_id = f\"output_{int(time.time())}\"\n",
        "    self._output_area = widgets.Output(\n",
        "        layout=widgets.Layout(\n",
        "            width='95%',\n",
        "            height='400px',\n",
        "            border='1px solid black',\n",
        "            overflow='auto'\n",
        "        )\n",
        "    )\n",
        "    # Add a unique CSS class to output area\n",
        "    self._output_area.add_class(self._output_id)\n",
        "\n",
        "    self._command_input = widgets.Text(\n",
        "        placeholder='Type your message and press Enter...',\n",
        "        description='‚ùì',\n",
        "        value = task,\n",
        "        layout=widgets.Layout(width='95%')\n",
        "    )\n",
        "\n",
        "    # Container for the UI elements\n",
        "    ui_container = widgets.VBox([\n",
        "        self._output_area,\n",
        "        self._command_input,\n",
        "    ], layout=widgets.Layout(width='100%'))\n",
        "\n",
        "    # Add CSS styling\n",
        "    display(HTML(\"\"\"\n",
        "    <style>\n",
        "    .widget-text input[type=\"text\"] {\n",
        "        width: 100% !important;\n",
        "        padding: 8px;\n",
        "        margin: 8px 0;\n",
        "        box-sizing: border-box;\n",
        "    }\n",
        "    .jupyter-widgets-output-area {\n",
        "        overflow-y: auto !important;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\"))\n",
        "\n",
        "    self._command_input.on_submit(self._on_command)\n",
        "    display(ui_container)\n",
        "\n",
        "  def _set_emoji(self, emoji):\n",
        "    self._command_input.description = emoji\n",
        "\n",
        "  def set_state(self, state: IOState):\n",
        "    match state:\n",
        "      case IOState.THINKING:\n",
        "        self._set_emoji('ü§î')\n",
        "      case IOState.RUNNING_CODE:\n",
        "        self._set_emoji('üåé')\n",
        "      case IOState.WAITING_FOR_USER_INPUT:\n",
        "        self._set_emoji('‚ùì')\n",
        "      case IOState.DONE:\n",
        "        self._set_emoji('‚úÖ ')\n",
        "      case _:\n",
        "        self._set_emoji('ü¶ô')\n",
        "\n",
        "  def _on_command(self, widget):\n",
        "    \"\"\"Accepts user input and passes it to the agent.\"\"\"\n",
        "    message = widget.value\n",
        "    if message.strip():\n",
        "      self.display(f\"> {message}\")\n",
        "      widget.value = ''\n",
        "\n",
        "      if self._user_input_handler:\n",
        "        self._user_input_handler(message)\n",
        "\n",
        "  def set_user_input_handler(self, handler):\n",
        "    \"\"\"Sets a handler function to be called when the user submits input.\"\"\"\n",
        "    self._user_input_handler = handler\n",
        "\n",
        "  def display(self, text: str) -> None:\n",
        "    with self._output_area:\n",
        "      display(HTML(f\"<p style='white-space: pre-wrap;'>{text}</p>\"))\n",
        "    self._scroll_to_bottom()\n",
        "\n",
        "  def _scroll_to_bottom(self):\n",
        "    js_code = f\"\"\"\n",
        "        requestAnimationFrame(() => {{\n",
        "            const element = document.querySelector('.{self._output_id}');\n",
        "            if (element) {{\n",
        "                element.scrollTop = element.scrollHeight;\n",
        "            }}\n",
        "        }});\n",
        "    \"\"\"\n",
        "    display(Javascript(js_code))\n",
        "\n",
        "\n",
        "\n",
        "class CustomLoggingHandler(logging.Handler):\n",
        "  \"\"\"Csustom logging handler that sends agent internal logs to the Colab UI.\"\"\"\n",
        "  _io_manager: IOManager\n",
        "\n",
        "  def __init__(self, io_manager):\n",
        "    super().__init__(logging.INFO)\n",
        "    self._io_manager = io_manager\n",
        "\n",
        "  def emit(self, record):\n",
        "    msg = self.format(record)\n",
        "    self._io_manager.display(msg)"
      ],
      "metadata": {
        "id": "MBgMPTNmzEce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab agent"
      ],
      "metadata": {
        "id": "MSLQ7O89zHBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the agent after this many turns to prevent runaway loops.\n",
        "MAX_TURNS = 100\n",
        "\n",
        "class Agent:\n",
        "  \"\"\"Main class for running the functionsmith agent.\"\"\"\n",
        "  _llm: llm.LLM\n",
        "  _num_turns: int\n",
        "\n",
        "  # The twp dictionaries below will contain functions that the LLM can call.\n",
        "  # The _syscalls dict has system functions - they are defined\n",
        "  # by the agent beforehand. Their output is not intercepted.\n",
        "  # The _functions dict will have functions dynamically created by the LLM.\n",
        "  _syscalls: dict[str, code_parser.Function]\n",
        "  _functions: dict[str, code_parser.Function]\n",
        "\n",
        "  _io_manager: IOManager\n",
        "  _supervisor: Supervisor\n",
        "  _code_parser: code_parser.Parser\n",
        "  _code_executor: executor.Executor\n",
        "\n",
        "  def __init__(self, io_manager: IOManager, llm_interface: llm.LLM):\n",
        "    self._io_manager = io_manager\n",
        "    io_manager.set_user_input_handler(self.handle_user_input)\n",
        "    self._supervisor = Supervisor(io_manager)\n",
        "\n",
        "    self._llm = llm_interface\n",
        "    self._syscalls = {}\n",
        "    self._functions = {}\n",
        "\n",
        "    logger = self._create_logger()\n",
        "    self._code_parser = code_parser.Parser(logger)\n",
        "    self._code_executor = executor.Executor(logger)\n",
        "\n",
        "    self._extract_syscalls()\n",
        "\n",
        "  def _create_logger(self):\n",
        "    logger = logging.getLogger('functionsmith')\n",
        "    logger.handlers = []\n",
        "    logger.addHandler(CustomLoggingHandler(self._io_manager))\n",
        "    logger.propagate = False\n",
        "    return logger\n",
        "\n",
        "  def _extract_syscalls(self):\n",
        "    \"\"\"Extracts system calls from the IO manager.\"\"\"\n",
        "    for method in self._supervisor.syscalls():\n",
        "      supervisor_syscalls = self._code_parser.extract_functions(\n",
        "          inspect.getsource(method))\n",
        "      self._syscalls.update(supervisor_syscalls.functions)\n",
        "\n",
        "  def _function_signatures(self) -> str:\n",
        "    all_functions = copy.deepcopy(self._functions)\n",
        "    all_functions.update(self._syscalls)\n",
        "    # We tell the LLM about the signatures and docstrings of all the functions\n",
        "    # available so far, either predefined in the agent as syscalls or defined\n",
        "    # dynamically during the earlier turns.\n",
        "    return (\n",
        "        'The following functions are available:\\n' +\n",
        "        '\\n'.join([x.signature() for x in all_functions.values()]))\n",
        "\n",
        "  def _get_llm_response(self, question: str) -> code_parser.ParsedResponse:\n",
        "    self._io_manager.set_state(IOState.THINKING)\n",
        "    question_with_tools = question + self._function_signatures()\n",
        "    response = self._llm.chat(question_with_tools)\n",
        "    self._io_manager.display(f\"Agent: {response}\")\n",
        "    return self._code_parser.extract_functions(response)\n",
        "\n",
        "  def _handle_no_code_response(self):\n",
        "      \"\"\"Handles the case where the LLM response has no code.\"\"\"\n",
        "      self._io_manager.set_state(IOState.WAITING_FOR_USER_INPUT)\n",
        "      self._supervisor.running = False\n",
        "\n",
        "  def _execute_code(self, code: str) -> str:\n",
        "    # We add the code for all the functions defined so far.\n",
        "    # Only non-syscall source code is used, as we intercept syscalls\n",
        "    # in execution.\n",
        "    code_with_tools = (\n",
        "        '\\n'.join([x.code for x in self._functions.values()]) +\n",
        "        '\\n' + code\n",
        "    )\n",
        "\n",
        "    sandbox_env = {\n",
        "      'task_done': self._supervisor.task_done,\n",
        "    }\n",
        "    self._io_manager.set_state(IOState.RUNNING_CODE)\n",
        "    return self._code_executor.run_code(code_with_tools, sandbox_env)\n",
        "\n",
        "  def handle_user_input(self, user_input):\n",
        "    question = user_input\n",
        "    self._supervisor.running = True\n",
        "\n",
        "    # To respond to user input, we run an infinite loop until one of these\n",
        "    # things happens:\n",
        "    # 1. The agent returns a response without any code, which probably means\n",
        "    #    it's asking the user something.\n",
        "    # 2. The agent is no longer running, which probably means it thinks\n",
        "    #    the task is done.\n",
        "    self._num_turns = 0\n",
        "    while self._supervisor.running:\n",
        "      self._io_manager.display(STARS)\n",
        "      self._num_turns += 1\n",
        "      if self._num_turns > MAX_TURNS:\n",
        "        self._io_manager.display(f'REACHED {MAX_TURNS} TURNS, TERMINATING')\n",
        "        return\n",
        "\n",
        "      parsed_response = self._get_llm_response(question)\n",
        "\n",
        "      if not parsed_response.code and not parsed_response.functions:\n",
        "        if parsed_response.error:\n",
        "          # We couldn't parse the LLM-produced code, so we send the parsing\n",
        "          # error to the LLM.\n",
        "          question = parsed_response.error\n",
        "          continue\n",
        "\n",
        "        # The answer has no functions or top-level code.\n",
        "        # This means the task is not done - the LLM needs user input.\n",
        "        # We return control to the user.\n",
        "        self._handle_no_code_response()\n",
        "        return\n",
        "\n",
        "      # If we are here, the response has top-level code, functions, or both.\n",
        "      self._functions.update(parsed_response.functions)\n",
        "\n",
        "      if not parsed_response.code:\n",
        "        # The agent only defined functions, but gave no top-level code.\n",
        "        question = 'go on'\n",
        "        continue\n",
        "\n",
        "      # The code execution output is saved into 'question', as it will be\n",
        "      # sent to the LLM as the next user turn.\n",
        "      question = self._execute_code(parsed_response.code)\n",
        "    # End of while loop\n",
        "\n",
        "    self._io_manager.display(STARS)\n",
        "    self._io_manager.set_state(IOState.DONE)\n"
      ],
      "metadata": {
        "id": "NIugaq2CW6Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start the agent"
      ],
      "metadata": {
        "id": "JQ4IvHAdzJ9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_interface = llm.Gemini(system_instruction, api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "#llm_interface= llm.Claude(system_instruction, api_key=userdata.get('ANTHROPIC_API_KEY'))\n",
        "#llm_interface = llm.ChatGPT(system_instruction, api_key=userdata.get('OPENAI_API_KEY'))\n",
        "#llm_interface = llm.DeepSeek(system_instruction, api_key=userdata.get('DEEPSEEK_API_KEY'))\n",
        "\n",
        "io_manager = ColabIOManager()\n",
        "agent = Agent(io_manager, llm_interface)\n",
        "\n",
        "print(\"\"\"\n",
        "Legend:\n",
        "‚ùì = Waiting for user input\n",
        "ü§î = Thinking\n",
        "üåé = Running code\n",
        "‚úÖ = The task is done\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "EVLJ1sQwXCG8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
