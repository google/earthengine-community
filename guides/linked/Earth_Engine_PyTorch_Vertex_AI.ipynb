{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSIfBsgi8dNK"
      },
      "source": [
        "#@title Copyright 2023 Google LLC. { display-mode: \"form\" }\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV1xZ1CPi3Nw"
      },
      "source": [
        "<table class=\"ee-notebook-buttons\" align=\"left\"><td>\n",
        "<a target=\"_blank\"  href=\"http://colab.research.google.com/github/google/earthengine-community/blob/master/guides/linked/Earth_Engine_PyTorch_Vertex_AI.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a>\n",
        "</td><td>\n",
        "<a target=\"_blank\"  href=\"https://github.com/google/earthengine-community/blob/master/guides/linked/Earth_Engine_PyTorch_Vertex_AI.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /> View source on GitHub</a></td></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC8adBmw-5m3"
      },
      "source": [
        "# Connecting Earth Engine to a Vertex AI hosted PyTorch model\n",
        "\n",
        "This notebook demonstrates training a per-pixel neural network in PyTorch, hosting the model on Vertex AI, and using it in Earth Engine for interactive prediction using `ee.Model.fromVertexAi` with the `'ND_ARRAYS'` payloadFormat parameter.\n",
        "\n",
        "**Running this demo may incur charges to your Google Cloud Account!**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ],
      "metadata": {
        "id": "xICc7SyIbXav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import ee\n",
        "\n",
        "import folium\n",
        "import google\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "E-nnPe9x8BBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate the notebook.\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "K1VPnt9xhpbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REPLACE WITH YOUR CLOUD PROJECT!\n",
        "PROJECT = 'your-project'"
      ],
      "metadata": {
        "id": "-2eaoMwBegMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project {PROJECT}"
      ],
      "metadata": {
        "id": "O9oagdxYoYw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrXLkJC2QJdP"
      },
      "source": [
        "## Define variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHTOc5YLQZ5B"
      },
      "source": [
        "\"\"\" Training variables\"\"\"\n",
        "\n",
        "# Cloud Storage bucket with training and testing datasets.\n",
        "DATA_BUCKET = 'ee-docs-demos'\n",
        "TRAIN_FILE_PATH = 'gs://' + DATA_BUCKET + '/Pytorch_training_demo.csv'\n",
        "TEST_FILE_PATH = 'gs://' + DATA_BUCKET + '/Pytorch_testing_demo.csv'\n",
        "\n",
        "# Training parameters.\n",
        "BANDS = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        "INPUT_TILE_X_DIM = 1\n",
        "INPUT_TILE_Y_DIM = 1\n",
        "BATCH_SIZE = 4\n",
        "OUTPUT_CLASS = 'landcover'\n",
        "\n",
        "\n",
        "\"\"\" Model deployment variables\"\"\"\n",
        "\n",
        "# Output bucket for trained models. REPLACE WITH YOUR WRITABLE BUCKET!\n",
        "OUTPUT_BUCKET = 'gs://your-bucket'\n",
        "\n",
        "# Metadata for model deployment\n",
        "REGION = 'us-central1'\n",
        "MODEL_NAME = 'vertex_pytorch_demo'\n",
        "MODEL_DIR = OUTPUT_BUCKET + '/' + MODEL_NAME\n",
        "CONTAINER_IMAGE = 'us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-12:latest'\n",
        "ENDPOINT_NAME = 'vertex_pytorch_demo_endpoint'\n",
        "\n",
        "\n",
        "\"\"\" Earth Engine visualization variables\"\"\"\n",
        "\n",
        "ATTRIBUTION = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare training data\n",
        "\n",
        "The training data are landcover labels with a single vector of Landsat 8 pixel values (BANDS) as predictors. See [this Code Editor script](https://code.earthengine.google.com/a96ddba52131951f5613c88a0ceb8a96) for an example on how to generate this training data."
      ],
      "metadata": {
        "id": "vWfUHgKOZj8Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sosRFEDdOMA"
      },
      "source": [
        "## Read into tensors\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install additional package\n",
        "!pip install gcsfs"
      ],
      "metadata": {
        "id": "AJXDdOP_gFiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the training data into a Pandas dataframe.\n",
        "df_train = pd.read_csv(TRAIN_FILE_PATH)\n",
        "\n",
        "# Split into features and labels.\n",
        "features = df_train[BANDS].values\n",
        "target = df_train[OUTPUT_CLASS].values\n",
        "\n",
        "# Convert to PyTorch tensors.\n",
        "features = torch.tensor(features, dtype=torch.float32)\n",
        "target = torch.tensor(target, dtype=torch.long)"
      ],
      "metadata": {
        "id": "LNXnDV6CFcmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reshape tensors\n",
        "\n",
        " Once we have the data as tensors, we need to reshape the features and target to have the shape that our PyTorch model will expect as input."
      ],
      "metadata": {
        "id": "vMEw2yLjsJ2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_features = torch.reshape(features, (-1, len(BANDS), INPUT_TILE_X_DIM, INPUT_TILE_Y_DIM))\n",
        "print(reshaped_features.shape)\n",
        "\n",
        "reshaped_targets = torch.reshape(target, (-1, INPUT_TILE_X_DIM, INPUT_TILE_Y_DIM))\n",
        "print(reshaped_targets.shape)"
      ],
      "metadata": {
        "id": "ZeSVhHCJsJK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load tensors into a `DataLoader`\n",
        "\n",
        "Finally, we load the tensors into a PyTorch DataLoader, which makes it easier to batch and shuffle the data for training."
      ],
      "metadata": {
        "id": "Eof6eoPqskfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(reshaped_features, reshaped_targets)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "Cg1a3K0MsFaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model setup\n",
        "\n",
        "Now we will define and train a simple landcover classification model with 2 convolutional layers. Note that the model used here is purely for demonstration purposes."
      ],
      "metadata": {
        "id": "bt6lbwTr2ioE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the model"
      ],
      "metadata": {
        "id": "kIRwu6EN3Uxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=len(BANDS), out_channels=15, kernel_size=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=15, out_channels=3, kernel_size=1)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model\n",
        "model = ClassificationModel()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "djuL9DdBnqd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the training data.\n",
        "num_epochs = 13\n",
        "for epoch in range(num_epochs):\n",
        "  for inputs, targets in train_loader:\n",
        "    # Forward pass\n",
        "    output = model(inputs)\n",
        "    loss = criterion(output, targets)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # Print the loss every other epoch\n",
        "  if (epoch) % 2 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "sNRaD75jUelA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the model"
      ],
      "metadata": {
        "id": "eBKjqoXy22Ng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained! Now let's test how well our model is able to classify our target landcover classes. When we prepared the training data, we reserved 30% of it for testing. Let's read that testing data from Cloud Storage now and prepare it for our model."
      ],
      "metadata": {
        "id": "QUVeJrIzZa1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(TEST_FILE_PATH)\n",
        "\n",
        "testing_features = torch.tensor(df_test[BANDS].values, dtype=torch.float32)\n",
        "testing_target = torch.tensor(df_test[OUTPUT_CLASS].values, dtype=torch.long)\n",
        "\n",
        "reshaped_testing_features = torch.reshape(testing_features, (-1, len(BANDS), INPUT_TILE_X_DIM, INPUT_TILE_Y_DIM))\n",
        "reshaped_testing_target = torch.reshape(testing_target, (-1, INPUT_TILE_X_DIM, INPUT_TILE_Y_DIM))"
      ],
      "metadata": {
        "id": "0mrWhzNlo1fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can pass the testing features to our model and compare the returned predictions with the ground truth targets to get an accuracy percentage."
      ],
      "metadata": {
        "id": "op7SLZFQZhlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_predictions = model(reshaped_testing_features)\n",
        "\n",
        "testing_loss = criterion(test_predictions, reshaped_testing_target)\n",
        "\n",
        "print(\"Loss is:\", testing_loss)\n",
        "\n",
        "test_predicted_labels = torch.argmax(test_predictions, dim=1)\n",
        "accuracy = (test_predicted_labels == reshaped_testing_target).float().mean()\n",
        "print(\"Accuracy is:\", accuracy)"
      ],
      "metadata": {
        "id": "f8btFdrlpKSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model artifacts\n",
        "\n",
        "Once we're happy with our model architecture and accuracy, we can move on to preparing all of the model artifacts we'll need to deploy it on Vertex AI."
      ],
      "metadata": {
        "id": "eKTfrAfSdVou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model's script module"
      ],
      "metadata": {
        "id": "JkJauSEtgBM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pytorch_model\n",
        "script_module = torch.jit.script(model)\n",
        "script_module.save('pytorch_model/script_module.pt')"
      ],
      "metadata": {
        "id": "icSfDutcKkA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write a custom handler"
      ],
      "metadata": {
        "id": "CNby4yV0f_EW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to specify a handler for our model. We could  use a Torchserve default handler or write our own custom one. Here, we'll define a custom handler since our model returns the predictions as a tensor of probabilities, but we'll just look at the highest probability class value."
      ],
      "metadata": {
        "id": "STWSevy7gJga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pytorch_model/custom_handler.py\n",
        "\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "from ts.torch_handler.base_handler import BaseHandler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ClassifierHandler(BaseHandler):\n",
        "\n",
        "  def postprocess(self, data):\n",
        "    # Data comes in as a pytorch tensor of probabilities, so use argmax to\n",
        "    # select the class with the highest probability.\n",
        "    predictions = torch.argmax(data, dim=1).float()\n",
        "\n",
        "    # Return the data as a list.\n",
        "    return predictions.tolist()\n",
        "\n",
        "  def handle(self, data, context):\n",
        "    self.context = context\n",
        "\n",
        "    input_tensor = self.preprocess(data)\n",
        "    pred_out = self.inference(input_tensor)\n",
        "    return self.postprocess(pred_out)"
      ],
      "metadata": {
        "id": "KpK6rHs5E8EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save a model archive"
      ],
      "metadata": {
        "id": "eVXPnJ6IBwOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-model-archiver"
      ],
      "metadata": {
        "id": "tAXOY3ZSgmdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!torch-model-archiver -f \\\n",
        "  --model-name model \\\n",
        "  --version 1.0 \\\n",
        "  --serialized-file 'pytorch_model/script_module.pt' \\\n",
        "  --handler 'pytorch_model/custom_handler.py' \\\n",
        "  --export-path pytorch_model/"
      ],
      "metadata": {
        "id": "OSujqQj3R7te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copy files to GCS"
      ],
      "metadata": {
        "id": "RtYE43kTYQTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the bucket if it doesn't exist\n",
        "!gcloud storage buckets create {OUTPUT_BUCKET}"
      ],
      "metadata": {
        "id": "t30DZcgwLLkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp -r pytorch_model {MODEL_DIR}"
      ],
      "metadata": {
        "id": "7-nd0xA8YQTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy to Vertex AI"
      ],
      "metadata": {
        "id": "msy_hCJHd89F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload the model\n",
        "\n",
        "Add an entry to the model registry that points to the location of the saved model and a container image needed to run the model. See [Vertex Docs](https://cloud.google.com/sdk/gcloud/reference/ai/models/upload) for more info."
      ],
      "metadata": {
        "id": "SWSlwmYwVtGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud ai models upload \\\n",
        "  --artifact-uri={MODEL_DIR} \\\n",
        "  --project={PROJECT} \\\n",
        "  --region={REGION} \\\n",
        "  --container-image-uri={CONTAINER_IMAGE} \\\n",
        "  --description={MODEL_NAME} \\\n",
        "  --display-name={MODEL_NAME} \\\n",
        "  --model-id={MODEL_NAME}"
      ],
      "metadata": {
        "id": "ypOvtOV2GRAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an endpoint\n",
        "\n",
        "Create an endpoint from which to serve the model. See [Vertex Docs](https://cloud.google.com/sdk/gcloud/reference/ai/endpoints/create) for more info."
      ],
      "metadata": {
        "id": "UX9KbR5ufzuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud ai endpoints create \\\n",
        "  --display-name={ENDPOINT_NAME} \\\n",
        "  --region={REGION} \\\n",
        "  --project={PROJECT}"
      ],
      "metadata": {
        "id": "_UAEo1sziinc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploy the model to the endpoint\n",
        "\n",
        "First, look up the endpoint ID, then deploy the model. See [Vertex Docs](https://cloud.google.com/sdk/gcloud/reference/ai/endpoints/deploy-model) for more info."
      ],
      "metadata": {
        "id": "e7uzlQkOf5dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ENDPOINT_ID = !gcloud ai endpoints list \\\n",
        "  --project={PROJECT} \\\n",
        "  --region={REGION} \\\n",
        "  --filter=displayName:{ENDPOINT_NAME} \\\n",
        "  --format=\"value(ENDPOINT_ID.scope())\"\n",
        "ENDPOINT_ID = ENDPOINT_ID[-1]"
      ],
      "metadata": {
        "id": "iGFh6GaDgQji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud ai endpoints deploy-model {ENDPOINT_ID} \\\n",
        "  --project={PROJECT} \\\n",
        "  --region={REGION} \\\n",
        "  --model={MODEL_NAME} \\\n",
        "  --display-name={MODEL_NAME}"
      ],
      "metadata": {
        "id": "oQT0ECBh2Bjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aTGza-rWIjp"
      },
      "source": [
        "# Connect to the hosted model from Earth Engine\n",
        "\n",
        "1. Generate the input imagery.  This should be done in exactly the same way as the training data were generated.  See [this example notebook](http://colab.research.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/TF_demo1_keras.ipynb) for details.\n",
        "2. Connect to the hosted model.\n",
        "3. Use the model to make predictions.\n",
        "4. Display the results.\n",
        "\n",
        "Note that it may take the model a couple minutes to spin up before it is ready to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "credentials, _ = google.auth.default()\n",
        "ee.Initialize(credentials, project=PROJECT, opt_url='https://earthengine-highvolume.googleapis.com')"
      ],
      "metadata": {
        "id": "W52vJtSUtCJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2OsyrJ7HAhE"
      },
      "source": [
        "# The image input data is a 2018 cloud-masked median composite.\n",
        "landsatCollection = ee.ImageCollection('LANDSAT/LC08/C02/T1').filterDate('2018-01-01', '2018-12-31')\n",
        "\n",
        "composite = ee.Algorithms.Landsat.simpleComposite(\n",
        "  collection=landsatCollection,\n",
        "  asFloat=True\n",
        ");\n",
        "\n",
        "# Get a URL to serve image tiles.\n",
        "mapid = composite.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
        "\n",
        "# Use folium to visualize the imagery.\n",
        "map = folium.Map(location=[38., -122.5])\n",
        "\n",
        "# Inputs.\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr=ATTRIBUTION,\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "\n",
        "endpoint_path = (\n",
        "    'projects/' + PROJECT + '/locations/' + REGION + '/endpoints/' + str(ENDPOINT_ID))\n",
        "\n",
        "# Connect to the hosted model.\n",
        "vertex_model = ee.Model.fromVertexAi(\n",
        "  endpoint=endpoint_path,\n",
        "  inputTileSize=[50, 50],\n",
        "  proj=ee.Projection('EPSG:4326').atScale(30),\n",
        "  fixInputProj=True,\n",
        "  outputBands={'output': {\n",
        "      'type': ee.PixelType.float(),\n",
        "      'dimensions': 0\n",
        "    }},\n",
        "  payloadFormat='ND_ARRAYS'\n",
        "  )\n",
        "\n",
        "predictions = vertex_model.predictImage(composite.select(BANDS))\n",
        "# probabilities = predictions.arrayFlatten([['bare', 'veg', 'water']])\n",
        "predictions_vis = {'min': 0, 'max': 2, 'palette': ['red', 'green', 'blue']}\n",
        "predictions_mapid = predictions.getMapId(predictions_vis)\n",
        "folium.TileLayer(\n",
        "    tiles=predictions_mapid['tile_fetcher'].url_format,\n",
        "    attr=ATTRIBUTION,\n",
        "    overlay=True,\n",
        "    name='predictions',\n",
        "  ).add_to(map)\n",
        "\n",
        "map.add_child(folium.LayerControl())\n",
        "display(map)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}