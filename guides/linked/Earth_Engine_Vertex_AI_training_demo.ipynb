{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSIfBsgi8dNK"
      },
      "source": [
        "#@title Copyright 2023 Google LLC. { display-mode: \"form\" }\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV1xZ1CPi3Nw"
      },
      "source": [
        "\u003ctable class=\"ee-notebook-buttons\" align=\"left\"\u003e\u003ctd\u003e\n",
        "\u003ca target=\"_blank\"  href=\"http://colab.research.google.com/github/google/earthengine-community/blob/master/guides/linked/Earth_Engine_Vertex_AI_training_demo.ipynb\"\u003e\n",
        "    \u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003e Run in Google Colab\u003c/a\u003e\n",
        "\u003c/td\u003e\u003ctd\u003e\n",
        "\u003ca target=\"_blank\"  href=\"https://github.com/google/earthengine-community/blob/master/guides/linked/Earth_Engine_Vertex_AI_training_demo.ipynb\"\u003e\u003cimg width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003e View source on GitHub\u003c/a\u003e\u003c/td\u003e\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC8adBmw-5m3"
      },
      "source": [
        "# Training a model in Vertex AI\n",
        "\n",
        "This notebook demonstrates training a convolutional neural network on Vertex AI.  The trained model is suitable for use in Earth Engine with `ee.Model.fromVertexAIPredictor`.  The model is a simple convolutional model of land\n",
        "cover.  The training data generation and model setup are described in detail in the [2022 Geo for Good Deep Learning session](https://earthoutreachonair.withgoogle.com/events/geoforgood22?talk=day1-trackthree-talk2).\n",
        "\n",
        "**Running this demo may incur charges to your Google Cloud Account!**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs and imports"
      ],
      "metadata": {
        "id": "mT2E1n_b6L8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U google-cloud-aiplatform"
      ],
      "metadata": {
        "id": "bprWMrF9so4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "from google.colab import auth\n",
        "\n",
        "import ee\n",
        "import folium\n",
        "import google\n",
        "import keras\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "E-nnPe9x8BBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "TLmI05-wT_GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT = 'your-project'"
      ],
      "metadata": {
        "id": "CnImUhvg_Grp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credentials, _ = google.auth.default()\n",
        "ee.Initialize(credentials, project=PROJECT, opt_url='https://earthengine-highvolume.googleapis.com')"
      ],
      "metadata": {
        "id": "zKKbmu0gYu9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable declarations"
      ],
      "metadata": {
        "id": "lAN_jCM8-YA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REGION = 'us-central1'\n",
        "# Trained model output locations.  REPLACE WITH YOUR BUCKET!\n",
        "OUTPUT_DIR = 'your-bucket'\n",
        "EEIFIED_DIR = 'your-bucket'\n",
        "\n",
        "# Name of package containing training code.\n",
        "PACKAGE_PATH = 'demo_model'\n",
        "# Name of the hosted model and endpoint.\n",
        "MODEL_NAME = 'demo_lc_model'\n",
        "ENDPOINT_NAME = 'demo_lc_endpoint'\n",
        "# A container image that can run the hosted model.\n",
        "CONTAINER_IMAGE = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest'\n",
        "\n",
        "# The time range over which to compute composites.\n",
        "START = '2020-1-1'\n",
        "END = '2021-1-1'\n",
        "\n",
        "# A random spot near Ho Chi Minh City, Vietnam.\n",
        "COORDS = [105.695, 9.883]\n",
        "\n",
        "# Sentinel-2 bands to be used in prediction.\n",
        "BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7',\n",
        "          'B8', 'B8A', 'B9', 'B11', 'B12']\n",
        "CLASSIFICATIONS = {\n",
        "    \"Water\":              \"419BDF\",\n",
        "    \"Trees\":              \"397D49\",\n",
        "    \"Grass\":              \"88B053\",\n",
        "    \"Flooded vegetation\": \"7A87C6\",\n",
        "    \"Crops\":              \"E49635\",\n",
        "    \"Shrub and scrub\":    \"DFC35A\",\n",
        "    \"Built-up areas\":     \"C4281B\",\n",
        "    \"Bare ground\":        \"A59B8F\",\n",
        "    \"Snow and ice\":       \"B39FE1\",\n",
        "}\n",
        "\n",
        "ATTRIBUTION = 'Map Data \u0026copy; \u003ca href=\"https://earthengine.google.com/\"\u003eGoogle Earth Engine\u003c/a\u003e'"
      ],
      "metadata": {
        "id": "eJFWDjsW-b1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a training package\n",
        "\n",
        "Make a directory to hold the training code.  This script will be uploaded to Vertex AI in order to start the training job."
      ],
      "metadata": {
        "id": "KvYDJ2OH5Ijh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtvd2Kspl64P"
      },
      "outputs": [],
      "source": [
        "!rm -rf {PACKAGE_PATH}\n",
        "!mkdir {PACKAGE_PATH}\n",
        "!ls -l"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a self-contained file that will load datasets, train and save the model."
      ],
      "metadata": {
        "id": "UTlhPujjDVKD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhc1ToBT7-hd"
      },
      "outputs": [],
      "source": [
        "%%writefile {PACKAGE_PATH}/task.py\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "\n",
        "\n",
        "OUTPUT_DIR = 'your-bucket'\n",
        "\n",
        "# Put gs://ee-docs-demos/g4g-tf-demos/tiles_2022/*.gz into a us-central1 bucket.\n",
        "TRAINING_PATTERN = 'your-bucket/training*.tfrecord.gz'\n",
        "VALIDATION_PATTERN = 'your-bucket/validation*.tfrecord.gz'\n",
        "\n",
        "PATCH_SIZE = 128  # Pixels\n",
        "SCALE = 10        # Meters per pixel\n",
        "# Predictors.\n",
        "BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7',\n",
        "          'B8', 'B8A', 'B9', 'B11', 'B12']\n",
        "# Target variable.\n",
        "LABEL = 'landcover'\n",
        "CLASSIFICATIONS = {\n",
        "    \"Water\":              \"419BDF\",\n",
        "    \"Trees\":              \"397D49\",\n",
        "    \"Grass\":              \"88B053\",\n",
        "    \"Flooded vegetation\": \"7A87C6\",\n",
        "    \"Crops\":              \"E49635\",\n",
        "    \"Shrub and scrub\":    \"DFC35A\",\n",
        "    \"Built-up areas\":     \"C4281B\",\n",
        "    \"Bare ground\":        \"A59B8F\",\n",
        "    \"Snow and ice\":       \"B39FE1\",\n",
        "}\n",
        "# Input stack.\n",
        "FEATURE_NAMES = BANDS + [LABEL]\n",
        "\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=[PATCH_SIZE, PATCH_SIZE], dtype=tf.float32)\n",
        "  for k in FEATURE_NAMES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURE_NAMES, COLUMNS))\n",
        "\n",
        "\n",
        "class Augment(tf.keras.layers.Layer):\n",
        "  def __init__(self, seed=42):\n",
        "    super().__init__()\n",
        "    # both use the same seed, so they'll make the same random changes.\n",
        "    self.augment_inputs = {\n",
        "      b: tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=seed)\n",
        "      for b in BANDS}\n",
        "    self.augment_labels = tf.keras.layers.RandomFlip(\n",
        "        mode=\"horizontal_and_vertical\", seed=seed)\n",
        "\n",
        "  def call(self, inputs, labels):\n",
        "    inputs = {b: self.augment_inputs[b](inputs[b]) for b in BANDS}\n",
        "    labels = self.augment_labels(labels)\n",
        "    return inputs, labels\n",
        "\n",
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"Deserialize an example proto.  Return (inputs, labels)\"\"\"\n",
        "  parsed_features = tf.io.parse_example(example_proto, FEATURES_DICT)\n",
        "  labels = parsed_features.pop(LABEL)\n",
        "  return ({k: tf.expand_dims(v, axis=2) for k, v in parsed_features.items()},\n",
        "          tf.one_hot(tf.cast(labels, tf.uint8), len(CLASSIFICATIONS)))\n",
        "\n",
        "\n",
        "def get_dataset(pattern):\n",
        "  dataset = tf.data.Dataset.list_files(pattern).interleave(\n",
        "      lambda filename: tf.data.TFRecordDataset(filename, compression_type='GZIP'))\n",
        "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  # dataset = dataset.map(to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  dataset = dataset.cache()\n",
        "  dataset = dataset.shuffle(1024)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def get_model(input_depth, num_classes):\n",
        "    inputs = keras.Input(shape=[None, None, input_depth], name='array')\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    for filters in [64, 128]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [128, 64, 32]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "# A Layer to stack and reshape the input tensors.\n",
        "class MyPreprocessing(keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyPreprocessing, self).__init__(**kwargs)\n",
        "\n",
        "  def call(self, features_dict):\n",
        "    # (None, H, W, 1) -\u003e (None, H, W, P)\n",
        "    return tf.concat([features_dict[b] for b in BANDS], axis=3)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    return config\n",
        "\n",
        "\n",
        "# A Model that wraps the base model with the preprocessing layer.\n",
        "class MyModel(keras.Model):\n",
        "  def __init__(self, preprocessing, backbone, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.preprocessing = preprocessing\n",
        "    self.backbone = backbone\n",
        "\n",
        "  def call(self, features_dict):\n",
        "    x = self.preprocessing(features_dict)\n",
        "    return self.backbone(x)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    return config\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  training_dataset = get_dataset(TRAINING_PATTERN).map(\n",
        "    Augment(), num_parallel_calls=tf.data.AUTOTUNE).batch(32)\n",
        "  validation_dataset = get_dataset(VALIDATION_PATTERN).batch(1)\n",
        "\n",
        "  foo, bar = iter(training_dataset).next()\n",
        "\n",
        "  model = get_model(len(BANDS), len(CLASSIFICATIONS))\n",
        "  m = MyModel(MyPreprocessing(), model)\n",
        "\n",
        "  m.compile(\n",
        "      optimizer='adam',\n",
        "      loss='categorical_crossentropy',\n",
        "      metrics=[\n",
        "          tf.keras.metrics.OneHotIoU(\n",
        "              num_classes=len(CLASSIFICATIONS),\n",
        "              target_class_ids=list(range(len(CLASSIFICATIONS))),\n",
        "          ),\n",
        "          tf.keras.metrics.Accuracy(),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  m.fit(\n",
        "      training_dataset,\n",
        "      validation_data=validation_dataset,\n",
        "      epochs=25,\n",
        "      callbacks=[tf.keras.callbacks.TensorBoard(\n",
        "          'your-bucket/logs', histogram_freq=1)],\n",
        "  )\n",
        "\n",
        "  m.save(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo91lkMwe362"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT, location=REGION, staging_bucket=OUTPUT_DIR)\n",
        "\n",
        "job = aiplatform.CustomTrainingJob(\n",
        "    display_name='demo-fcnn-training',\n",
        "    script_path=f'{PACKAGE_PATH}/task.py',\n",
        "    container_uri='us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-11:latest',\n",
        ")\n",
        "\n",
        "job.run(\n",
        "    machine_type='n1-standard-4',\n",
        "    accelerator_type='NVIDIA_TESLA_T4',\n",
        "    accelerator_count=1,\n",
        "    args=[],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Host the model\n",
        "\n",
        "Reload the model saved by the training job and inspect the inputs it expects.  Note that the spatial dimensions are `None` to allow flexible input tile sizing.  Wrap the model in de/serialization layers to do the `base64` conversion.  Then re-save the wrapped model, which is ready to be hosted on Vertex AI."
      ],
      "metadata": {
        "id": "LmscUcIphCb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = keras.models.load_model(OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "5959-C2khl9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trained_model.inputs)"
      ],
      "metadata": {
        "id": "o-ngFl-J2OKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeSerializeInput(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def call(self, inputs_dict):\n",
        "    return {\n",
        "      k: tf.map_fn(lambda x: tf.io.parse_tensor(x, tf.float32),\n",
        "                   tf.io.decode_base64(v),\n",
        "                   fn_output_signature=tf.float32)\n",
        "        for (k, v) in inputs_dict.items()\n",
        "    }\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    return config\n",
        "\n",
        "\n",
        "class ReSerializeOutput(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def call(self, output_tensor):\n",
        "    return tf.map_fn(lambda x: tf.io.encode_base64(tf.io.serialize_tensor(x)),\n",
        "                    output_tensor,\n",
        "                    fn_output_signature=tf.string)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    return config\n",
        "\n",
        "input_deserializer = DeSerializeInput()\n",
        "output_deserializer = ReSerializeOutput()\n",
        "\n",
        "serialized_inputs = {\n",
        "    b: tf.keras.Input(shape=[], dtype='string', name=b) for b in BANDS\n",
        "}\n",
        "\n",
        "updated_model_input = input_deserializer(serialized_inputs)\n",
        "updated_model = trained_model(updated_model_input)\n",
        "updated_model = output_deserializer(updated_model)\n",
        "updated_model = tf.keras.Model(serialized_inputs, updated_model)"
      ],
      "metadata": {
        "id": "7t4YNGuJhBd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_model.save(EEIFIED_DIR)"
      ],
      "metadata": {
        "id": "MVIoLHeyk28R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_uTqQAaVTIK"
      },
      "source": [
        "# Deploy the model on Vertex AI\n",
        "\n",
        "Upload the model artifacts to Vertex AI create a model.  Create an endpoint and deploy the model to the endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Upload the model](https://cloud.google.com/sdk/gcloud/reference/ai/models/upload)\n",
        "Add an entry to the model registry that points to the location of the saved model and a container image needed to run the model."
      ],
      "metadata": {
        "id": "SNlHMjrin51r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud ai models delete {MODEL_NAME} --project={PROJECT} --region={REGION}"
      ],
      "metadata": {
        "id": "rsUq4i8Redmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RZRRzcfVu5T"
      },
      "source": [
        "!gcloud ai models upload \\\n",
        "  --artifact-uri={EEIFIED_DIR} \\\n",
        "  --project={PROJECT} \\\n",
        "  --region={REGION} \\\n",
        "  --container-image-uri={CONTAINER_IMAGE} \\\n",
        "  --description={MODEL_NAME} \\\n",
        "  --display-name={MODEL_NAME} \\\n",
        "  --model-id={MODEL_NAME}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Create an endpoint](https://cloud.google.com/sdk/gcloud/reference/ai/endpoints/create)\n",
        "\n",
        "Create an endpoint from which to serve the model."
      ],
      "metadata": {
        "id": "zwaYjMgYoHD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud ai endpoints create \\\n",
        "  --display-name={ENDPOINT_NAME} \\\n",
        "  --region={REGION} \\\n",
        "  --project={PROJECT}"
      ],
      "metadata": {
        "id": "7tJZt_XcWmr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Deploy the model to the endpoint](https://cloud.google.com/sdk/gcloud/reference/ai/endpoints/deploy-model)\n",
        "\n",
        "First, look up the endpoint ID, then deploy the model."
      ],
      "metadata": {
        "id": "QS_Rksv-wNQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ENDPOINT_ID = !gcloud ai endpoints list \\\n",
        "  --project={PROJECT} \\\n",
        "  --region={REGION} \\\n",
        "  --filter=displayName:{ENDPOINT_NAME} \\\n",
        "  --format=\"value(ENDPOINT_ID.scope())\"\n",
        "ENDPOINT_ID = ENDPOINT_ID[-1]"
      ],
      "metadata": {
        "id": "m3tgIGXld4xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud ai endpoints deploy-model {ENDPOINT_ID} \\\n",
        "  --project={PROJECT} \\\n",
        "  --region={REGION} \\\n",
        "  --model={MODEL_NAME} \\\n",
        "  --display-name={MODEL_NAME}"
      ],
      "metadata": {
        "id": "TIO93cnuwXhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aTGza-rWIjp"
      },
      "source": [
        "# Connect to the hosted model from Earth Engine\n",
        "\n",
        "1. Generate the input imagery.  This should be done in exactly the same way as the training data were generated.  See [this example notebook](http://colab.research.google.com/github/google/earthengine-api/blob/master/python/examples/ipynb/TF_demo1_keras.ipynb) for details.\n",
        "2. Connect to the hosted model.\n",
        "3. Use the model to make predictions.\n",
        "4. Display the results.\n",
        "\n",
        "Note that it may take the model a couple minutes to spin up before it is ready to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2OsyrJ7HAhE"
      },
      "source": [
        "def get_s2_composite(roi, start, end):\n",
        "  \"\"\"Get a cloud-free median composite in the specified ROI and date range.\"\"\"\n",
        "  s2c = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
        "  s2sr = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
        "  s2c = s2c.filterBounds(roi.buffer(100*1000, 1000)).filterDate(start, end)\n",
        "  s2sr = s2sr.filterBounds(roi.buffer(100*1000, 1000)).filterDate(start, end)\n",
        "\n",
        "  def index_join(collection_a, collection_b, property_name):\n",
        "    joined = ee.ImageCollection(\n",
        "        ee.Join.saveFirst(property_name).apply(\n",
        "            primary=collection_a,\n",
        "            secondary=collection_b,\n",
        "            condition=ee.Filter.equals(\n",
        "                leftField='system:index',\n",
        "                rightField='system:index')))\n",
        "    return joined.map(\n",
        "        lambda image: image.addBands(ee.Image(image.get(property_name))))\n",
        "\n",
        "  def mask_image(image):\n",
        "    prob = image.select('probability')\n",
        "    return image.select('B.*').divide(10000).updateMask(prob.lt(50))\n",
        "\n",
        "  with_cloud_probability = index_join(s2sr, s2c, 'cloud_probability')\n",
        "  masked = ee.ImageCollection(with_cloud_probability.map(mask_image))\n",
        "  return masked.select(BANDS).median().float().unmask(0)\n",
        "\n",
        "image = get_s2_composite(ee.Geometry.Point(COORDS), START, END)\n",
        "\n",
        "# Get a URL to serve image tiles.\n",
        "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
        "# Use folium to visualize the imagery.\n",
        "map = folium.Map(location=[COORDS[1], COORDS[0]], zoom_start=14)\n",
        "\n",
        "# Inputs.\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr=ATTRIBUTION,\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "\n",
        "endpoint_path = (\n",
        "    'projects/' + PROJECT + '/locations/' + REGION + '/endpoints/' + str(ENDPOINT_ID))\n",
        "\n",
        "# Connect to the hosted model.\n",
        "vertex_model = ee.Model.fromVertexAi(**{\n",
        "  'endpoint': endpoint_path,\n",
        "  'inputTileSize': [64, 64],\n",
        "  'inputOverlapSize': [32, 32],\n",
        "  'proj': ee.Projection('EPSG:4326').atScale(10),\n",
        "  'fixInputProj': True,\n",
        "  'outputBands': {'output': {\n",
        "      'type': ee.PixelType.float(),\n",
        "      'dimensions': 1\n",
        "    }\n",
        "  }\n",
        "})\n",
        "\n",
        "predictions = vertex_model.predictImage(image.select(BANDS).float())\n",
        "labels = predictions.arrayArgmax().arrayGet(0).rename('label')\n",
        "\n",
        "vis_params = {\n",
        "  'min': 0,\n",
        "  'max': len(CLASSIFICATIONS) - 1,\n",
        "  'palette': list(CLASSIFICATIONS.values()),\n",
        "  'bands': ['label'],\n",
        "}\n",
        "\n",
        "tile_url = labels.getMapId(vis_params)['tile_fetcher'].url_format\n",
        "folium.TileLayer(\n",
        "    tiles=tile_url,\n",
        "    attr=ATTRIBUTION,\n",
        "    overlay=True,\n",
        "    name='labels',\n",
        "  ).add_to(map)\n",
        "\n",
        "map.add_child(folium.LayerControl())\n",
        "display(map)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
