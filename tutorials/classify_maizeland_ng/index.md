---
title: Rapid classification of croplands: Case Study of smallholder maize-cultivated lands
description: This tutorial provides a quick template for rapid and replicable binary classification of maize-cultivated land in Nigeria, using google earth engine (GEE).
author: PJNation
tags: maize, smallholder, cropland, classify, rapid, binary
date_published: 2020-06-31
---
#

<!--
Copyright 2020 The Google Earth Engine Community Authors

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Background
Land-cover classification in complex landscapes have been constrained by inherent short-distance transition in crop/vegetation types, especially in complex smallholder farming systems. The increasing availability and accessibility of earth observation imageries provides significant opportunities to assess status and monitor changes in land cover, yet unlocking such capability is contingent on availability of relevant ground-truth data to calibrate and validate classification algorithms. The critically needed spatially-explicit ground-truth data are often unavailable in sub-Saharan African farming systems and this constrains development of relevant analytical tools to monitor cropland dynamics or generate [near]real-time insights on farming systems.  This tutorial was developed as a quick guide for users who are interested in implementing landcover classification routine in google earth engine environment, using ground-truth data and available sentinel 2A spectral bands imageries. The goal is to provide an easy-to-implement workflow that can be adapted by researchers and analysts to quickly classify croplands. As more efforts are invested in collecting spatially-rich georeferenced data at national and regional levels, this tutorial can be useful to generate immeditiate/timely insights for maize and crop types.

# Caveat
This landcover classification was implemented based on available data which was collected under a  multi-year project (https://tamasa.cimmyt.org/) which was focused on advancing digital agronomic innovation for decision support in maize-based farming systems. Therefore, the ground-truth data in this analytical workflow is rich in maize farm locations, and contains much fewer datapoints for other croptypes within the focal geography. Cosidering this limitation, the scope of this classification tool and this tutorial is limited to binary classification of maizelands (i.e. maize vs. non-maize cultivated) within the period of data collection (i.e. 2017).

# Acknowledgement
This tutorial was composed using the data that have been generated by teams who worked on TAMASA project, with funding from the Bill and Melinda Gates Foundation (BMGF).

## Outline

1. Ingesting, Importing, and Visualizing Data
a.  Ingest boundary files
b.  Ingest groundtruth data
c.  Import sentinel 2A Imageries

2.	Setting-Up and Implementing Analytics
a. Select Bands and extract training data
b. Train Classifiers, calculate error matrix, and assess training accuracy
c. Test Classification outputs, generate validation matrix, and calculate accuracy

3.	Calculate area and Export Outputs

4.	Final notes


## 1. Ingesting, Importing, and Visualizing Data

a. Ingest the Nigerian boundary as the focal geography and maize target region boundary as the area of interest (AOI). Using the code below, you will import a FeatureCollection object, and filter by "Country" to select "Nigeria". FeatureCollections are groups of features (spatial data and attributes). Filter is the method to extract a specific set of features from a feature collection. Assign the output to a variable called "nigerianBorder". The imagery analyses will be limited to the maize target region in Nigeria, i.e. the region that accounts for ~70% of Nigeria's maize production. Therefore, you will import a predefined shapefile layer (already converted to GEE asset) and assign to the variable "aoi". The maize target region asset can be assessed here (https://code.earthengine.google.com/?asset=users/juliusadewopo/MzeTargetRegion_alt_dslv2).  Display both layers to the map it using Map.addLayer() and tweak the symbology with the color parameters specified below.

// Ingest country boundaries feature collection.
var dataset = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017');

// Apply filter where country name equals Nigeria.
var nigeriaBorder = dataset.filter(ee.Filter.eq('country_na', 'Nigeria'));

// Print new "nigeriaBorder" object and explore features and properties.
// There should only be one feature representing Nigeria.
print(nigeriaBorder);

// Center the map view on Nigeria, otherwise, the view defaults to the Country of your IP address
Map.centerObject(nigeriaBorder, 6);

//Setting color parameters for Nigeria Boundary
var shown = true; // true or false, 1 or 0 
var opacity = 0.2; // number [0-1]
var nameLayer = 'map'; // string
var visParams = {color: 'red'}; // dictionary: 

//Add Nigeria as a layer to the map view
Map.addLayer(nigeriaBorder, visParams, nameLayer, shown, opacity);

//Setting color parameters for Maize target area boundary
var shown = true; // true or false, 1 or 0 
var opacity = 0.5; // number [0-1]
var nameLayer = 'map2'; // string
var visParams = {color: 'brown', strokeWidth: 5}; // dictionary:
Map.addLayer(aoi, visParams, nameLayer, shown, opacity);


b. Ingest groundtruth data for georeferenced locations where maize (and other crops) were cultivated during the growing season of 2017 (June - Oct). The data has been pre-processed and randomly split (70:30) into training and testing datasets. Import the training dataset as GEE asset here (https://code.earthengine.google.com/?asset=users/juliusadewopo/Mzetrgt_Train17_Rev), and the testing dataset here (https://code.earthengine.google.com/?asset=users/juliusadewopo/Mzetrgt_Test17_Rev). Assing variable names "trainpts" and "testpts" to the training and testing points, respectively, and add them as layer to the map view.

//Display training and test points to visualize distribution within the aoi
Map.addLayer(trainpts, {color:'FF0000'});
Map.addLayer(testpts, {color:'00FFFF'});


c. Next, you will import Copernicus Sentinel 2A spectral band imageries. The imageries are organized as an ImageCollection object, which is a container for a collection of individual images. With the code snippet below, you will import the sentinel 2A ImageCollection , and similar  method can be used to import an ImageCollection for other types of multi-temporal or multi-spectral data including Landsat, vegetation index,  rainfall, temperature etc. Considering the context, you will apply relevant filters to restrict selected imagery tiles to the aoi and date range for the growing season in 2017 (to coincide with the period of data collection). Standard quality control bands for cloud will be used in a function to create a mask layer, ans assigned to variable "maskS2Clouds". The function is passed on to a variable that minimizes cloud contamination by selecting pixels with the least percentage of cloud contamination within the temporal imagery composite (during the season). which will be combined with other filters to generate a "mosaic" of cloud-minimized imageries. Note that the aoi is north of the equator and often characterized by heavy cloudiness during the rainy season, so this workflow is essesntial and can be further tweaked to achieve better results.

//Ingest sentinel 2A imageries
var s2 = ee.ImageCollection("COPERNICUS/S2");

// Define parameters for cloudmask; Bits 10 and 11 are clouds and cirrus, respectively.
var cloudBitMask = ee.Number(2).pow(10).int();
var cirrusBitMask = ee.Number(2).pow(11).int();

//set function to generate cloud mask
function maskS2clouds(image) {
  var qa = image.select('QA60');
  // Both flags should be set to zero, indicating clear conditions.
  var mask = qa.bitwiseAnd(cloudBitMask).eq(0).and(
             qa.bitwiseAnd(cirrusBitMask).eq(0));
  return image.updateMask(mask);
}

//Apply the cloud mask with other filters to derive a mosaic within spatial and temporal context
var cloudMasked = s2.filterBounds(aoi).map(maskS2clouds).filterDate('2017-06-15', '2017-10-15');
var min = cloudMasked.min();
var mosaic = ee.ImageCollection(min).mosaic();

//Create Custom mosaic from selected bands to visualize the cloud-minimized imagery; You apply similar code to compare the initial cloud-contaminated imagery, setting "s2" as the image
Map.addLayer(mosaic, {bands: ['B4', 'B3', 'B2'], max: 2000}, 'custom mosaic');


## 2.	Setting-Up and Implementing Analytics
a. Now that you have prepared the mosaic, proceed to select the spectral bands that are relevant for the classification. By selecting more bands, the analysis will become more computationally intensive. The bands also have differing spatial resolution (https://en.wikipedia.org/wiki/Sentinel-2), so the relative computation cost (and time) will vary with the choice of bands. In the code below, all bands of the S2A are selected, but you can tweak this by selecting fewer bands. Note that our goal is to utilize as much spectral information as possible to train the classifier algorithm to differentiate between maize and non-maize. The training points (trainpts) will be used to extract the reflectance values of the pixels from all spectral bands and this will be passed to the classifier algorithms.

//Specify and select bands that will be used in the classification
var bands = ['B1','B2','B3','B4','B5','B6', 'B7', 'B8', 'B8A', 'B9', 'B10','B11', 'B12'];

var image_cl = mosaic
  .select(bands);

// Overlay the points on the imagery to get training.
var training = image_cl.sampleRegions({
  collection: trainpts,
  properties: ['class'],
  scale: 30
});


b. For the binary classification you will be applying 2 classifiers - classification and regression trees (CART) and Random Forest (RF), which are both suitable for categorical classification and have been used in various contexts for classification. By comparing outputs from both CART and RF, users can make objective inference on most accurate classifier. Default parameters will be accepted and further tweaks (such as optiming number of trees in RF) is outside the scope of this tutorial. The output imagery will be a bi-colored imagery, and the console will show the metrics. While reviewing the output metrics, note that maize is labeled as "0" while non-maize is labeled as "1".

// Train a CART classifier with default parameters.
var trained = ee.Classifier.smileCart().train(training, 'class', bands);

//Train a RF classifier with default parameters.
var trained_rf = ee.Classifier.smileRandomForest(10)
    .train({
      features: training,
      classProperty: 'class',
      inputProperties: bands
    });

// Classify the image with the same bands used for training.
var classified = image_cl.select(bands).classify(trained);
var classified_rf = image_cl.select(bands).classify(trained_rf);

// Create a palette to display the classes.
var palette =['00008B', '32CD32'];

//Add the output of the training classification to the map view
Map.addLayer(classified,{min: 0, max: 1, palette: palette},'class');
Map.addLayer(classified_rf,{min: 0, max: 1, palette: palette},'class');


// Calculate the training error matrix and accuracy for both classifiers by using the "confusionMatrix" function to generate metrics on the resubstitution accuracy, as shown below

//Accuracy calculation for CART
var trainAccuracy = trained.confusionMatrix();
print('Resubstitution error matrix: ', trainAccuracy);
print('Training overall accuracy: ', trainAccuracy.accuracy());

//Accuracy calculation for RF
var trainAccuracy_rf = trained_rf.confusionMatrix();
print('Resubstitution error matrix: ', trainAccuracy_rf);
print('Training overall accuracy: ', trainAccuracy_rf.accuracy());
  
  
c. To assess the reliability of the classification outputs, use the "testpts" dataset (earlier ingested as assets) to extract spectral information from the bands. You will further apply ee.Filter.neq on the "B1" band to remove pixels with null value, and extract the classified values for the "testpts" pixels from the training-mode algorithm classification. Note that different accuracy assessment is conducted for each classifier.
    
  // Use the testpts to extract pixel values from the bands for validation
 var testing = image_cl.sampleRegions({
  collection: testpts,
  properties: ['class'],
  scale: 30
}).filter(ee.Filter.neq('B1', null)); //filter added to rid out null pixels

// Classify the validation data
var validated = testing.classify(trained);
var validated_rf = testing.classify(trained_rf);

// Calculate confusion matrix to generate validation accuracy for CART
var testAccuracy = validated.errorMatrix('class', 'classification');
print('Validation error matrix: ', testAccuracy);
print('Validation overall accuracy: ', testAccuracy.accuracy());

// Calculate confusion matrix to generate validation accuracy for CART
var testAccuracy_rf = validated_rf.errorMatrix('class', 'classification');
print('Validation error matrix: ', testAccuracy_rf);
print('Validation overall accuracy: ', testAccuracy_rf.accuracy());

//Draw the "aoi" layer on top of the classified grid for visualization. You can tweak the opacity parameter or turn off the "aoi" layer to see the layer beneath more clearly
Map.addLayer(aoi);   


## 3. Calculate area and Export the output
With the binary classification completed, you can now export the classified imagery to google bucket drive (or any other compatible storage) for further analysis. Check the export resolution parameter (called "scale") and adjust accordingly to control output file size, if neccessary. The larger the scale, the lighter the file size. The maxPixels parameter sets an upper boundary on th enumber of pixels allowable for export to avoid export of large file or prolonged file creation time. Calculate the area for each landcover class by applying ee.Image.pixelArea on the classifed imagery and assign to the variable areaImage. By passing on the new variable to the Image reducing function, constrained by the aoi boundary geometry and specifying other parameters (per below), the area for both classes are generated in squeare meters. 


//Exporting to google bucket drive; If you use any other type of staorage solution, set the command as appropriate
  Export.image.toDrive({
 image: classified,
  description: 'Maizeland_Classified',
  scale: 20,
  region: aoi,
  maxPixels: 100000000000,
});

//To calculate Maize Area in square meters
var areaImage = ee.Image.pixelArea().addBands(
      classified_rf);
      
var areas = areaImage.reduceRegion({
      reducer: ee.Reducer.sum().group({
      groupField: 1,
      groupName: 'class',
    }),
    geometry: aoi.geometry(),
    scale: 500,
    maxPixels: 1e10
    }); 
 print (areas);


## 4.Final notes
Although this tutorial offers a template for users, further tweaks can possibly improve the results.  The results (in the Console tab) shows that RF outperformed CART in the validation mode (RF accuracy = 73.4%; CART accuracy = 69%). Also, the maize area estimated was 19.09*1e10 sq meter (i.e. ~19 million ha). Is is probable that this is over/under estimated, so further area-based validation may be necessary to validate the estimate. In any case, this analytical tool (and tutorial) can support rapid generation of national cropland area estimate that is scalable (across, regions, local governments/districts, wards or villages) based on delineated boundaries. 
