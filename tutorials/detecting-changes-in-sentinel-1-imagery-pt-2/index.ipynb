{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kdsGkYJXXKc"
   },
   "outputs": [],
   "source": [
    "#@title Copyright 2020 The Earth Engine Community Authors { display-mode: \"form\" }\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l18M9_r5XmAQ"
   },
   "source": [
    "# Detecting Changes in Sentinel-1 Imagery (Part 2)\n",
    "Author: mortcanty\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7i55vr_aKCB"
   },
   "source": [
    "### Run me first\n",
    "\n",
    "Run the following cell to initialize the API. The output will contain instructions on how to grant this notebook access to Earth Engine using your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeFsiSp2aDL6"
   },
   "outputs": [],
   "source": [
    "import ee\n",
    "\n",
    "# Trigger the authentication flow.\n",
    "ee.Authenticate()\n",
    "\n",
    "# Initialize the library.\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOf_UnIcZKBJ"
   },
   "source": [
    "### Datasets and Python modules\n",
    "One [dataset](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S1_GRD) will be used in the tutorial:\n",
    "\n",
    "- COPERNICUS/S1_GRD_FLOAT\n",
    "    - Sentinel-1 ground range detected images\n",
    "\n",
    "The following cell imports some python modules which we will be using as we go along and enables inline graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JR0cxCpeIxoY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm, gamma, f, chi2\n",
    "import IPython.display as disp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eelxHh2qc6xg"
   },
   "source": [
    "And to make use of interactive graphics, we import the _folium_ package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIiyf6azf4mU"
   },
   "outputs": [],
   "source": [
    "# Import the Folium library.\n",
    "import folium\n",
    "\n",
    "# Define a method for displaying Earth Engine image tiles to folium map.\n",
    "def add_ee_layer(self, ee_image_object, vis_params, name):\n",
    "  map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n",
    "  folium.raster_layers.TileLayer(\n",
    "    tiles = map_id_dict['tile_fetcher'].url_format,\n",
    "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "    name = name,\n",
    "    overlay = True,\n",
    "    control = True\n",
    "  ).add_to(self)\n",
    "\n",
    "# Add EE drawing method to folium.\n",
    "folium.Map.add_ee_layer = add_ee_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfDTVBnvu5un"
   },
   "source": [
    "## Part 2. Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-GuxE6lJHHx"
   },
   "source": [
    "We continue from [Part 1](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-1) of the Tutorial with the area of interest _aoi_ covering the Frankfurt International Airport and a subset _aoi\\_sub_  consisting of uniform pixels within a forested region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hww9JAK0JgFm"
   },
   "outputs": [],
   "source": [
    "geoJSON = {\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [\n",
    "          [\n",
    "            [\n",
    "              8.473892211914062,\n",
    "              49.98081240937428\n",
    "            ],\n",
    "            [\n",
    "              8.658599853515625,\n",
    "              49.98081240937428\n",
    "            ],\n",
    "            [\n",
    "              8.658599853515625,\n",
    "              50.06066538593667\n",
    "            ],\n",
    "            [\n",
    "              8.473892211914062,\n",
    "              50.06066538593667\n",
    "            ],\n",
    "            [\n",
    "              8.473892211914062,\n",
    "              49.98081240937428\n",
    "            ]\n",
    "          ]\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "coords = geoJSON['features'][0]['geometry']['coordinates']\n",
    "aoi = ee.Geometry.Polygon(coords)\n",
    "geoJSON = {\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [\n",
    "          [\n",
    "            [\n",
    "              8.534317016601562,\n",
    "              50.021637833966786\n",
    "            ],\n",
    "            [\n",
    "              8.530540466308594,\n",
    "              49.99780882512238\n",
    "            ],\n",
    "            [\n",
    "              8.564186096191406,\n",
    "              50.00663576154257\n",
    "            ],\n",
    "            [\n",
    "              8.578605651855469,\n",
    "              50.019431940583104\n",
    "            ],\n",
    "            [\n",
    "              8.534317016601562,\n",
    "              50.021637833966786\n",
    "            ]\n",
    "          ]\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "coords = geoJSON['features'][0]['geometry']['coordinates']\n",
    "aoi_sub = ee.Geometry.Polygon(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6rM63_lTqJ9"
   },
   "source": [
    "This time we filter the S1 archive to get an image collection consisting of two images acquired in the  month of August, 2020. Because we are interested in change detection, it is essential that the local incidence angles be the same in both images. So now we specify both the orbit pass (ASCENDING) as well the relative orbit number (15):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALF5YkahTvRl"
   },
   "outputs": [],
   "source": [
    "im_coll = (ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT')\n",
    "                .filterBounds(aoi)\n",
    "                .filterDate(ee.Date('2020-08-01'),ee.Date('2020-08-31'))\n",
    "                .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
    "                .filter(ee.Filter.eq('relativeOrbitNumber_start', 15))\n",
    "                .sort('system:time_start'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVWNXUIqI-lC"
   },
   "source": [
    "Here are the acquisition times in the collection, formatted with Python's _time_ module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pXdq7BbGUpm"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "acq_times = im_coll.aggregate_array('system:time_start').getInfo()\n",
    "[time.strftime('%x', time.gmtime(acq_time/1000)) for acq_time in acq_times]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkU_Topgr1Ul"
   },
   "source": [
    "### A ratio image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toLdO1Qe9eIf"
   },
   "source": [
    "Let's select the first two images and extract the VV bands,  clipping them to _aoi\\_sub_,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cj9Z7thsJa9t"
   },
   "outputs": [],
   "source": [
    "im_list = im_coll.toList(im_coll.size())\n",
    "im1 = ee.Image(im_list.get(0)).select('VV').clip(aoi_sub)\n",
    "im2 = ee.Image(im_list.get(1)).select('VV').clip(aoi_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0p5u2Kn9ufA"
   },
   "source": [
    "Now we'll build the ratio of the VV bands and display it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLICx5Y0UCvD"
   },
   "outputs": [],
   "source": [
    "ratio = im1.divide(im2)\n",
    "url = ratio.getThumbURL({'min': 0, 'max': 10})\n",
    "disp.Image(url=url, width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAoSUp799_ZK"
   },
   "source": [
    "As in the first part of the Tutorial, standard GEE reducers can be used to calculate a histogram, mean and variance of the ratio image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLY4C4V8LoLS"
   },
   "outputs": [],
   "source": [
    "hist = ratio.reduceRegion(ee.Reducer.fixedHistogram(0, 5, 500), aoi_sub).get('VV').getInfo()\n",
    "mean = ratio.reduceRegion(ee.Reducer.mean(), aoi_sub).get('VV').getInfo()\n",
    "variance = ratio.reduceRegion(ee.Reducer.variance(), aoi_sub).get('VV').getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcC98FbvUpmz"
   },
   "source": [
    "Here is a plot of the (normalized) histogram using _numpy_ and _matplotlib_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMgsrzHHUuSX"
   },
   "outputs": [],
   "source": [
    "a = np.array(hist)\n",
    "x = a[:, 0]\n",
    "y = a[:, 1] / np.sum(a[:, 1])\n",
    "plt.grid()\n",
    "plt.plot(x, y, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zyPzJOzU16A"
   },
   "source": [
    "This looks a bit like the gamma distribution we met in [Part 1](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-1#pixel_distributions) but is in fact an _F probability distribution_. The _F_ distribution is defined as the ratio of two chi square distributions, see [Eq. (1.12)](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-1#speckle), with $m_1$ and $m_2$ degrees of freedom. The above histogram is an $F$ distribution  with $m_1=2m$ and $m_2=2m$ degrees of freedom and is given by\n",
    " \n",
    "$$\n",
    "p_{f;2m,2m}(x) = {\\Gamma(2m)\\over \\Gamma(m)^2} x^{m-1}(1+x)^{-2m},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\quad {\\rm mean}(x) = {m\\over m-1},\\tag{2.1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\quad {\\rm var}(x) = {m(2m-1)\\over (m-1)^2 (m-2)}\n",
    "$$\n",
    " \n",
    "with parameter $m = 5$. We can see this empirically by overlaying the distribution onto the histogram with the help of _scipy.stats.f_. The histogram bucket widths are 0.01 so we have to divide by 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vQjpWmJU9jf"
   },
   "outputs": [],
   "source": [
    "m = 5\n",
    "plt.grid()\n",
    "plt.plot(x, y, '.', label='data')\n",
    "plt.plot(x, f.pdf(x, 2*m, 2*m) / 100, '-r', label='F-dist')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4p0R-O4r7kO"
   },
   "source": [
    "Checking the mean and variance, we get approximate agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6vkv1M7gpzy"
   },
   "outputs": [],
   "source": [
    "print(mean, m/(m-1))\n",
    "print(variance, m*(2*m-1)/(m-1)**2/(m-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2g5xTiYMF65a"
   },
   "source": [
    "So what is so special about this distribution? When looking for changes between two co-registered Sentinel-1 images acquired at different times, it might seem natural to subtract one from the other and then examine the difference, much as we would do for instance with visual/infrared ground reflectance images. In the case of SAR intensity images this is not a good idea. In the difference of two uncorrelated multilook images $\\langle s_1\\rangle$ and $\\langle s_2\\rangle$ the variances add together and, from Eq. (1.21) in the first part of the Tutorial,\n",
    " \n",
    "$$\n",
    "{\\rm var}(\\langle s_1\\rangle-\\langle s_2\\rangle) = {a_1^2+a_2^2\\over m}, \\tag{2.4}\n",
    "$$\n",
    " \n",
    "where $a_1$ and $a_2$ are mean intensities. So difference pixels in bright areas will have a higher variance than difference pixels in darker areas. It is not possible to set a reliable threshold to determine with a given confidence where change has occurred. \n",
    " \n",
    "It turns out that the _F_ distributed ratio of the two images which we looked at above is much more informative. For each pixel position in the two images, the quotient $\\langle s_1\\rangle / \\langle s_2\\rangle$ is a _likelihood ratio test statistic_ for deciding whether or not a change has occurred between the two acquisition dates at that position. We will explain what this means below. Here for now is the ratio of the two  Frankfurt Airport images, this time within the complete _aoi_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RYbVUN-G9LR"
   },
   "outputs": [],
   "source": [
    "im1 = ee.Image(im_list.get(0)).select('VV').clip(aoi)\n",
    "im2 = ee.Image(im_list.get(1)).select('VV').clip(aoi)\n",
    "ratio = im1.divide(im2)\n",
    "\n",
    "location = aoi.centroid().coordinates().getInfo()[::-1]\n",
    "mp = folium.Map(location=location, zoom_start=12, height=800, width=1000)\n",
    "mp.add_ee_layer(ratio,\n",
    "                {'min': 0, 'max': 20, 'palette': ['black', 'white']}, 'Ratio')\n",
    "mp.add_child(folium.LayerControl())\n",
    "\n",
    "display(mp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWFs_0UiK-xs"
   },
   "source": [
    "We might guess that the bright pixels here are significant changes, for instance due to aircraft movements on the tarmac or vehicles moving on the highway. Of course ''significant'' doesn't necessarily imply ''interesting''. We already know Frankfurt has a busy airport and that a German Autobahn is always crowded. The question is, how significant are the changes in the statistical sense? Let's now try to answer that question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gE7aypYKo0q"
   },
   "source": [
    "### Statistical testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3XO4qP3wNUP"
   },
   "source": [
    "A _statistical hypothesis_ is a conjecture about the distributions of one or more measured variables. It might, for instance, be an assertion about the mean of a distribution, or about the equivalence of the variances of two different distributions. We distinguish between _simple_ hypotheses, for which the distributions are completely specified, for example: _the mean of a normal distribution with  variance $\\sigma^2$  is  $\\mu=0$_, and _composite_ hypotheses, for which this is not the case, e.g., _the mean is $\\mu\\ge 0$_.\n",
    "\n",
    "In order to test such assertions on the basis of measured values, it is also necessary to formulate _alternative_ hypotheses. To distinguish these from the original assertions, the latter are traditionally called _null_ hypotheses. Thus we might be interested in testing the simple null hypothesis $\\mu = 0$ against the composite alternative hypothesis $\\mu\\ne 0$. An appropriate combination of measurements for deciding whether or not to reject the null hypothesis in favor of its alternative is referred to as a _test statistic_, often denoted by the symbol $Q$. An appropriate _test procedure_ will partition the possible test statistics into two subsets: an acceptance region for the null hypothesis and a rejection region. The latter is customarily referred to as the _critical region_.\n",
    "\n",
    "Referring to the null hypothesis as $H_0$, there are two kinds of errors which can arise from any test procedure:\n",
    "\n",
    "  - $H_0$ may be rejected when in fact it is true. This is called an _error of the first kind_ and the probability that it will occur is denoted $\\alpha$.\n",
    "  - $H_0$ may be accepted when in fact it is false, which is called an _error of the second kind_ with probability of occurrence $\\beta$.\n",
    "\n",
    "The probability of obtaining a value of the test statistic within the critical region when $H_0$ is true is thus $\\alpha$. The probability $\\alpha$ is also referred to as the _level of significance_ of the test or the _probability of a false positive_. It is generally the case that the lower the value of $\\alpha$, the higher is the probability $\\beta$ of making a second kind error, so there is always a trade-off. (Judge Roy Bean, from the film of the same name, didn't believe in trade-offs. He hanged all defendants regardless of the evidence. His $\\beta$ was zero, but his $\\alpha$ was rather large.)\n",
    "\n",
    "At any rate, traditionally, significance levels of 0.01 or 0.05 are often used.\n",
    "\n",
    "#### The _P_ value\n",
    "\n",
    "Suppose we determine the test statistic to have the value $q$. The _P value_ is defined as the probability of getting a test statistic $Q$ that is at least as extreme as the one observed given the null hypothesis. What is meant by \"extreme\" depends on how we choose the test statistic. If this probability is small, then the null hypothesis is unlikely. If it is smaller than the prescribed significance level $\\alpha$, then the null hypothesis is rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqqH9A3OtY2f"
   },
   "source": [
    "#### Likelihood Functions\n",
    "\n",
    "The $m$-look VV intensity bands of the two Sentinel-1 images that we took from the archive have pixel values\n",
    "\n",
    "$$\n",
    "\\langle s\\rangle=\\langle|S_{vv}|^2\\rangle, \\quad {\\rm with\\  mean}\\ a=|S^a_{vv}|^2,\n",
    "$$\n",
    "\n",
    "and are _gamma_ distributed according to [Eq. (1.1)](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-1#pixel_distributions), with parameters $\\alpha=m$ and $\\beta = a/m$. To make the notation a bit simpler, let's write $s =  \\langle s \\rangle$, so that the multi-look averaging is understood.\n",
    "\n",
    "Using subscript $i=1,2$ to refer to the two images, the probability densities are\n",
    "\n",
    "$$\n",
    "p(s_i| a_i) = {1 \\over (a_i/m)^m\\Gamma(m)}s_i^{m-1}e^{-s_i m/a_i},\\quad i=1,2. \\tag{2.5}\n",
    "$$\n",
    "\n",
    "We've left out the number of looks $m$ on the left hand side, since it is the same for both images. \n",
    "\n",
    "Now let's formulate a null hypothesis, namely that no change has taken place in the signal strength $a = |S^a_{vv}|^2$ between the two acquisitions, i.e.,\n",
    "\n",
    "$$\n",
    "H_0: \\quad a_1=a_2 = a\n",
    "$$ \n",
    "\n",
    "and test it against the alternative hypothesis that a change took place\n",
    "\n",
    "$$\n",
    "H_1: \\quad a_1\\ne a_2.\n",
    "$$ \n",
    "\n",
    "If the null hypothesis is true, then the so-called _likelihood_ for getting the measured pixel intensities $s_1$ and $s_2$ is defined as the product of the probability densities for that value of $a$,\n",
    "\n",
    "$$\n",
    "L_0(a) = p(s_1|a)p(s_2|a) = {1\\over(a/m)^{2m}\\Gamma(m)^2}(s_1s_2)^{m-1}e^{-(s_1+s_2)m/a}. \\tag{2.6}\n",
    "$$\n",
    "\n",
    "Taking the product of the probability densities like this is justified by the fact that the measurements $s_1$ and $s_2$ are independent.\n",
    "\n",
    "The _maximum likelihood_ is obtained by maximizing $L_0(a)$ with respect to  $a$,\n",
    "\n",
    "$$\n",
    "L_0(\\hat a) = p(s_1|\\hat a)p(s_2|\\hat a), \\quad \\hat a = \\arg\\max_a L_0(a). \n",
    "$$\n",
    "\n",
    "We can get $\\hat a$ simply by solving the equation\n",
    "\n",
    "$$\n",
    "{d L_0(a)\\over da} = 0\n",
    "$$\n",
    "\n",
    "for which we derive the maximum likelihood estimate (an easy exercise)\n",
    "\n",
    "$$\n",
    "\\hat a = {s_1 + s_2 \\over 2}.\n",
    "$$\n",
    "\n",
    "Makes sense: the only information we have is $s_1$ and $s_2$, so, if there was no change, our best estimate of the intensity $a$ is to take the average. Thus, substituting this value into Eq. (2.6), the maximum likelihood under $H_0$ is\n",
    "\n",
    "$$\n",
    "L_0(\\hat a) = {1\\over ((s_1+s_2)/2m)^{2m}\\Gamma(m)^2}(s_1s_2)^{m-1}e^{-2m}. \\tag{2.7}\n",
    "$$\n",
    "\n",
    "Similarly, under the alternative hypothesis $H_1$, the maximum likelihood is\n",
    "\n",
    "$$\n",
    "L_1(\\hat a_1,\\hat a_2) = p(s_1|\\hat a_1)p(s_2|\\hat a_2)\\quad \\hat a_1, \\hat a_2 = \\arg\\max_{a_1,a_2} L_1(a_1,a_2). \n",
    "$$\n",
    "\n",
    "Again, setting derivatives equal to zero, we get for $H_1$\n",
    "\n",
    "$$\n",
    "\\hat a_1 = s_1, \\quad \\hat a_2 = s_2,\n",
    "$$\n",
    "\n",
    "and the maximum likelihood\n",
    "\n",
    "$$\n",
    "L_1(\\hat a_1,\\hat a_2) = {m^{2m}\\over \\Gamma(m)^2}s_1s_2 e^{-2m}. \\tag{2.8}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki2RrEjgTb39"
   },
   "source": [
    "### The Likelihood Ratio Test\n",
    " \n",
    "The theory of statistical testing specifies methods for\n",
    "determining the most appropriate test procedure, one which minimizes the probability $\\beta$ of an error of the second kind  for a fixed level of significance $\\alpha$. Rather than giving a general definition, we state the appropriate test for our case: \n",
    " \n",
    "We should reject the null hypothesis if the _ratio_ of the two likelihoods satisfies the inequality\n",
    " \n",
    "$$\n",
    "Q = {L_0(\\hat a)\\over L_1(\\hat a_1,\\hat a_2)} \\le k \\tag{2.9}\n",
    "$$\n",
    " \n",
    "for some appropriately small value of threshold $k$.\n",
    " \n",
    "This definition simply reflects the fact that, if the null hypothesis is true, the maximum likelihood when $a_1=a_2$ should be close to the maximum likelihood without that restriction, given the measurements $s_1$ and $s_2$. Therefore, if the likelihood ratio is small, (less than or equal to some small value $k$), then $H_0$ should be rejected. \n",
    " \n",
    "With some (very) simply algebra, Eq. (2.9) evaluates to\n",
    " \n",
    "$$\n",
    "Q = \\left[2^2 \\left( s_1s_2\\over (s_1+s_2)^2\\right)\\right]^m \\le k \\tag{2.10}\n",
    "$$\n",
    " \n",
    "using (2.7) and (2.8). This is the same as saying\n",
    " \n",
    "$$\n",
    "{s_1s_2\\over (s_1+s_2)^2} \\le k'\\quad {\\rm or}\\quad {(s_1+s_2)^2\\over s_1s_2}\\ge k''\\quad {\\rm or}\\quad  {s_1\\over s_2}+{s_2\\over s_1}\\ge k''-2\n",
    "$$\n",
    " \n",
    "where $k',k''$ depend on $k$. The last inequality is satisfied if either term is small enough:\n",
    " \n",
    "$$\n",
    "{s_1\\over s_2} < c_1 \\quad {\\rm or}\\quad {s_2\\over s_1} < c_2 \\tag{2.11}\n",
    "$$\n",
    " \n",
    "again for some appropriate threshold $c_1$ and $c_2$ which depend on $k''$. \n",
    " \n",
    "So the ratio image $s_1/s_2$ that we generated above is indeed a _Likelihood Ratio Test (LRT) statistic_, one of two possible. We'll call it $Q_1 = s_1/s_2$ and the other one $Q_2 = s_2/s_1$. The former tests for a significant increase in intensity between times $t_1$ and $t_2$, the latter for a significant decrease.\n",
    " \n",
    "Fine, but where does the _F_ distribution come in?\n",
    " \n",
    "Both $s_1$ and $s_2$ are gamma distributed\n",
    " \n",
    "$$\n",
    "p(s\\mid a) = {1\\over (a/m)^m\\Gamma(m)}s^{m-1}e^{-sm/a}.\n",
    "$$\n",
    " \n",
    "Let $z = 2sm/a$. Then\n",
    " \n",
    "$$\n",
    "p(z\\mid a) = p(s\\mid a)\\left |{ds\\over dz}\\right | = {1\\over (a/m)^m\\Gamma(m)}\\left({za\\over 2m}\\right)^{m-1}\\left({a\\over 2m}\\right) = {1\\over 2^m\\Gamma(m)}z^{m-1}e^{-z/2}.\n",
    "$$\n",
    " \n",
    "Comparing this with [Eq. (1.12)](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-1#speckle) from the first part of the Tutorial, we see that $z$ is chi square distributed with $2m$ degrees of freedom, and therefore so are the variables $2s_1m/a$ and $2s_2m/a$. The quotients $s_1/s_2$ and $s_2/s_1$ are thus ratios of two chi square distributed variables with $2m$ degrees of freedom. They therefore have the _F_ distribution of Eq. (2.1).\n",
    " \n",
    "In order to decide the test for $Q_1$, we need the _P_ value for a measurement $q_1$ of the statistic. Recall that this is the probability of getting a result at least as extreme as the one measured under the null hypothesis. So in this case\n",
    " \n",
    "$$\n",
    "P_1 = {\\rm Prob}(Q_1\\le q_1\\mid H_0), \\tag{2.12}\n",
    "$$\n",
    " \n",
    "which we can calculate from the percentiles of the _F_ distribution, Eq. (2.1). Then if $P_1\\le \\alpha/2$ we reject $H_0$ and conclude with significance $\\alpha/2$ that a change occurred. We do the same test for $Q_2$, so that the combined significance is $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kocUkbzZ3vkQ"
   },
   "source": [
    "Now we can make a change map for the Frankfurt Airport for the two acquisitions, August 5 and August 11, 2020. We want to see quite large changes associated primarily with airplane and vehicle movements, so we will set the significance generously low to $\\alpha = 0.001$. We will also distinguish the direction of change and mask out the no-change pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXpGkHFrp4Q3"
   },
   "outputs": [],
   "source": [
    "# Decision threshold alpha/2:\n",
    "dt = f.ppf(0.0005, 2*m, 2*m)\n",
    "\n",
    "# LRT statistics.\n",
    "q1 = im1.divide(im2)\n",
    "q2 = im2.divide(im1)\n",
    "\n",
    "# Change map with 0 = no change, 1 = decrease, 2 = increase in intensity.\n",
    "c_map = im1.multiply(0).where(q2.lt(dt), 1)\n",
    "c_map = c_map.where(q1.lt(dt), 2)\n",
    "\n",
    "# Mask no-change pixels.\n",
    "c_map = c_map.updateMask(c_map.gt(0))\n",
    "\n",
    "# Display map with red for increase and blue for decrease in intensity.\n",
    "location = aoi.centroid().coordinates().getInfo()[::-1]\n",
    "mp = folium.Map(\n",
    "    location=location, tiles='Stamen Toner',\n",
    "    zoom_start=13, height=800, width=1000)\n",
    "folium.TileLayer('OpenStreetMap').add_to(mp)\n",
    "mp.add_ee_layer(ratio,\n",
    "                {'min': 0, 'max': 20, 'palette': ['black', 'white']}, 'Ratio')\n",
    "mp.add_ee_layer(c_map,\n",
    "                {'min': 0, 'max': 2, 'palette': ['black', 'blue', 'red']},\n",
    "                'Change Map')\n",
    "mp.add_child(folium.LayerControl())\n",
    "\n",
    "display(mp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSfPdRftGdrW"
   },
   "source": [
    "Most changes are within the airport or on the Autobahn. Barge movements on the Main River (upper left hand corner) are also signaled as significant changes. Note that the 'red' changes (significant increases in intensity) do not show up in the 'ratio' overlay, which displays $s_1/s_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mYnuTu4rpcn"
   },
   "source": [
    "### Bivariate change detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCZbWa7gEaHH"
   },
   "source": [
    "Rather than analyzing the VV and VH bands individually, it would make more sense to treat them together, and that is what we will now do. It is convenient to work with the covariance matrix form for measured intensities that we introduce in Part 1, see [Eq.(1.6a)](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-1#single_look_complex_slc_sar_measurements). Again with the aim of keeping the notation simple, define\n",
    "\n",
    "$$\n",
    "\\pmatrix{ s_i & 0\\cr 0 & r_i} = \\pmatrix{\\langle|S_{vv}|^2\\rangle_i & 0 \\cr 0 & \\langle|S_{vh}|^2\\rangle_i}, \\quad {\\rm with\\ means}\\quad a_i = \\langle|S^{a_i}_{vv}|^2\\rangle, \\quad b_i = \\langle|S^{b_i}_{vh}|^2\\rangle \\tag{2.13}\n",
    "$$\n",
    "\n",
    "for the two acquisition times $t_i,\\ i=1,2$. \n",
    "\n",
    "Under $H_0$ we have $a_1=a_2=a$ and $b_1=b_2=b$. Assuming independence of $s_i$ and $r_i$, the likelihood function is the product of the four gamma distributions\n",
    "\n",
    "$$\n",
    "L_0(a,b) = p(s_1\\mid a)p(r_1\\mid b)p(s_2\\mid a)p(r_2\\mid b).\n",
    "$$\n",
    "\n",
    "Under $H_1$,\n",
    "\n",
    "$$\n",
    "L_1(a_1,b_1,a_2,b_2) = p(s_1\\mid a_1)p(r_1\\mid b_1)p(s_2\\mid a_2)p(r_2\\mid b_2).\n",
    "$$\n",
    "\n",
    "With maximum likelihood estimates under $H_0$ \n",
    "\n",
    "$$\n",
    "\\hat a = (s_1+s_2)/2\\quad {\\rm and}\\quad \\hat b = (r_1+r_2)/2\n",
    "$$ \n",
    "\n",
    "for the parameters and some simple algebra, we get \n",
    "\n",
    "$$\n",
    "L_0(\\hat a,\\hat b) = {(2m)^{4m}\\over (s_1+s_2)^{2m}(r_1+r_2)^{2m}\\Gamma(m)^4}s_1r_1s_2r_2e^{-4m}. \\tag{2.14}\n",
    "$$ \n",
    "\n",
    "Similarly with $\\hat a_1=s_1,\\ \\hat b_1=r_1,\\ \\hat a_2=s_2,\\ \\hat b_2=r_2$, we calculate\n",
    "\n",
    "$$\n",
    "L_1(\\hat a_1,\\hat b_1,\\hat a_2,\\hat b_2) = {m^{4m}\\over s_1r_1s_2r_2}e^{-4m}.\n",
    "$$\n",
    "\n",
    "The likelihood test statistic in then\n",
    "\n",
    "$$\n",
    "Q = {L_0(\\hat a,\\hat b)\\over L_1(\\hat a_1,\\hat b_1,\\hat a_2,\\hat b_2)}={2^4(s_1r_1s_2r_2)^m\\over (s_1+s_2)^{2m}(r_1+r_2)^{2m}}.\n",
    "$$\n",
    "\n",
    "Writing this in terms of the covariance matrix representation,\n",
    "\n",
    "$$\n",
    "c_i = \\pmatrix{s_i & 0\\cr 0 & r_i},\\quad i=1,2,\n",
    "$$\n",
    "\n",
    "we derive, finally, the likelihood ratio test\n",
    "\n",
    "$$\n",
    "Q = \\left[2^4\\pmatrix{|c_1| |c_2|\\over |c_1+c_2|^2 }\\right]^m \\le k, \\tag{2.15}\n",
    "$$\n",
    "\n",
    "where $|\\cdot|$ indicates the matrix determinant, $|c_i|=s_ir_i$. \n",
    "\n",
    "So far so good. But in order to determine _P_ values, we need the probability distribution of $Q$. This time we have no idea how to obtain it. Here again, statistical theory comes to our rescue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWU-gu4W3p-_"
   },
   "source": [
    "Let $\\Theta$ be the parameter space for the LRT. In our example it is \n",
    "$$\n",
    "\\Theta = \\{ a_1,b_1,a_2,b_2\\}\n",
    "$$ \n",
    "and has $d=4$ dimensions. Under the null hypothesis the parameter space is restricted by the conditions $a=a_1=a_2$ and $b=b_1=b_2$ to \n",
    "$$\n",
    "\\Theta_0 = \\{ a,b\\}\n",
    "$$ \n",
    "with $d_0=2$ dimensions. According to [Wilks' Theorem](https://en.wikipedia.org/wiki/Wilks%27_theorem), as the number of measurements determining the LRT statistic $Q$ approaches $\\infty$, the test statistic $-2\\log Q$ approaches a chi square distribution with $d-d_0=2$ degrees of freedom. (Recall that, in order to determine the matrices $c_1$ and $c_2$, five individual measurements were averaged or multi-looked.) So rather than working with $Q$ directly, we use $-2\\log Q$ instead and hope that Wilk's theorem is a good enough approximation for our case.\n",
    "\n",
    "In order to check if this is so, we just have to program \n",
    "\n",
    "$$\n",
    "-2\\log Q = (\\log{|c_1|}+\\log{|c_2|}-2\\log{|c_1+c_2|}+4\\log{2})(-2m)\n",
    "$$ \n",
    "\n",
    "in GEE-ese:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HKdnWr8YU1U"
   },
   "outputs": [],
   "source": [
    "def det(im):\n",
    "  return im.expression('b(0) * b(1)')\n",
    "\n",
    "# Number of looks.\n",
    "m = 5\n",
    "\n",
    "im1 = ee.Image(im_list.get(0)).select('VV', 'VH').clip(aoi)\n",
    "im2 = ee.Image(im_list.get(1)).select('VV', 'VH').clip(aoi)\n",
    "\n",
    "m2logQ = det(im1).log().add(det(im2).log()).subtract(\n",
    "    det(im1.add(im2)).log().multiply(2)).add(4*np.log(2)).multiply(-2*m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wmprc_di-PBz"
   },
   "source": [
    "and then plot its histogram, comparing it with the chi square distribution _scipy.stats.chi2.pdf()_ with two degrees of freedom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVAjH_uR_kkg"
   },
   "outputs": [],
   "source": [
    "hist = m2logQ.reduceRegion(\n",
    "    ee.Reducer.fixedHistogram(0, 20, 200), aoi).get('VV').getInfo()\n",
    "a = np.array(hist)\n",
    "x = a[:, 0]\n",
    "y = a[:, 1] / np.sum(a[:, 1])\n",
    "plt.plot(x, y, '.', label='data')\n",
    "plt.plot(x, chi2.pdf(x, 2)/10, '-r', label='chi square')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvOXWaRhCQ8G"
   },
   "source": [
    "Looks pretty good. Note now that a small value of the LRT $Q$ in Eq. (2.15) corresponds to a large value of $-2\\log{Q}$. Therefore the _P_ value for a measurement $q$ is now the probability of getting the value $-2\\log{q}$\n",
    "or higher,\n",
    "$$\n",
    "P = {\\rm Prob}(-2\\log{Q} \\ge -2\\log{q}) = 1 - {\\rm Prob}(-2\\log{Q} < -2\\log{q}).\n",
    "$$\n",
    "\n",
    "So let's try out our bivariate change detection procedure, this time on an agricultural scene where we expect to see larger regions of change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHYIAceXKNjG"
   },
   "outputs": [],
   "source": [
    "geoJSON ={\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [\n",
    "          [\n",
    "            [\n",
    "              -98.2122802734375,\n",
    "              49.769291532628515\n",
    "            ],\n",
    "            [\n",
    "              -98.00559997558594,\n",
    "              49.769291532628515\n",
    "            ],\n",
    "            [\n",
    "              -98.00559997558594,\n",
    "              49.88578690918283\n",
    "            ],\n",
    "            [\n",
    "              -98.2122802734375,\n",
    "              49.88578690918283\n",
    "            ],\n",
    "            [\n",
    "              -98.2122802734375,\n",
    "              49.769291532628515\n",
    "            ]\n",
    "          ]\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "coords = geoJSON['features'][0]['geometry']['coordinates']\n",
    "aoi1 = ee.Geometry.Polygon(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpOZqWqUROoG"
   },
   "source": [
    "This is a mixed agricultural/forest area in southern Manitoba, Canada. We'll gather two images, one from the beginning of August and one from the beginning of September, 2018. A lot of harvesting takes place in this interval, so we expect some extensive changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmQfIKvmLdCc"
   },
   "outputs": [],
   "source": [
    "im1 = ee.Image(ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT')\n",
    "                .filterBounds(aoi1)\n",
    "                .filterDate(ee.Date('2018-08-01'), ee.Date('2018-08-31'))\n",
    "                .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
    "                .filter(ee.Filter.eq('relativeOrbitNumber_start', 136))\n",
    "                .first()\n",
    "                .clip(aoi1))\n",
    "im2 = ee.Image(ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT').filterBounds(aoi1)\n",
    "                .filterDate(ee.Date('2018-09-01'), ee.Date('2018-09-30'))\n",
    "                .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
    "                .filter(ee.Filter.eq('relativeOrbitNumber_start', 136))\n",
    "                .first()\n",
    "                .clip(aoi1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaeCqVQhEe9F"
   },
   "source": [
    "Here are the acquisition times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZjsAxhKKmZs"
   },
   "outputs": [],
   "source": [
    "acq_time = im1.get('system:time_start').getInfo()\n",
    "print( time.strftime('%x', time.gmtime(acq_time/1000)) )\n",
    "acq_time = im2.get('system:time_start').getInfo()\n",
    "print( time.strftime('%x', time.gmtime(acq_time/1000)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T9VFW1hSZMR"
   },
   "source": [
    "Fortunately it is possible to map the chi square cumulative distribution function over an _ee.Image()_  so that a _P_ value image can be calculated directly. This wasn't possible in the single band case, as the _F_ cumulative distribution is not available on the GEE. Here are the _P_ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmU0Qt8GK8CB"
   },
   "outputs": [],
   "source": [
    "def chi2cdf(chi2, df):\n",
    "  ''' Chi square cumulative distribution function for df degrees of freedom\n",
    "      using the built-in incomplete gamma function gammainc() '''\n",
    "  return ee.Image(chi2.divide(2)).gammainc(ee.Number(df).divide(2))\n",
    "\n",
    "# The observed test statistic image -2logq.\n",
    "m2logq = det(im1).log().add(det(im2).log()).subtract(\n",
    "    det(im1.add(im2)).log().multiply(2)).add(4*np.log(2)).multiply(-2*m)\n",
    "\n",
    "# The P value image prob(m2logQ > m2logq) = 1 - prob(m2logQ < m2logq).\n",
    "p_value = ee.Image.constant(1).subtract(chi2cdf(m2logq, 2))\n",
    "\n",
    "# Project onto map.\n",
    "location = aoi1.centroid().coordinates().getInfo()[::-1]\n",
    "mp = folium.Map(location=location, zoom_start=12, height=800, width=1000)\n",
    "mp.add_ee_layer(p_value,\n",
    "                {'min': 0,'max': 1, 'palette': ['black', 'white']}, 'P-value')\n",
    "mp.add_child(folium.LayerControl())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLQlE1IlM6Dj"
   },
   "source": [
    "The uniformly dark areas correspond to small or vanishing _P_ values and signify change. The bright areas correspond to no change. Why they are not uniformly bright will be explained below. Now we set a significance threshold of $\\alpha=0.01$ and display the significant changes, whereby 1% of them will be false positives. For reference we also show the 2018 [Canada AAFC Annual Crop Inventory](https://developers.google.com/earth-engine/datasets/catalog/AAFC_ACI) map, which is available as a GEE collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lInJMXCyTlaF"
   },
   "outputs": [],
   "source": [
    "c_map = p_value.multiply(0).where(p_value.lt(0.01), 1)\n",
    "\n",
    "crop2018 = (ee.ImageCollection('AAFC/ACI')\n",
    "             .filter(ee.Filter.date('2018-01-01', '2018-12-01'))\n",
    "             .first()\n",
    "             .clip(aoi1))\n",
    "\n",
    "mp = folium.Map(location=location, zoom_start=12, height=800, width=1000)\n",
    "mp.add_ee_layer(crop2018, {min: 0, max: 255}, 'crop2018')\n",
    "mp.add_ee_layer(c_map.updateMask(\n",
    "    c_map.gt(0)), {'min': 0, 'max': 1, 'palette': ['black', 'red']}, 'c_map')\n",
    "mp.add_child(folium.LayerControl())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vATDsxFOOESB"
   },
   "source": [
    " The major crops in the scene are soybeans (dark brown), oats (light brown), canola (light green), corn (light yellow) and winter wheat (dark gray). The wooded areas exhibit little change, while canola has evidently been extensively harvested in the interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COkFrH11bpNI"
   },
   "source": [
    "#### A note on _P_ values\n",
    "Because small _P_ values are indicative of change, it is tempting to say that, the larger the _P_ value, the higher the probability of no change. Or more explicitly, the _P_ value is itself the no change probability. Let's see why this is false. Below we choose a wooded area of the agricultural scene where few significant changes are to be expected and use it to subset the _P_ value image. Then we plot the histogram of the subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "761UOg0UCEmQ"
   },
   "outputs": [],
   "source": [
    "geoJSON ={\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [\n",
    "          [\n",
    "            [\n",
    "              -98.18550109863281,\n",
    "              49.769735012247885\n",
    "            ],\n",
    "            [\n",
    "              -98.13949584960938,\n",
    "              49.769735012247885\n",
    "            ],\n",
    "            [\n",
    "              -98.13949584960938,\n",
    "              49.798109268622\n",
    "            ],\n",
    "            [\n",
    "              -98.18550109863281,\n",
    "              49.798109268622\n",
    "            ],\n",
    "            [\n",
    "              -98.18550109863281,\n",
    "              49.769735012247885\n",
    "            ]\n",
    "          ]\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "coords = geoJSON['features'][0]['geometry']['coordinates']\n",
    "aoi1_sub = ee.Geometry.Polygon(coords)\n",
    "hist = p_value.reduceRegion(ee.Reducer.fixedHistogram(0, 1, 100), aoi1_sub).get('constant').getInfo()\n",
    "a = np.array(hist)\n",
    "x = a[:,0]\n",
    "y = a[:,1]/np.sum(a[:,1])\n",
    "plt.plot(x, y, '.b', label='p-value')\n",
    "plt.ylim(0, 0.05)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz4ipvMKPmxK"
   },
   "source": [
    "So the P values of no-change measurements are uniformly distributed over $[0, 1]$ (the excess of small _P_ values at the left can be ascribed to genuine changes within the polygon). A large _P_ value is no more indicative of no change than a small one. Of course it has to be this way. When, for example, we set a significance level of 5%, then the fraction of false positives, i.e., the fraction of _P_ values smaller than 0.05 given $H_0$, must also be 5%. This accounts for the noisy appearance of the _P_ value image in the no-change regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LnZpAKQcRz5"
   },
   "source": [
    "#### Change direction: the Loewner order\n",
    "What about the direction of change in the bivariate case? This is less clear, as we can have the situation where the VV intensity gets larger and the VH smaller from time $t_1$ to $t_2$, or vice versa. When we are dealing with the C2 covariance matrix representation of SAR imagery, see Eq. (2.13), a characterization of change can be made as follows [(Nielsen et al. (2019))](https://ieeexplore.ieee.org/document/8736751): For each significantly changed pixel, we determine the difference $C2_{t_2}-C2_{t_1}$ and examine its so-called _definiteness_, also known as the _Loewner order_ of the change. A matrix is said to be _positive definite_ if all of its eigenvalues are positive, _negative definite_ if they are all negative, otherwise _indefinite_. In the case of the $2\\times 2$ diagonal matrices that we are concerned with the eigenvalues are just the two diagonal elements themselves, so determining the Loewner order is trivial. For full $2\\times 2$ dual pol or $3\\times 3$ quad pol SAR imagery, devising an efficient way to determine the Loewner order is more difficult, see [Nielsen (2019)](https://ieeexplore.ieee.org/document/8913617).\n",
    "\n",
    "So let's include the Loewner order in our change map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOL0V1DNsCqH"
   },
   "outputs": [],
   "source": [
    "c_map = p_value.multiply(0).where(p_value.lt(0.01), 1)\n",
    "diff = im2.subtract(im1)\n",
    "d_map = c_map.multiply(0)                    # Initialize the direction map to zero.\n",
    "d_map = d_map.where(det(diff).gt(0), 2)      # All pos or neg def diffs are now labeled 2.\n",
    "d_map = d_map.where(diff.select(0).gt(0), 3) # Re-label pos def (and label some indef) to 3.\n",
    "d_map = d_map.where(det(diff).lt(0), 1)      # Label all indef to 1.\n",
    "c_map = c_map.multiply(d_map)                # Re-label the c_map, 0*X = 0, 1*1 = 1, 1*2= 2, 1*3 = 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVh9eU6V3bVF"
   },
   "source": [
    "Now we display the changes, with positive definite red, negative definite blue, and indefinite yellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8_ggucVvOuh"
   },
   "outputs": [],
   "source": [
    "mp = folium.Map(location=location, zoom_start=12, height=800, width=1000)\n",
    "mp.add_ee_layer(crop2018, {min: 0, max: 255}, 'crop2018')\n",
    "mp.add_ee_layer(\n",
    "    c_map.updateMask(c_map.gt(0)), {\n",
    "        'min': 0,\n",
    "        'max': 3,\n",
    "        'palette': ['black', 'yellow', 'blue', 'red']\n",
    "    }, 'c_map')\n",
    "mp.add_child(folium.LayerControl())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghLYSYeLSj0P"
   },
   "source": [
    "The more or less compact blue changes indicate a decrease in reflectivity in both VV and VH bands, and correspond to crop harvesting (especially canola).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvQO9S0W8UEQ"
   },
   "source": [
    "### Outlook\n",
    "We have now covered the subject of bitemporal change detection with GEE Sentinel-1 imagery. The beauty of GEE is that it is trivially easy to gather arbitrarily long time series of S1 images from the archive, all with revisit times of 6 or 12 days depending on whether one or both satellites are collecting data. The next part of the Tutorial will generalize the techniques we have learned so far to treat multitemporal change detection.\n",
    "\n",
    "### Oh, and one more thing ...\n",
    "\n",
    "We didn't mention it above, but note the similarity between Eq. (2.10) and Eq. (2.15). To go from the monovariate LRT to the bivariate LRT, we simply replace the product of  intensities $s_1s_2$ by the product of determinants $|c_1||c_2|$, the sum $s_1+s_2$ by $|c_1+c_2|$ and the factor $2^{2}$ by $2^4=2^{2\\cdot2}$. This observation will come in handy in Part 3."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Detecting Changes in Sentinel-1 Imagery (Part 2)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
