{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial-pt-3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kdsGkYJXXKc"
      },
      "source": [
        "#@title Copyright 2020 The Earth Engine Community Authors { display-mode: \"form\" }\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l18M9_r5XmAQ"
      },
      "source": [
        "# Detecting Changes in Sentinel-1 Imagery (Part 3)\n",
        "\n",
        "Author: mortcanty\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7i55vr_aKCB"
      },
      "source": [
        "### Run me first\n",
        "\n",
        "Run the following cell to initialize the API. The output will contain instructions on how to grant this notebook access to Earth Engine using your account."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeFsiSp2aDL6"
      },
      "source": [
        "import ee\n",
        "# Trigger the authentication flow.\n",
        "ee.Authenticate()\n",
        "\n",
        "# Initialize the library.\n",
        "ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOf_UnIcZKBJ"
      },
      "source": [
        "### Datasets and Python modules\n",
        "One [dataset](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S1_GRD) will be used in the tutorial:\n",
        "- COPERNICUS/S1_GRD_FLOAT\n",
        "  - Sentinel-1 ground range detected images\n",
        "\n",
        "The following cell imports some python modules which we will be using as we go along and enables inline graphics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmusFZcZHEjE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import norm, gamma, f, chi2\n",
        "import IPython.display as disp\n",
        "import time\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZEVxUZ2mTSe"
      },
      "source": [
        "This cell carries over the chi square cumulative distribution function and the determinant of a Sentinel-1 image from Part 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li189P8wmOTS"
      },
      "source": [
        "def chi2cdf(chi2,df):\n",
        "    ''' Chi square cumulative distribution function for df degrees of freedom \n",
        "        using the built-in incomplete gamma function gammainc() '''\n",
        "    return ee.Image(chi2.divide(2)).gammainc(ee.Number(df).divide(2))\n",
        "\n",
        "def det(im):\n",
        "    ''' Determinant of 2x2 diagonal covariance matrix '''\n",
        "    return im.expression('b(0)*b(1)')    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eelxHh2qc6xg"
      },
      "source": [
        "And to make use of interactive graphics, we import the _folium_ package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEiSY5zdoFPe"
      },
      "source": [
        "# Import the Folium library.\n",
        "import folium\n",
        "\n",
        "# Define a method for displaying Earth Engine image tiles to folium map.\n",
        "def add_ee_layer(self, ee_image_object, vis_params, name):\n",
        "  map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n",
        "  folium.raster_layers.TileLayer(\n",
        "    tiles = map_id_dict['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    name = name,\n",
        "    overlay = True,\n",
        "    control = True\n",
        "  ).add_to(self)\n",
        "\n",
        "# Add EE drawing method to folium.\n",
        "folium.Map.add_ee_layer = add_ee_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXo28Rx8HTEd"
      },
      "source": [
        "## Part 3. Multitemporal change detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d43UObDY-hjL"
      },
      "source": [
        "Continuing from Part 2, in which we discussed bitemporal change detection with Sentinel-1 images, we turn our attention to the multitemporal case. To get started, we obviously need ...\n",
        " \n",
        "### A time series\n",
        " \n",
        "Here is a fairly interesting one: a region in South Yorkshire, England where, in November 2019, extensive flooding occurred along the River Don just north of the city of Doncaster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0IIB_YB_ZoA"
      },
      "source": [
        "geoJSON = {\n",
        "  \"type\": \"FeatureCollection\",\n",
        "  \"features\": [\n",
        "    {\n",
        "      \"type\": \"Feature\",\n",
        "      \"properties\": {},\n",
        "      \"geometry\": {\n",
        "        \"type\": \"Polygon\",\n",
        "        \"coordinates\": [\n",
        "          [\n",
        "            [\n",
        "              -1.2998199462890625,\n",
        "              53.48028242228504\n",
        "            ],\n",
        "            [\n",
        "              -0.841827392578125,\n",
        "              53.48028242228504\n",
        "            ],\n",
        "            [\n",
        "              -0.841827392578125,\n",
        "              53.6958933974518\n",
        "            ],\n",
        "            [\n",
        "              -1.2998199462890625,\n",
        "              53.6958933974518\n",
        "            ],\n",
        "            [\n",
        "              -1.2998199462890625,\n",
        "              53.48028242228504\n",
        "            ]\n",
        "          ]\n",
        "        ]\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "coords = geoJSON['features'][0]['geometry']['coordinates']\n",
        "aoi = ee.Geometry.Polygon(coords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqvC_m12kZ6X"
      },
      "source": [
        "The image collection below covers the months of September, 2019 through January, 2020 at 6-day intervals:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W317cgEc_uhH"
      },
      "source": [
        "im_coll = ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT') \\\n",
        "                .filterBounds(aoi) \\\n",
        "                .filterDate(ee.Date('2019-09-01'),ee.Date('2020-01-31')) \\\n",
        "                .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING')) \\\n",
        "                .filter(ee.Filter.eq('relativeOrbitNumber_start', 154)) \\\n",
        "                .sort('system:time_start')\n",
        "acquisition_times = ee.List(im_coll.aggregate_array('system:time_start')).getInfo()\n",
        "\n",
        "# List of timestamps in YYYYMMDD format.\n",
        "timestamplist = []\n",
        "for timestamp in acquisition_times:\n",
        "    tmp = time.gmtime(int(timestamp)/1000)\n",
        "    timestamplist.append(time.strftime('%x', tmp)) \n",
        "timestamplist = [x.replace('/','') for x in timestamplist]  \n",
        "timestamplist = ['T20'+x[4:]+x[0:4] for x in timestamplist]                     \n",
        "timestamplist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUiTi-eynGPi"
      },
      "source": [
        "It will turn out to be convenient to work with a list rather than a collection, so we'll convert the collection to a list and, while  we're at it, clip the images to our AOI using list iteration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOCbUmW-UHIK"
      },
      "source": [
        "def clip_iter(current,prev):\n",
        "    '''Iteration function to clip a list of images.'''\n",
        "    prev = ee.Dictionary(prev)\n",
        "    im_list = ee.List(prev.get('im_list'))\n",
        "    aoi = prev.get('aoi') \n",
        "    im_list = im_list.add(ee.Image(current).clip(aoi))\n",
        "    return ee.Dictionary({'im_list': im_list, 'aoi': aoi})\n",
        "\n",
        "im_list = im_coll.toList(im_coll.size())    \n",
        "\n",
        "# Clip the list of images.\n",
        "first = ee.Dictionary({'im_list': ee.List([]), 'aoi': aoi}) \n",
        "im_list = ee.List(ee.Dictionary(im_list.iterate(clip_iter, first)).get('im_list'))  \n",
        "\n",
        "im_list.length().getInfo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-dEBL3Gnbnn"
      },
      "source": [
        "Here is an RGB composite of the VV bands for three images in early November, after conversion to decibels. Note that some changes, especially those due to flooding, already show up in this representation as colored pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwJv_Ib8MFKH"
      },
      "source": [
        "def selectvv(current):\n",
        "    return ee.Image(current).select('VV')\n",
        "\n",
        "vv_list = im_list.map(selectvv)\n",
        "\n",
        "location = aoi.centroid().coordinates().getInfo()[::-1]\n",
        "mp = folium.Map(location=location, zoom_start=11, width=1000)\n",
        "rgb_images = ee.Image.rgb(vv_list.get(10),vv_list.get(11),vv_list.get(12)).log10().multiply(10)\n",
        "mp.add_ee_layer(rgb_images, {'min': -20,'max': 0}, 'rgb composite')\n",
        "mp.add_child(folium.LayerControl())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnsgS-VVs6rS"
      },
      "source": [
        "Now we have a series of 26 SAR images and, for whatever reason, would like to know where and when changes have taken place. A first reaction might be: \n",
        " \n",
        "*What's the problem? Just apply the bitemporal method we developed in Part 2 to each of the 25 time intervals.*\n",
        " \n",
        "Well, one problem is the rate of false positives. If the bitemporal tests are statistically independent, then the probability of **not** getting a false positive over a series of length $k$ is the product of not getting one in each of the $k-1$ intervals, i.e., $(1-\\alpha)^{k-1}$ and the overall first kind error probabilty $\\alpha_T$ is its complement:\n",
        " \n",
        "$$\n",
        "\\alpha_T = 1-(1-\\alpha)^{k-1}. \\tag{3.1}\n",
        "$$\n",
        " \n",
        "For our case, even with a small value of $\\alpha=0.01$, this gives a whopping 22.2% false positive rate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkDk-YKthoA2"
      },
      "source": [
        "alpha = 0.01\n",
        "1-(1-alpha)**25  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8_wvF6rby08"
      },
      "source": [
        "Actually things are a bit worse. The bitemporal tests are manifestly not independent because consecutive tests have one image in common. The best one can say in this situation is \n",
        " \n",
        "$$\n",
        "\\alpha_T \\le (k-1)\\alpha, \\tag{3.2}\n",
        "$$\n",
        " \n",
        "or $\\alpha_T \\le 25\\%$ for $k=26$ and $\\alpha=0.01$ . If we wish to set a false positive rate of at most, say, 1% for the entire series, then each bitemporal test must have a significance level of $\\alpha=0.0004$ and a correspondingly large false negative rate $\\beta$. In other words  many significant changes may be missed. \n",
        "\n",
        "How to proceed? Perhaps by being a bit less ambitious at first and asking the simpler question: _Were there any changes at all over the interval?_ If the answer is affirmative, we can worry about how many there were and when they occurred later. Let's formulate this question as ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGqBb29T2bWW"
      },
      "source": [
        "### An omnibus test for change\n",
        " \n",
        "We'll start again with the easier  single polarization case. For the series of _VV_ intensity images acquired at times $t_1, t_2,\\dots t_k$, our null hypothesis is that, at a given pixel position,  there has been no change in the signal strengths $a_i=\\langle|S^{a_i}_{vv}|^2\\rangle$ over the entire period, i.e.,\n",
        " \n",
        "$$\n",
        "H_0:\\quad a_1 = a_2 = \\dots = a_k = a.\n",
        "$$\n",
        " \n",
        "The alternative hypothesis is that there was at least one change (and possibly many) over the interval. For the more mathematically inclined this can be witten succinctly as\n",
        " \n",
        "$$\n",
        "H_1:\\quad \\exists\\ i,j :\\ a_i \\ne a_j,\n",
        "$$\n",
        " \n",
        "which says _there exist indices $i, j$ for which $a_i$ is not equal to_ $a_j$.\n",
        " \n",
        "Again, the likelihood functions are products of gamma distributions:\n",
        " \n",
        "$$\n",
        "L_1(a_1,\\dots,a_k) =\\prod_{i=1}^k p(s_i\\mid a_i) = {1\\over\\Gamma(m)^k}\\left[\\prod_i{a_i\\over m}\\right]^{-m}\\left[\\prod_i s_i\\right]^{m-1}\\exp(-m\\sum_i{s_i\\over a_i}) \\tag{3.3}\n",
        "$$\n",
        " \n",
        "$$\n",
        "L_0(a)  = \\prod_{i=1}^k p(s_i\\mid a) = {1\\over\\Gamma(m)^k} \\left[{a\\over m}\\right]^{-mk}\\left[\\prod_i s_i\\right]^{m-1}\\exp(-{m\\over a}\\sum_i s_i) \\tag{3.4}\n",
        "$$\n",
        " \n",
        "and $L_1$ is maximized for $\\hat a_i = s_i,\\ i=1\\dots k,$ while $L_0$ is maximized for $\\hat a = {1\\over k}\\sum_i s_i$. So with a bit of simple algebra our likelihood ratio test statistic is\n",
        " \n",
        "$$\n",
        "Q_k = {L_0(\\hat a)\\over L_1(\\hat a_1,\\dots,\\hat a_k)} = \\left[k^k{\\prod_i s_i\\over (\\sum_i s_i)^k}\\right]^m \\tag{3.5}\n",
        "$$\n",
        " \n",
        "and is called an _omnibus test statistic_. Note that, for $k=2$, we get the bitemporal LRT given by Eq. (2.10).\n",
        " \n",
        "We can't expect to find an analytical expression for the probability distribution of this LRT statistic, so we will again invoke Wilks' Theorem and work with\n",
        " \n",
        "$$\n",
        "-2 \\log{Q_k} = \\big[k\\log{k}+\\sum_i\\log{s_i}-k\\log{\\sum_i s_i}\\big](-2m) \\tag{3.6}\n",
        "$$\n",
        " \n",
        "According to Wilks, it should be approximately chi square distributed with $k-1$ degrees of freedom under $H_0$. (Why?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmEjLX1tGs1K"
      },
      "source": [
        "The input cell below evaluates the test statistic Eq. (3.6) for a list of single polarization images. We prefer from now on to use as default the equivalent number of looks 4.4 that we discussed at the end of Part 1 rather than the actual number of looks $m=5$, in the hope of getting a better agreement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z2EGvCDD1ec"
      },
      "source": [
        "def omnibus(im_list, m = 4.4):\n",
        "    '''omnibus test statistic, monovariate case'''\n",
        "    def log(current):\n",
        "        return ee.Image(current).log()\n",
        "\n",
        "    im_list = ee.List(im_list)\n",
        "    k = im_list.length() \n",
        "    klogk = k.multiply(k.log())\n",
        "    klogk = ee.Image.constant(klogk)\n",
        "    sumlogs = ee.ImageCollection(im_list.map(log)).reduce(ee.Reducer.sum())\n",
        "    logsum = ee.ImageCollection(im_list).reduce(ee.Reducer.sum()).log()\n",
        "    return klogk.add(sumlogs).subtract(logsum.multiply(k)).multiply(-2*m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJmqoSgrJZDn"
      },
      "source": [
        "Let's see if this test statistic does indeed follow the chi square distribution. First we define small polygon _aoi_sub_ over the Thorne Moors (on the eastern side of the AOI) for which we hope there are few significant changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXBkOhWQJd0_"
      },
      "source": [
        "geoJSON = {\n",
        "  \"type\": \"FeatureCollection\",\n",
        "  \"features\": [\n",
        "    {\n",
        "      \"type\": \"Feature\",\n",
        "      \"properties\": {},\n",
        "      \"geometry\": {\n",
        "        \"type\": \"Polygon\",\n",
        "        \"coordinates\": [\n",
        "          [\n",
        "            [\n",
        "              -0.9207916259765625,\n",
        "              53.63649628489509\n",
        "            ],\n",
        "            [\n",
        "              -0.9225082397460938,\n",
        "              53.62550271303527\n",
        "            ],\n",
        "            [\n",
        "              -0.8892059326171875,\n",
        "              53.61022911107819\n",
        "            ],\n",
        "            [\n",
        "              -0.8737564086914062,\n",
        "              53.627538775780984\n",
        "            ],\n",
        "            [\n",
        "              -0.9207916259765625,\n",
        "              53.63649628489509\n",
        "            ]\n",
        "          ]\n",
        "        ]\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "coords = geoJSON['features'][0]['geometry']['coordinates']\n",
        "aoi_sub = ee.Geometry.Polygon(coords)\n",
        "\n",
        "location = aoi.centroid().coordinates().getInfo()[::-1]\n",
        "mp = folium.Map(location=location, zoom_start=11, width=1000)\n",
        "mp.add_ee_layer(rgb_images.clip(aoi_sub), {'min': -20,'max': 0}, 'aoi_sub rgb composite')\n",
        "mp.add_child(folium.LayerControl())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LZnZc2AIne2"
      },
      "source": [
        "Here is a comparison for pixels in _aoi_sub_ with the chi square distribution with $k-1$ degrees of freedom. We choose the first 10 images in the series ($k=10$) because we expect fewer changes in September/October than over the complete sequence $k=24$, which extends into January. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "his4vdFXt8l2"
      },
      "source": [
        "k = 10\n",
        "hist = omnibus(vv_list.slice(0,k)) \\\n",
        "         .reduceRegion(ee.Reducer.fixedHistogram(0, 40, 200),geometry=aoi_sub,scale=10) \\\n",
        "         .get('constant') \\\n",
        "         .getInfo()\n",
        " \n",
        "a = np.array(hist)\n",
        "x = a[:,0]\n",
        "y = a[:,1]/np.sum(a[:,1])\n",
        "plt.plot(x, y, '.', label='data')\n",
        "plt.plot(x, chi2.pdf(x, k-1)/5, '-r', label='chi square')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgZuh94UsEZf"
      },
      "source": [
        "It appears that Wilks' Theorem is again a fairly good approximation. So why not generate a change map for the full series? The good news is that we now have the overall false positive probability $\\alpha$ under control. Here we set it to $\\alpha=0.01$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL4N3GednWs9"
      },
      "source": [
        "# The change map for alpha = 0.01.\n",
        "k = 26; alpha = 0.01\n",
        "p_value = ee.Image.constant(1).subtract(chi2cdf(omnibus(vv_list),k-1))\n",
        "c_map = p_value.multiply(0).where(p_value.lt(alpha),1)\n",
        "# Make the no-change pixels transparent.\n",
        "c_map = c_map.updateMask(c_map.gt(0))    \n",
        "# Overlay onto the folium map.\n",
        "location = aoi.centroid().coordinates().getInfo()[::-1]\n",
        "mp = folium.Map(location=location, zoom_start=11, width=1000)\n",
        "mp.add_ee_layer(c_map, {'min': 0,'max': 1, 'palette': ['black','red']}, 'change map')\n",
        "mp.add_child(folium.LayerControl())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OFU7fgvtzCm"
      },
      "source": [
        "So plenty of changes, but hard to interpret considering the time span. Although we can see _where_ changes took place, we know neither _when_ they occurred nor their _multiplicity_. Also there is a matter that we have glossed over up until now, and that is ...\n",
        " \n",
        "### A question of scale\n",
        " \n",
        "The number of looks plays an important role in all of the formulae that we have discussed so far, and for the Sentinel-1 ground range detected imagery we first used $m=5$ and now the ENL $=4.4$.  When we display a change map interactively, the zoom factor determines the image pyramid level at which the GEE servers perform the required calculations and pass the result to the folium map client. If the calculations are not at the nominal scale of 10m then the number of looks is effectively larger than the ENL due to the averaging involved in constructing higher pyramid levels. The effect can be seen in the  output cell above: the number of change pixels seems to decrease when we zoom out. There is no problem when we export our results to GEE assets, to Google Drive or to Cloud storage, since we can simply choose the correct nominal scale for export. \n",
        " \n",
        "In order to see the changes correctly at all zoom levels, we can force GEE to work at the nominal scale by reprojecting before displaying on the map:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shwzGq2tWMva"
      },
      "source": [
        "c_map_10m = c_map.reproject(c_map.projection().crs(), scale=10)\n",
        "mp = folium.Map(location=location, zoom_start=11, width=1000)\n",
        "mp.add_ee_layer(c_map, {'min': 0,'max': 1, 'palette': ['black','red']}, 'Change map')\n",
        "mp.add_ee_layer(c_map_10m, {'min': 0,'max': 1, 'palette': ['black','blue']}, 'Change map (10m)')\n",
        "\n",
        "mp.add_child(folium.LayerControl())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTkiTAIWLxpU"
      },
      "source": [
        "You will notice in the output cell above that the calculation at nominal scale (the blue pixels) now takes considerably longer to complete. Also some red pixels are not completely covered by blue ones. Those changes are a spurious result of the falsified number of looks. Nevertheless for quick previewing purposes we might prefer to do without the reprojection. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9mlvH7oUtJe"
      },
      "source": [
        "### A sequential omnibus test\n",
        "Recalling the last remark at the end of Part 2, let's now guess the omnibus LRT for the dual polarization case. From Eq. (3.5), replacing $s_i \\to|c_i|$,  $\\ \\sum s_i \\to |\\sum c_i|\\ $ and $k^k \\to k^{2k}$, we get \n",
        " \n",
        "$$\n",
        "Q_k =  \\left[k^{2k}{\\prod_i |c_i|\\over |\\sum_i c_i|^k}\\right]^m. \\tag{3.7}\n",
        "$$\n",
        "\n",
        "This is in fact a special case of a more general omnibus test statistic\n",
        "\n",
        "$$\n",
        "Q_k =  \\left[k^{pk}{\\prod_i |c_i|\\over |\\sum_i c_i|^k}\\right]^m\n",
        "$$\n",
        "\n",
        "which holds for $p\\times p$ polarimetric covariance matrix images, for example for the full dual pol matrix  Eq. (1.5) or for full $3\\times 3$ quad pol matrices ($p=3$), but also for diagonal $2\\times 2$ and $3\\times 3$ matrices.\n",
        " \n",
        "Which brings us to the **heart of this Tutorial**. We will now decompose Eq. (3.7) into a product of independent likelihood ratio tests which will enable us to determine when changes occurred at each pixel location. Then we'll code a complete multitemporal change detection algorithm on the GEE Python API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tePZxpVI0Tkg"
      },
      "source": [
        " \n",
        "#### Single polarization\n",
        " \n",
        " \n",
        " \n",
        "Rather than make a formal derivation, we will illustrate the decomposition on a series of $k=5$ single polarization (VV) measurements. The omnibus test Eq. (3.5) for any change over the series from $t_1$ to $t_5$ is\n",
        " \n",
        "$$\n",
        "Q_5 = \\left[ 5^5 {s_1s_2s_3s_4s_5\\over (s_1+s_2+s_3+s_4+s_5)^5}\\right]^m.\n",
        "$$\n",
        " \n",
        "If we accept the null hypothesis $a_1=a_2=a_3=a_4=a_5$ we're done and can move on to the next pixel (figuratively of course, since this stuff is all done in parallel). But suppose we have rejected the null hypothesis, i.e., there was a least one significant change. In order to find it (or them), we begin by testing the first of the four intervals. That's just the bitemporal test from Part 2, but let's call it $R_2$ rather than $Q_2$,\n",
        " \n",
        "$$\n",
        "R_2 = \\left[ 2^2 {s_1s_2\\over (s_1+s_2)^2}\\right]^m.\n",
        "$$\n",
        " \n",
        "Suppose we conclude no change, that is, $a_1=a_2$. Now we don't do just another bitemporal test on the second interval. Instead we test the hypothesis\n",
        " \n",
        "\\begin{align*}\n",
        "H_0:\\ & a_1=a_2= a_3\\ (=a)\\cr\n",
        "{\\rm against}\\quad H_1:\\  &a_1=a_2\\ (=a) \\ne a_3.\n",
        "\\end{align*}\n",
        " \n",
        "So the alternative hypothesis is: _There was no change in the first interval **and** there was a change in the second interval_. The LRT is easy to derive, but let's go through it anyway.\n",
        " \n",
        "\\begin{align*}\n",
        "        {\\rm From\\ Eq.}\\ (3.4):\\  &L_0(a)  = {1\\over\\Gamma(m)^3} \\left[{a\\over m}\\right]^{-3m}\\left[s_1s_2s_3\\right]^{m-1}\\exp(-{m\\over a}(s_1+s_2+s_3)  \\cr\n",
        "        &\\hat a = {1\\over 3}(s_1+s_2+s_3) \\cr\n",
        "=>\\           &L_0(\\hat a) = {1\\over\\Gamma(m)^3} \\left[{s_1+s_2+s_3\\over 3m}\\right]^{-3m}\\left[s_1s_2s_3\\right]^{m-1} \\exp(-3m) \\cr\n",
        "{\\rm From\\ Eq.}\\ (3.3):\\ &L_1(a_1,a_2,a_3) = {1\\over\\Gamma(m)^3}\\left[a_1a_2a_3\\over m\\right]^{-m}[s_1s_2s_3]^{m-1}\\exp(-m(s_1/a_1+s_2/a_2+s_3/a_3)\\cr\n",
        "&\\hat a_1 = \\hat a_2 = {1\\over 2}(s_1+s_2),\\quad \\hat a_3 = s_3 \\cr\n",
        "=>\\ &L_1(\\hat a_1,\\hat a_2, \\hat a_3) = {1\\over\\Gamma(m)^3}\\left[(s_1+s_2)^2s_3\\over 2^2m \\right]^{-m}[s_1s_2s_3]^{m-1}\\exp(-3m)\n",
        "\\end{align*}\n",
        " \n",
        "And, taking the ratio $L_0/L_1$of the maximum likelihoods,\n",
        " \n",
        "$$\n",
        "R_3 = \\left[{3^3\\over 2^2}{(s_1+s_2)^2s_3\\over (s_1+s_2+s_3)^3}\\right]^m.\n",
        "$$\n",
        " \n",
        "Not too hard to guess that, if we accept $H_0$ again, we go on to test\n",
        "\n",
        "\\begin{align*}\n",
        "H_0:\\ a_1=a_2=a_3=a_4\\ (=a)\\cr\n",
        "{\\rm against}\\quad H_1:\\ a_1=a_2=a_3\\ (=a) \\ne a_4.\n",
        "\\end{align*}\n",
        "\n",
        "with LRT statistic\n",
        " \n",
        "$$\n",
        "R_4 = \\left[{4^4\\over 3^3}{(s_1+s_2+s_3)^3s_4\\over (s_1+s_2+s_3+s_4)^4}\\right]^m,\n",
        "$$\n",
        " \n",
        "and so on to $R_5$ and the end of the time series. \n",
        "\n",
        "Now for the cool part (try it out yourself):\n",
        " \n",
        "$$\n",
        "R_2\\times R_3\\times R_4 \\times R_5 = Q_5.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twkipaPuT1qP"
      },
      "source": [
        " \n",
        "So, generalizing to a series of length $k$: \n",
        " \n",
        "**The omnibus test statistic $Q_k$ may be factored into the product of  LRT's $R_j$ which test for homogeneity in the measured reflectance signal up to and including time $t_j$, assuming homegeneity up to time $t_{j-1}$:**\n",
        " \n",
        "$$\n",
        "Q_k = \\prod_{j=2}^k R_j, \\quad R_j = \\left[{j^j\\over (j-1)^{j-1}}{(s_1+\\dots +s_{j-1})^{j-1}s_j\\over (s_1+\\dots +s_j)^j}\\right]^m,\\quad j = 2\\dots k.  \\tag{3.8}\n",
        "$$\n",
        " \n",
        "Moreover the test statistics $R_j$ are stochastically independent under $H_0$. \n",
        "This can be shown analytically, see [Conradsen et al. (2016)](https://ieeexplore.ieee.org/document/7398022) or P. 405 in my [textbook](https://www.taylorfrancis.com/books/9780429464348), but we'll show it here empirically by sampling the test statistics $R_j$ in the region _aoi\\_sub_ and examining the correlation matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJTNfKKSRIZ2"
      },
      "source": [
        "def list_iter(current,prev):\n",
        "    '''Iteration algorithm for sampling the test statistics'''    \n",
        "#  Get current index i from the list. \n",
        "    j = ee.Number(current)\n",
        "#  Extract aoi, vv_list and list of samples from previous iteration.\n",
        "    prev = ee.Dictionary(prev)\n",
        "    aoi = prev.get('aoi')\n",
        "    vv_list = ee.List(prev.get('vv_list'))\n",
        "    samples = ee.List(prev.get('samples'))\n",
        "#  Get the factors in the expression for Rj.    \n",
        "    sj = vv_list.get(j.subtract(1))\n",
        "    jfact = j.pow(j).divide(j.subtract(1).pow(j.subtract(1)))\n",
        "    sumj = ee.ImageCollection(vv_list.slice(0,j)).reduce(ee.Reducer.sum())\n",
        "    sumjm1 = ee.ImageCollection(vv_list.slice(0,j.subtract(1))).reduce(ee.Reducer.sum())\n",
        "#  Put them together.     \n",
        "    Rj = sumjm1.pow(j.subtract(1)).multiply(sj).multiply(jfact).divide(sumj.pow(j)).pow(5)    \n",
        "#  Sample Rj.    \n",
        "    sample = Rj.clip(aoi) \\\n",
        "               .sample(scale=10, numPixels=1000, seed=123) \\\n",
        "               .aggregate_array('VV_sum')\n",
        "#  Return the prev dictionary, appending the current sample to the list of samples.                  \n",
        "    return ee.Dictionary({'aoi': aoi, 'vv_list': vv_list, 'samples': samples.add(sample)})\n",
        " \n",
        "# Initialize the iteration with an empty list of samples and aoi_sub.\n",
        "first = ee.Dictionary({'aoi': aoi_sub, 'vv_list': vv_list, 'samples': ee.List([])})\n",
        "# Iterate over the first few list indices.\n",
        "result = ee.List.sequence(2,8).iterate(list_iter,first)\n",
        "# Extract the samples list.\n",
        "samples = ee.List(ee.Dictionary(result).get('samples'))\n",
        "# Calculate and display the correlation matrix.\n",
        "np.set_printoptions(precision=2, suppress=True)\n",
        "print( np.corrcoef(samples.getInfo()) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1wKex1oFrqU"
      },
      "source": [
        "The off-diagonal elements are mostly small. The not-so-small values can be attributed sampling error or to the presence of some change pixels in the samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFzoRyIOU5Rx"
      },
      "source": [
        "#### Dual polarization and an algorithm\n",
        "\n",
        "With our substitution trick, we can now write down the sequential test for   the dual polarization (bivariate) image time series. From Eq. (3.8) we get\n",
        "\n",
        "$$\n",
        "Q_k = \\prod_{j=2}^k R_j , \\quad R_j = \\left[{j^{2j}\\over (j-1)^{2(j-1)}}{|c_1+\\dots +c_{j-1}|^{j-1}|c_j|\\over |c_1+\\dots +c_j|^j}\\right]^m,\\quad j = 2\\dots k. \\tag{3.9}\n",
        "$$\n",
        "\n",
        "And of course we have again to use Wilks' Theorem to get the _P_ values, so we work with\n",
        "\n",
        "$$\n",
        "-2\\log{R_j} = -2m\\Big[2(j\\log{j}-(j-1)\\log(j-1)+(j-1)\\log\\Big|\\sum_{i=1}^{j-1}c_i \\Big|+\\log|c_j|-j\\log\\Big|\\sum_{i=1}^j c_i\\Big|\\ \\Big] \\tag{3.10a}\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "-2\\log Q_k = \\sum_{j=2}^k -2\\log R_j. \\tag{3.10b}\n",
        "$$\n",
        "\n",
        "The statistic $-2\\log R_j$ is approximately chi square distributed with two degrees of freedom. Similarly $-2\\log Q_\\ell$ is approximately chi square distributed with $2(k-1)$ degrees of freedom. Readers should satisfy themselves that these numbers are indeed the correct, taking into account that each measurement $c_i$ has two free parameters $|S^a_{vv}|^2$ and $|S^b_{vh}|^2$, see Eq. (2.13).\n",
        "\n",
        "Now for the algorithm:\n",
        "\n",
        "**The sequential omnibus change detection algorithm**\n",
        "\n",
        "With a time series of $k$ SAR images $(c_1,c_2,\\dots,c_k)$,\n",
        "\n",
        "1.   Set $\\ell = k$.\n",
        "2.   Set $s = (c_{k-\\ell+1}, \\dots c_k)$.\n",
        "3.   Perform the omnibus test $Q_\\ell$ for any changes change over $s$.\n",
        "4.   If no significant changes are found, stop.\n",
        "5.   Successively test series $s$ with $R_2, R_3, \\dots$ until the first significant change is met for $R_j$.\n",
        "6.   Set $\\ell = k-j+1$ and go to 2.  \n",
        "\n",
        "|Table 3.1 |       |       |       |       |       |        |\n",
        "|----------|-------|-------|-------|-------|-------|--------|\n",
        "|  $\\ell$  | $c_1$ | $c_2$ | $c_3$ | $c_4$ | $c_5$ |        |\n",
        "| 5        |       | $R^5_2$ | $R^5_3$ | $R^5_4$ | $R^5_5$ | $Q_5$  |\n",
        "| 4        |       |       | $R^4_2$ | $R^4_3$ | $R^4_4$ | $Q_4$  |\n",
        "| 3        |       |       |       | $R^3_2$ | $R^3_3$ | $Q_3$  |\n",
        "| 2        |       |       |       |       | $R^2_2$ | $Q_2$  | \n",
        "\n",
        "\n",
        "Thus if a change is found, the series is truncated up to the point of change and the testing procedure is repeated for the rest of the series. Take for example a series of $k=5$ images. (See Table 3.1 where, to avoid ambuguity, we add superscript $\\ell$ to each $R_j$ test). Suppose there is one change in the second interval only. Then the test sequence is (the asterisk means $H_0$ is rejected)\n",
        "\n",
        "$$\n",
        "Q^*_5 \\to R^5_2 \\to R^{5*}_3 \\to Q_3.\n",
        "$$\n",
        "\n",
        "If there are changes in the second and last intervals,\n",
        "\n",
        "$$\n",
        "Q^*_5 \\to R^5_2 \\to R^{5*}_3 \\to Q^*_3 \\to R^3_2 \\to R^{3*}_3,\n",
        "$$\n",
        "\n",
        "and if there are significant changes in all four intervals,\n",
        "\n",
        "$$\n",
        "Q^*_5 \\to R^{5*}_2 \\to Q^*_4 \\to R^{4*}_2 \\to Q^*_3 \\to R^{3*}_2 \\to Q^*_2.\n",
        "$$\n",
        "\n",
        "The approach taken in the coding of this algorithm is to pre-calculate  _P_ values for all of the $Q_\\ell / R_j$ tests and then, in a second pass, to filter them to determine the points of change.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0fpZbEfthH3"
      },
      "source": [
        "#### Pre-calculating the _P_ value array\n",
        "\n",
        "The following code cell performs a double iteration on the indices $\\ell$ and $j$, returning an array of _P_ values for all possible LRT statistics. For example again for $k=5$, the code calculates the _P_ values for each $R_j$ entry in Table 3.1 as a list of lists. Before calculating each row, the time series $c_1, c_2,c_3,c_4, c_5$ is sliced from $k-\\ell+1$ to $k$. The last entry in each row is simply the product of the other entries,  $Q_\\ell =\\prod_{j=2}^\\ell R_j.$\n",
        "\n",
        "The program actually operates on the logarithms of the test statistics, Equations (3.10).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DDfm-FxtylT"
      },
      "source": [
        "def log_det_sum(im_list, j):\n",
        "    '''Return log of determinant of the sum of the first j images in im_list.'''\n",
        "    im_ist = ee.List(im_list)\n",
        "    sumj = ee.ImageCollection(im_list.slice(0,j)).reduce(ee.Reducer.sum())                \n",
        "    return ee.Image(det(sumj)).log()  \n",
        "\n",
        "def log_det(im_list, j):\n",
        "    '''Return log of the determinant of the jth image in im_list.'''\n",
        "    im = ee.Image(ee.List(im_list).get(j.subtract(1)))\n",
        "    return ee.Image(det(im)).log()      \n",
        "\n",
        "def pval(im_list, j, m = 4.4):\n",
        "    '''Calculate -2logRj for im_list and return P value and -2logRj.'''\n",
        "    im_list = ee.List(im_list)\n",
        "    j = ee.Number(j)\n",
        "    m2logRj = log_det_sum(im_list,j.subtract(1)) \\\n",
        "             .multiply(j.subtract(1)) \\\n",
        "             .add(log_det(im_list,j))  \\\n",
        "             .add(ee.Number(2).multiply(j).multiply(j.log())) \\\n",
        "             .subtract(ee.Number(2).multiply(j.subtract(1)) \\\n",
        "             .multiply(j.subtract(1).log())) \\\n",
        "             .subtract(log_det_sum(im_list,j).multiply(j)) \\\n",
        "             .multiply(-2).multiply(m)\n",
        "    pv = ee.Image.constant(1).subtract(chi2cdf(m2logRj, 2))\n",
        "    return (pv, m2logRj)             \n",
        "\n",
        "def js_iter(current, prev):\n",
        "    '''Iteration function for j=2,3,...,l.'''\n",
        "    j = ee.Number(current) \n",
        "    prev = ee.Dictionary(prev)   \n",
        "    im_list = prev.get('im_list')\n",
        "    pvs = ee.List(prev.get('pvs'))\n",
        "    m2logQl = ee.Image(prev.get('m2logQl'))\n",
        "#  Get P value and -2logQj.     \n",
        "    pv, m2logRj = pval(im_list, j)\n",
        "#  Append to P value list  \n",
        "    pvs = pvs.add(pv)       \n",
        "#  Update -2logQl = sum of -2logQj    \n",
        "    m2logQl = m2logQl.add(m2logRj)\n",
        "    return ee.Dictionary({'im_list': im_list, 'pvs': pvs, 'm2logQl': m2logQl})\n",
        "\n",
        "def ells_iter(current, prev):\n",
        "    '''Iteration function for ell = k,k-1,...,2'''\n",
        "    ell = ee.Number(current)    \n",
        "    prev = ee.Dictionary(prev)\n",
        "    k = ee.Number(prev.get('k'))\n",
        "    im_list = ee.List(prev.get('im_list'))\n",
        "    pv_arr = ee.List(prev.get('pv_arr'))\n",
        "#  Slice the series from k-l+1 to k (image indices start from 0).\n",
        "    im_list_ell = im_list.slice(k.subtract(ell),k)\n",
        "#  Iterate over the sliced series for j = 2 to k-l+1.   \n",
        "    js = ee.List.sequence(2,ell)\n",
        "    first = ee.Dictionary({'im_list': im_list_ell, 'pvs': ee.List([]), \n",
        "                           'm2logQl': ee.Image.constant(0)})\n",
        "    result = ee.Dictionary(js.iterate(js_iter, first))\n",
        "#  Get the P values for m2logRj and m2logQl and append to P value array.  \n",
        "    pvs = ee.List(result.get('pvs'))\n",
        "    m2logQl = ee.Image(result.get('m2logQl'))\n",
        "    pvQl = ee.Image.constant(1).subtract(chi2cdf(m2logQl, ell.subtract(1).multiply(2)))\n",
        "    pvs = pvs.add(pvQl)\n",
        "    return ee.Dictionary({'k': k, 'im_list': im_list, 'pv_arr': pv_arr.add(pvs)})\n",
        "\n",
        "def p_values(im_list):\n",
        "    '''Pre-calculate the P-value array for a list if images'''       \n",
        "    im_list = ee.List(im_list)\n",
        "    k = im_list.length() \n",
        "#  Iterate over l = k to 2.\n",
        "    ells = ee.List.sequence(k,2,-1)    \n",
        "    first = ee.Dictionary({'k': k, 'im_list': im_list, 'pv_arr': ee.List([])})\n",
        "    result = ee.Dictionary(ells.iterate(ells_iter,first))\n",
        "#  Return the P value array ell = k,...,2, j = 2,...,l.\n",
        "    return result.get('pv_arr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx3XiJAEIpu7"
      },
      "source": [
        "#### Filtering the _P_ values\n",
        "\n",
        "|Table 3.2 |       |       |       |       |       |        |\n",
        "|----------|-------|-------|-------|-------|-------|--------|\n",
        "|$i\\ $ / $j$|      |     1 |     2 |     3 |     4 |        |\n",
        "| 1        |       | $P_2$ | $P_3$ | $P_4$ | $P_5$ | $P_{Q5}$  |\n",
        "| 2        |       |       | $P_2$ | $P_3$ | $P_4$ | $P_{Q4}$  |\n",
        "| 3        |       |       |       | $P_2$ | $P_3$ | $P_{Q3}$  |\n",
        "| 4        |       |       |       |       | $P_2$ | $P_{Q2}$  | \n",
        "\n",
        "The pre-calculated _P_ values in _pv_arr_ (shown schematically in Table 3.2 for $k=5$) are then scanned in nested iterations over indices $i$ and $j$ to determine the following thematic change maps:\n",
        "- cmap: the interval of the most recent change, one band, byte values $\\in [0,k-1]$,\n",
        "- smap: the interval of the first change, one band, byte values $\\in [0,k-1]$,\n",
        "- fmap: the number of changes, one band, byte values $\\in [0,k-1]$,\n",
        "- bmap: the changes in each interval, $\\ k-1$ bands, byte values $\\in [0,1]$).\n",
        "\n",
        "A boolean variable _median_ is included in the code. Its purpose is to reduce the salt-and-pepper effect in no-change regions, which is at least partly a consequence of the uniform distribution of the _P_ values under $H_0$ (see the section _A note on P values_ in Part 2). If _median_ is _True_, the _P_ values for each $Q_\\ell$ statistic are passed through a $5\\times 5$ median filter before being compared with the significance threshold. This is not statistically kosher but probably justifiable if one is only interested in large homogeneous changes, for example flood inundations or deforestation.\n",
        "\n",
        "Here is the code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1KBQwPWLYEI"
      },
      "source": [
        "def filter_j(current, prev):\n",
        "    '''Iteration function over j indices of pv_arr.'''\n",
        "    pv = ee.Image(current)\n",
        "    prev = ee.Dictionary(prev)\n",
        "    pvQ = ee.Image(prev.get('pvQ'))\n",
        "    i = ee.Number(prev.get('i'))\n",
        "    cmap = ee.Image(prev.get('cmap'))\n",
        "    smap = ee.Image(prev.get('smap'))\n",
        "    fmap = ee.Image(prev.get('fmap'))\n",
        "    bmap = ee.Image(prev.get('bmap'))\n",
        "    alpha = ee.Image(prev.get('alpha'))    \n",
        "    j = ee.Number(prev.get('j'))\n",
        "    cmapj = cmap.multiply(0).add(i.add(j).subtract(1))\n",
        "#  Check      Rj?            Ql?                  Row i?\n",
        "    tst = pv.lt(alpha).And(pvQ.lt(alpha)).And(cmap.eq(i.subtract(1)))\n",
        "#  Then update cmap\n",
        "    cmap = cmap.where(tst,cmapj)\n",
        "#  and fmap    \n",
        "    fmap = fmap.where(tst,fmap.add(1))\n",
        "#  and smap only if in first row.    \n",
        "    smap = ee.Algorithms.If(i.eq(1),smap.where(tst,cmapj),smap)\n",
        "#  Create bmap band and add it to bmap image.\n",
        "    idx = i.add(j).subtract(2)\n",
        "    tmp = bmap.select(idx)\n",
        "    bname = bmap.bandNames().get(idx)\n",
        "    tmp = tmp.where(tst,1)\n",
        "    tmp = tmp.rename([bname])    \n",
        "    bmap = bmap.addBands(tmp,[bname],True)    \n",
        "    return ee.Dictionary({'i':i,'j':j.add(1),'alpha': alpha,'pvQ': pvQ,\n",
        "                          'cmap': cmap, 'smap': smap, 'fmap': fmap, 'bmap':bmap})    \n",
        "\n",
        "def filter_i(current, prev):\n",
        "    '''Iteration function over row-indices of pv_arr'''\n",
        "    current = ee.List(current)\n",
        "    pvs = current.slice(0,-1 )\n",
        "    pvQ = ee.Image(current.get(-1))\n",
        "    prev = ee.Dictionary(prev)\n",
        "    i = ee.Number(prev.get('i'))\n",
        "    alpha = ee.Image(prev.get('alpha'))\n",
        "    median = prev.get('median')\n",
        "#  Filter Ql p value if desired.   \n",
        "    pvQ = ee.Algorithms.If(median, pvQ.focal_median(2.5), pvQ)\n",
        "    cmap = prev.get('cmap')\n",
        "    smap = prev.get('smap')\n",
        "    fmap = prev.get('fmap')\n",
        "    bmap = prev.get('bmap')\n",
        "    first = ee.Dictionary({'i': i, 'j': 1, 'alpha': alpha ,'pvQ': pvQ,\n",
        "                           'cmap': cmap, 'smap': smap, 'fmap': fmap, 'bmap': bmap})\n",
        "    result = ee.Dictionary(ee.List(pvs).iterate(filter_j, first))   \n",
        "    return ee.Dictionary({'i':i.add(1), 'alpha': alpha, 'median': median,\n",
        "                          'cmap': result.get('cmap'), 'smap': result.get('smap'), \n",
        "                          'fmap': result.get('fmap'), 'bmap': result.get('bmap')})       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjTNE7fC5YGl"
      },
      "source": [
        "The following function ties the two steps together:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iuzc7fZ-0s1P"
      },
      "source": [
        "def change_maps(im_list, median = False, alpha= 0.01):\n",
        "    '''Calculation of thematic change maps'''\n",
        "    k = im_list.length()\n",
        "#  Pre-calculate the P value array.    \n",
        "    pv_arr = ee.List(p_values(im_list))\n",
        "#  Filter P values for change maps    \n",
        "    cmap = ee.Image(im_list.get(0)).select(0).multiply(0)\n",
        "    bmap = ee.Image.constant(ee.List.repeat(0,k.subtract(1))).add(cmap)\n",
        "    alpha = ee.Image.constant(alpha)\n",
        "    first = ee.Dictionary({'i': 1, 'alpha': alpha, 'median': median, \n",
        "                          'cmap': cmap, 'smap': cmap, 'fmap': cmap, 'bmap': bmap})\n",
        "    return ee.Dictionary(pv_arr.iterate(filter_i, first))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz2FqH45CVpe"
      },
      "source": [
        "And now we run the algorithm and display the color-coded change map _cmap_ (blue early, red late):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4dvtMlna_8R"
      },
      "source": [
        "result = change_maps(im_list, median=True, alpha=0.05)\n",
        "\n",
        "# Extract the change maps and display\n",
        "cmap = ee.Image(result.get('cmap'))\n",
        "smap = ee.Image(result.get('smap'))\n",
        "fmap = ee.Image(result.get('fmap'))\n",
        "location = aoi.centroid().coordinates().getInfo()[::-1]\n",
        "palette = ['black','blue','cyan','yellow','red']\n",
        "mp = folium.Map(location=location, zoom_start=11, width=1000)\n",
        "mp.add_ee_layer(cmap, {'min': 0,'max': 23, 'palette': palette}, 'cmap')\n",
        "mp.add_ee_layer(smap, {'min': 0,'max': 23, 'palette': palette}, 'smap')\n",
        "mp.add_ee_layer(fmap, {'min': 0,'max': 23, 'palette': palette}, 'fmap')\n",
        "mp.add_child(folium.LayerControl())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lq1VHnoI23L"
      },
      "source": [
        "#### Post-processing: The Loewner order\n",
        " \n",
        "The above change maps are still difficult to interpret. But what about _bmap_, the map of changes detected in each interval? Before we look at them it makes sense to include the direction of change, i.e., the [Loewner order](https://ieeexplore.ieee.org/document/8736751), see Part 2. In the event of significant change at time $j$, we can simply determine the positive or negative definiteness (or indefiniteness) of the difference between consecutive covariance matrix pixels\n",
        " \n",
        "$$\n",
        "c_j-c_{j-1},\\quad j = 2,\\dots,k,\n",
        "$$\n",
        " \n",
        "to get the change direction. But we can do better. Instead of subtracting the value for the preceding image, $c_{j-1}$, we can subtract the average over all values up to and including time $j-1$ for which no change has been signalled. For example for $k=5$, suppose there are significant changes in the first and fourth (last) interval. Then to get their directions we examine the differences\n",
        " \n",
        "$$\n",
        "c_2-c_1\\quad{\\rm and}\\quad c_5 - (c_2+c_3+c_4)/3.\n",
        "$$\n",
        "\n",
        "The running averages can be conveniently determined with the so-called _provisional means algortihm_. The average $\\bar c_i$ of the first $i$ images is calculated recursively as\n",
        "\n",
        "\\begin{align*}\n",
        "\\bar c_i &= \\bar c_{i-1} + (c_i - \\bar c_{i-1})/i \\cr\n",
        "\\bar c_1 &= c_1.\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "The function _dmap_iter_ below is iterated over the bands of _bmap_, replacing the values for changed pixels with\n",
        "\n",
        "-    1 for positive definite differences,\n",
        "-    2 for negative definite differences,\n",
        "-    3 for indefinite differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG9UoPHH29tk"
      },
      "source": [
        "def dmap_iter(current, prev):\n",
        "    '''post-process for directional change maps'''\n",
        "    prev = ee.Dictionary(prev)\n",
        "    j = ee.Number(prev.get('j'))\n",
        "    image = ee.Image(current) \n",
        "    avimg = ee.Image(prev.get('avimg'))\n",
        "    diff = image.subtract(avimg)    \n",
        "#  Get positive/negative definiteness.      \n",
        "    posd = ee.Image(diff.select(0).gt(0).And(det(diff).gt(0)))\n",
        "    negd = ee.Image(diff.select(0).lt(0).And(det(diff).gt(0)))\n",
        "    bmap = ee.Image(prev.get('bmap'))\n",
        "    bmapj = bmap.select(j)\n",
        "    dmap = ee.Image.constant(ee.List.sequence(1,3))\n",
        "    bmapj = bmapj.where(bmapj,dmap.select(2))\n",
        "    bmapj = bmapj.where(bmapj.And(posd), dmap.select(0))\n",
        "    bmapj = bmapj.where(bmapj.And(negd), dmap.select(1))  \n",
        "    bmap = bmap.addBands(bmapj, overwrite=True)\n",
        "#  Update avimg with provisional means.\n",
        "    i = ee.Image(prev.get('i')).add(1)\n",
        "    avimg = avimg.add(image.subtract(avimg).divide(i))\n",
        "#  reset avimg to current image and set i=1 if change occurred.\n",
        "    avimg = avimg.where(bmapj, image)\n",
        "    i = i.where(bmapj, 1)\n",
        "    return ee.Dictionary({'avimg': avimg,'bmap': bmap,'j': j.add(1),'i': i})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7h1GuQ79-jH"
      },
      "source": [
        "We only have to modify the _change_maps_ function to include the change direction in the _b_map_ image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF3-_d6M5JGf"
      },
      "source": [
        "def change_maps(im_list, median = False, alpha= 0.01):\n",
        "    '''Calculation of thematic change maps'''\n",
        "    k = im_list.length()\n",
        "#  Pre-calculate the P value array.    \n",
        "    pv_arr = ee.List(p_values(im_list))\n",
        "#  Filter P values for change maps    \n",
        "    cmap = ee.Image(im_list.get(0)).select(0).multiply(0)\n",
        "    bmap = ee.Image.constant(ee.List.repeat(0,k.subtract(1))).add(cmap)\n",
        "    alpha = ee.Image.constant(alpha)\n",
        "    first = ee.Dictionary({'i': 1, 'alpha': alpha, 'median': median, \n",
        "                          'cmap': cmap, 'smap': cmap, 'fmap': cmap, 'bmap': bmap})\n",
        "    result = ee.Dictionary(pv_arr.iterate(filter_i, first))\n",
        "#  Post-process bmap for change direction.\n",
        "    bmap =  ee.Image(result.get('bmap'))\n",
        "    avimg = ee.Image(im_list.get(0))\n",
        "    j = ee.Number(0)\n",
        "    i = ee.Image.constant(1)\n",
        "    first = ee.Dictionary({'avimg': avimg, 'bmap': bmap, 'j': j, 'i': i})\n",
        "    dmap = ee.Dictionary(im_list.slice(1).iterate(dmap_iter, first)).get('bmap')   \n",
        "    return ee.Dictionary(result.set('bmap',dmap))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL3TPfxXdFaQ"
      },
      "source": [
        "Because of the long delays when the zoom level is changed, it is a lot more convenient to export the change maps to GEE Assets and then examine them, either here in Colab or in the Code Editor. This also means the maps will be shown at the correct scale, irrespective of the zoom level. Here I export all of the change maps as a single image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsSpbk74RViE"
      },
      "source": [
        "# Run the algorithm with median filter and at 1% significance.\n",
        "result = ee.Dictionary(change_maps(im_list, median=True, alpha=0.01))\n",
        "# Extract the change maps and export to assets.\n",
        "cmap = ee.Image(result.get('cmap'))\n",
        "smap = ee.Image(result.get('smap'))\n",
        "fmap = ee.Image(result.get('fmap'))\n",
        "bmap = ee.Image(result.get('bmap'))\n",
        "cmaps = ee.Image.cat(cmap,smap,fmap,bmap).rename(['cmap','smap','fmap']+timestamplist[1:])\n",
        "#         Change |       this           | to your own username/path if you like.\n",
        "assetId = 'users/mortcanty/tutorial/cmaps'\n",
        "assexport = ee.batch.Export.image.toAsset(cmaps,\n",
        "                    description = 'assetExportTask', \n",
        "                    assetId = assetId, scale=10, maxPixels=1e9)      \n",
        "# Uncomment this for exporting.\n",
        "#assexport.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eBeTB6sQMbd"
      },
      "source": [
        "The asset  _users/mortcanty/tutorial/cmaps_ is shared so we can all access it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qseBmVOH7xPg"
      },
      "source": [
        "cmaps = ee.Image('users/mortcanty/tutorial/cmaps').updateMask(cmaps.gt(0))\n",
        "\n",
        "location = aoi.centroid().coordinates().getInfo()[::-1]\n",
        "palette = ['black','blue','cyan','yellow','red']\n",
        "palette1 = ['black','red','cyan','yellow']\n",
        "mp = folium.Map(location=location, zoom_start=13, width=1000)\n",
        "\n",
        "mp.add_ee_layer(cmaps.select('T20191107'), {'min': 0,'max': 3, 'palette': palette1}, 'T20191107')\n",
        "mp.add_ee_layer(cmaps.select('T20191113'), {'min': 0,'max': 3, 'palette': palette1}, 'T20191113')\n",
        "mp.add_ee_layer(cmaps.select('T20191119'), {'min': 0,'max': 3, 'palette': palette1}, 'T20191119')\n",
        "mp.add_ee_layer(cmaps.select('T20191125'), {'min': 0,'max': 3, 'palette': palette1}, 'T20191125')\n",
        "mp.add_ee_layer(cmaps.select('T20191201'), {'min': 0,'max': 3, 'palette': palette1}, 'T20191201')\n",
        "mp.add_ee_layer(cmaps.select('T20191207'), {'min': 0,'max': 3, 'palette': palette1}, 'T20191207')\n",
        "\n",
        "mp.add_child(folium.LayerControl())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFuLna6zQ_zt"
      },
      "source": [
        "Now interpretation is somewhat easier. The negative definite (cyan) changes which appear between Nov. 7 and Nov. 13 correspond to decreases in intensity of _VV_ and _VH_ reflectance and are due to wide-spread flooding. The positive definite changes (red), which gradually overlay the flooded areas in subsequent intervals, correspond to receding flood waters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WhBsXEjsZCU"
      },
      "source": [
        "### Outlook\n",
        "\n",
        "Without reliable ground truth we can't really claim that change maps of the kind we have just generated will be helpful for flood damage assessment or control, but their potential usefulness is quite obvious. In the next and final part of the Tutorial we will have a look at some more (possible) applications of sequential change detection with SAR imagery on GEE."
      ]
    }
  ]
}