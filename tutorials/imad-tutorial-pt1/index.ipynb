{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kdsGkYJXXKc"
      },
      "outputs": [],
      "source": [
        "#@title Copyright 2023 The Earth Engine Community Authors { display-mode: \"form\" }\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJrMq0aNrF-u"
      },
      "source": [
        "# Change Detection in Google Earth Engine - The MAD Transformation (Part 1)\n",
        "Author: mortcanty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xFjivyNFe7U"
      },
      "source": [
        "## Context\n",
        "\n",
        "With the great tools available in Google Earth Engine (GEE) it is easy to generate satellite image differences and time series animations which visualize changes on the Earth surface, sometimes quite dramatically. Such representations are very good as eye-openers to give a conceptual, overall impression of change. They may, however, be perceived differently from person to person and not allow more than subjective interpretation. With more objective and rigorous statistical methods one can often extract more and better change information on both pixel and patch/field levels.\n",
        "\n",
        "The Multivariate Alteration Detection (MAD) transformation was proposed and developed some time ago by [Allan Nielsen and his coworkers](https://www2.imm.dtu.dk/pubdb/pubs/1220-full.html) at the Technical University of Denmark and later [extended to an iteratively re-weighted version](https://www2.imm.dtu.dk/pubdb/pubs/4695-full.html) called IR-MAD or iMAD. The iMAD algorithm has since found widespread application for the generation of change information from visual/infrared imagery as well as for performing [relative radiometric normalization](http://www2.imm.dtu.dk/pubdb/pubs/5362-full.html) of image pairs.\n",
        "\n",
        "This tutorial is intended to familiarize GEE users with the iMAD method so that they may use it with confidence in their research wherever they think it might be appropriate. It is based on material taken from Chapter 9 of [Canty (2019)](https://www.taylorfrancis.com/books/image-analysis-classification-change-detection-remote-sensing-morton-john-canty/10.1201/9780429464348). In preparing the tutorial we have tried to take as much advantage as possible of the GEE platform to illustrate and demonstrate the theory interactively.\n",
        "\n",
        "## Outline\n",
        "\n",
        "The tutorial is split into three parts, of which this is the first:\n",
        "\n",
        "    1. The MAD Transformation\n",
        "    2. The iMAD Algorithm\n",
        "    3. Radiometric Normalization and Harmonization\n",
        "\n",
        "In Part 3, a convenient Jupyter widget interface for running the iMAD algorithm from a Docker container is also described.\n",
        "\n",
        "## Prerequisites\n",
        "Little prior knowledge is required to follow the discussion, apart from some familiarity with the GEE API and a basic, intuitive understanding of the simple statistical concepts of probability distributions and their moments, in particular,  mean, variance (var) and covariance (cov). Since we'll be working with multispectral imagery, in the course of the tutorial we'll frequently mention the so-called *dispersion matrix* or *variance-covariance matrix*, or simply *covariance matrix* for short. The covariance matrix of a set of measured variables $X_i,\\ i=1\\dots N$, such as the band-wise values of a multispectral image pixel, is defined as\n",
        "\n",
        "$$\n",
        "\\Sigma_X\\ =\\ \\begin{pmatrix} \\sigma^2_1 \u0026 \\sigma_{12} \u0026 \\dots \u0026 \\sigma_{1N} \\cr\n",
        "                \\sigma_{21} \u0026 \\sigma^2_2\u0026 \\dots \u0026 \\sigma_{2N} \\cr\n",
        "                  \\vdots    \u0026  \\vdots     \u0026 \\vdots\u0026  \\vdots     \\cr\n",
        "                \\sigma_{N1} \u0026 \\sigma_{N2} \u0026 \\dots \u0026  \\sigma^2_N \\end{pmatrix},\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\sigma_{ij}  = {\\rm cov}(X_i,X_j) = \\langle (X_i - \\bar X_i)(X_j - \\bar X_j)\\rangle, \\quad\n",
        "\\sigma^2_i =  {\\rm cov}(X_i,X_i) = {\\rm var}(X_i) = \\langle (X_i-\\bar X_i)^2\\rangle, \\quad \\ i,j = 1\\dots N.\n",
        "$$\n",
        "\n",
        "Here, $\\bar X_i$ denotes mean value and $\\langle\\dots\\rangle$ is ensemble average. The covariance matrix is symmetric about its principal diagonal. The *correlation* between $X_i$ and $X_j$ is defined as the normalized covariance\n",
        "\n",
        "$$\n",
        "\\rho_{ij} = {{\\rm cov}(X_i,X_j)\\over \\sqrt{{\\rm var}(X_i)}\\sqrt{{\\rm var}(X_j)}} = {\\sigma_{ij} \\over \\sigma_i\\sigma_j}.\n",
        "$$\n",
        "\n",
        "Since we are in a Colab environment we will use the Python API, but it is so similar to the JavaScript version that this should pose no problems to anyone not acquainted with it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cClM-tAWSnuS"
      },
      "source": [
        "## Preliminaries\n",
        "\n",
        "The cells below execute the necessary formalities for accessing Earth Engine from this Colab notebook. They also import the various Python add-ons needed, including the [geemap](https://geemap.org/) interactive map package, and define a few helper routines for displaying results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdyBUBN4F1IA"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "\n",
        "ee.Authenticate(auth_mode='notebook')\n",
        "ee.Initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqcghIcEIWnZ"
      },
      "outputs": [],
      "source": [
        "# Import other packages used in the tutorial\n",
        "%matplotlib inline\n",
        "import geemap\n",
        "import numpy as np\n",
        "import random, time\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm, chi2\n",
        "\n",
        "from pprint import pprint  # for pretty printing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z-Jcr3n8MhD"
      },
      "outputs": [],
      "source": [
        "# Truncate a 1-D array to dec decimal places\n",
        "def trunc(values, dec = 3):\n",
        "    return np.trunc(values*10**dec)/(10**dec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXWO7Qug8MhE"
      },
      "outputs": [],
      "source": [
        "# Display an image in a one percent linear stretch\n",
        "def display_ls(image, map, name, centered = False):\n",
        "    bns = image.bandNames().length().getInfo()\n",
        "    if bns == 3:\n",
        "        image = image.rename('B1', 'B2', 'B3')\n",
        "        pb_99 = ['B1_p99', 'B2_p99', 'B3_p99']\n",
        "        pb_1 = ['B1_p1', 'B2_p1', 'B3_p1']\n",
        "        img = ee.Image.rgb(image.select('B1'), image.select('B2'), image.select('B3'))\n",
        "    else:\n",
        "        image = image.rename('B1')\n",
        "        pb_99 = ['B1_p99']\n",
        "        pb_1 = ['B1_p1']\n",
        "        img = image.select('B1')\n",
        "    percentiles = image.reduceRegion(ee.Reducer.percentile([1, 99]), maxPixels=1e11)\n",
        "    mx = percentiles.values(pb_99)\n",
        "    if centered:\n",
        "        mn = ee.Array(mx).multiply(-1).toList()\n",
        "    else:\n",
        "        mn = percentiles.values(pb_1)\n",
        "    map.addLayer(img, {'min': mn, 'max': mx}, name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn5m_-sOpkDj"
      },
      "source": [
        "## Simple Differences\n",
        "Let's begin by considering two $N$-band optical/infrared images of the same scene acquired with the same sensor at two different times, between which ground reflectance changes have occurred at\n",
        "some locations but not everywhere. To see those changes we can \"flick back and forth\" between gray scale or RGB representations of the images on a computer display. Alternatively we can simply subtract one image from the other and, provided the intensities at the two time points have been calibrated or normalized in some way, examine the difference. Symbolically the difference image is\n",
        "\n",
        "$$\n",
        "Z = X-Y\n",
        "$$\n",
        "\n",
        "where\n",
        "$$\n",
        "X= \\begin{pmatrix}X_1\\cr X_2\\cr\\vdots\\cr X_N\\end{pmatrix},\\quad Y= \\begin{pmatrix}Y_1\\cr Y_2\\cr\\vdots\\cr Y_N\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "are 'typical' pixel vectors (more formally: _random vectors_) representing the first and second image, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX-pPWU9wO5I"
      },
      "source": [
        "### Landkreis Olpe\n",
        "\n",
        "For example, here is an area of interest (aoi) covering a heavily forested administrative district (Landkreis Olpe) in North Rhine Westphalia, Germany. Due to severe drought in recent years large swaths of shallow-root coniferous trees have died and been cleared away, leaving deciduous trees for the most part untouched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xs39zZmNSdru"
      },
      "outputs": [],
      "source": [
        "aoi = ee.FeatureCollection(\n",
        "    'projects/google/imad_tutorial/landkreis_olpe_aoi').geometry()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwCBEe49d-xx"
      },
      "source": [
        "We first collect two Sentinel-2 scenes which bracket some of the recent clear cutting (June, 2021 and June, 2022) and print out their timestamps. For demonstration purposes we choose top of atmosphere reflectance since the MAD transformation does not require absolute surface reflectances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzDg-puTSybq"
      },
      "outputs": [],
      "source": [
        "def collect(aoi, t1a ,t1b, t2a, t2b):\n",
        "    try:\n",
        "        im1 = ee.Image( ee.ImageCollection(\"COPERNICUS/S2_HARMONIZED\")\n",
        "                               .filterBounds(aoi)\n",
        "                               .filterDate(ee.Date(t1a), ee.Date(t1b))\n",
        "                               .filter(ee.Filter.contains(rightValue=aoi,leftField='.geo'))\n",
        "                               .sort('CLOUDY_PIXEL_PERCENTAGE')\n",
        "                               .first()\n",
        "                               .clip(aoi) )\n",
        "        im2 = ee.Image( ee.ImageCollection(\"COPERNICUS/S2_HARMONIZED\")\n",
        "                               .filterBounds(aoi)\n",
        "                               .filterDate(ee.Date(t2a), ee.Date(t2b))\n",
        "                               .filter(ee.Filter.contains(rightValue=aoi,leftField='.geo'))\n",
        "                               .sort('CLOUDY_PIXEL_PERCENTAGE')\n",
        "                               .first()\n",
        "                               .clip(aoi) )\n",
        "        timestamp = im1.date().format('E MMM dd HH:mm:ss YYYY')\n",
        "        print(timestamp.getInfo())\n",
        "        timestamp = im2.date().format('E MMM dd HH:mm:ss YYYY')\n",
        "        print(timestamp.getInfo())\n",
        "        return (im1, im2)\n",
        "    except Exception as e:\n",
        "        print('Error: %s'%e)\n",
        "\n",
        "im1, im2 = collect(aoi, '2021-06-01', '2021-06-30', '2022-06-01', '2022-06-30')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zekLrceuaNtK"
      },
      "source": [
        "In order to view the images, we instantiate an interactive map located at the center of the aoi. Since the map requires lat/long coordinates we have to reverse the long/lat order returned from the *coordinates()* method of the *ee.Geometry.centroid()* object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc_pwScrphK1"
      },
      "outputs": [],
      "source": [
        "# Interactive map\n",
        "M1 = geemap.Map()\n",
        "M1.centerObject(aoi, 11)\n",
        "\n",
        "M1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYKXgtJ2FdYP"
      },
      "source": [
        "Then we display RGB composites of the first 3 (visual) bands of the Sentinel images as well as their difference. We use the previously defined function *display_ls()* to display the images in a one percent linear stretch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8_al6RlBpNq"
      },
      "outputs": [],
      "source": [
        "visirbands = ['B2','B3','B4','B8','B11','B12']\n",
        "visbands = ['B2','B3','B4']\n",
        "\n",
        "diff = im1.subtract(im2).select(visbands)\n",
        "display_ls(im1.select(visbands), M1, 'Image 1')\n",
        "display_ls(im2.select(visbands), M1, 'Image 2')\n",
        "display_ls(diff, M1, 'Difference')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2d-vg-ezwpe"
      },
      "source": [
        "Small intensity differences (the gray-ish background in the difference image above) indicate no change, large\n",
        "positive (bright) or negative (dark) colors indicate change. The most obvious changes are in fact due to the clear cutting. Decision thresholds can be set to define significant changes. The thresholds are usually expressed in terms of standard deviations from the mean difference value, which is taken to correspond to no change. The per-band variances of the difference images are simply\n",
        "\n",
        "$$\n",
        "{\\rm var}(Z_i) = {\\rm var}(X_i - Y_i) = {\\rm var}(X_i) + {\\rm var}(Y_i) - 2\\cdot{\\rm cov}(X_i,Y_i),\\quad i=1\\dots N,\n",
        "$$\n",
        "\n",
        "or, if the bands are uncorrelated, about twice as noisy as the individual image bands. Normally $X_i$ and $Y_i$ are positively correlated (${\\rm cov}(X_i,Y_i) \u003e 0$) so the variance of $Z_i$ is somewhat smaller. When the difference signatures in the spectral channels are combined so as to try to characterize the nature of the changes that have taken place, one speaks of _spectral change vector analysis_.\n",
        "\n",
        "While the recent clear cuts are fairly easily identified as (some but not all of) the dark areas in the simple difference image above, it is possible to derive a better and more informative change map. This involves taking greater advantage of the statistical properties of the images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhMR6KLhkar5"
      },
      "source": [
        "## The MAD Transformation\n",
        "Let's make a linear combination of the intensities for all $N$ bands in the first image $X$, thus creating a scalar image\n",
        "\n",
        "$$\n",
        "U  = a_1X_1 + a_2X_2 + \\dots + a_NX_N = {\\bf a}^\\top X. \\tag{1}\n",
        "$$\n",
        "\n",
        "The symbol ${\\bf a}^\\top$ denotes the transpose of the column vector ${\\bf a}=\\begin{pmatrix}a_1\\cr a_2\\cr\\vdots\\cr a_N\\end{pmatrix}$, in other words the row vector ${\\bf a}^\\top = (a_1,a_2 \\dots a_N)$.\n",
        "\n",
        "\n",
        "The expression ${\\bf a}^\\top X$ is called an _inner vector product_. The vector of coefficients ${\\bf a}$ is as yet unspecified.\n",
        "We'll do the same for the second image $Y$, forming the linear combination\n",
        "\n",
        "$$\n",
        "V = b_1Y_1 + b_2Y_2 + \\dots + b_NY_N = {\\bf b}^\\top Y, \\tag{2}\n",
        "$$\n",
        "\n",
        "and then look at the scalar difference $U-V$. Change information\n",
        "is now contained, for the time being, in a single image rather than distributed among all $N$ bands.\n",
        "\n",
        "We have of course to choose the coefficient vectors ${\\bf a}$ and\n",
        "${\\bf b}$ in some suitable way. In [Nielsen et al. (1998)](https://www2.imm.dtu.dk/pubdb/pubs/1220-full.html) it is suggested that they be determined by making the transformed images $U$ and $V$ _as similar as they can be_ before taking their difference. This sounds at first counter-intuitive: Why make the images similar when we want to see the dissimilarities? The crux is that, in making them match one another by taking suitable linear combinations, we ensure that pixels in the transformed images $U$ and $V$ at which *indeed no change has taken place* will be as similar as possible before they are compared (subtracted). The genuine changes will then, it is hoped, be all the more apparent in the difference image.\n",
        "\n",
        "This process of \"similar making\" can be accomplished in statistics with standard [*Canonical Correlation Analysis* (CCA)](https://en.wikipedia.org/wiki/Canonical_correlation), a technique first described by Harold Hotelling in 1936. Canonical Correlation Analysis of the images represented by Equations (1) and (2) *maximizes the correlation*\n",
        "\n",
        "$$\n",
        "\\rho = {{\\rm cov}(U,V)\\over \\sqrt{{\\rm var}(U)}\\sqrt{{\\rm var}(V)}}, \\tag{3}\n",
        "$$\n",
        "\n",
        "between the random variables $U$ and $V$, which is just what we want.\n",
        "\n",
        "One technical point: Arbitrary multiples of $U$ and $V$ would clearly have the same correlation (the multiples will cancel off in numerator and denominator), so a constraint must be chosen. A convenient one is\n",
        "\n",
        "$$\n",
        "{\\rm var}(U)={\\rm var}(V)=1. \\tag{4}\n",
        "$$\n",
        "\n",
        "For those more versed in linear algebra the details of CCA are given in [Canty (2019)](https://www.taylorfrancis.com/books/image-analysis-classification-change-detection-remote-sensing-morton-john-canty/10.1201/9780429464348) beginning on page 385. It all boils down to solving two so-called *generalized eigenvalue problems* for determining the transformation vectors ${\\bf a}$ and ${\\bf b}$ in Equations (1) and (2), respectively. There is not just one solution but rather $N$ solutions. They consist of the $N$ pairs of _eigenvectors_ $({\\bf a}^i, {\\bf b}^i)$,\n",
        "\n",
        "$$\n",
        "{\\bf a}^i=\\begin{pmatrix}a_1\\cr a_2\\cr\\vdots\\cr a_N \\end{pmatrix}_i,\\quad {\\bf b}^i=\\begin{pmatrix}b_1\\cr b_2\\cr\\vdots\\cr b_N \\end{pmatrix}_i, \\quad i=1\\dots N\n",
        "$$\n",
        "\n",
        "and, correspondingly, the $N$ pairs of *canonical variates* $(U_i, V_i),\\ i= 1\\dots N$. (Readers familiar with Principal Components Analysis (PCA) will recognize the similarity. When applying PCA to an N-band image, an ordinary (rather than a generalized) eigenvalue problem is solved to maximize variance, resulting in $N$ principal component bands.)\n",
        "\n",
        "Unlike the bands of the original images $X$ and $Y$, which are ordered by spectral wavelength, the canonical variates are ordered by similarity or correlation. The difference images\n",
        "\n",
        "$$\n",
        "M_i = U_i - V_i = ({\\bf a}^i)^\\top X - ({\\bf b}^i)^\\top Y,\\quad i=1\\dots N, \\tag{5}\n",
        "$$\n",
        "\n",
        "contain the change information and are called the *MAD variates*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePBGinhxsSg_"
      },
      "source": [
        "### Auxiliary functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0j6--qcsSg_"
      },
      "source": [
        "To run the MAD transformation on GEE image pairs we need some helper routines. These are coded below. The first, called *covarw()*, returns the weighted covariance matrix of an $N$-band image as well as a weighted, centered (mean-zero) version of the image. Why it is important to weight the pixels before centering or calculating the covariance matrix will become apparent in Part 2 of the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJXqZ3ZJsSg_"
      },
      "outputs": [],
      "source": [
        "def covarw(image, weights = None, scale = 20, maxPixels = 1e10):\n",
        "    '''Return the weighted centered image and its weighted covariance matrix'''\n",
        "    try:\n",
        "        geometry = image.geometry()\n",
        "        bandNames = image.bandNames()\n",
        "        N = bandNames.length()\n",
        "        if weights is None:\n",
        "            weights = image.constant(1)\n",
        "        weightsImage = image.multiply(ee.Image.constant(0)).add(weights)\n",
        "        means = image.addBands(weightsImage) \\\n",
        "                    .reduceRegion(ee.Reducer.mean().repeat(N).splitWeights(),\n",
        "                                scale = scale,\n",
        "                                maxPixels = maxPixels) \\\n",
        "                    .toArray() \\\n",
        "                    .project([1])\n",
        "        centered = image.toArray().subtract(means)\n",
        "        B1 = centered.bandNames().get(0)\n",
        "        b1 = weights.bandNames().get(0)\n",
        "        nPixels = ee.Number(centered.reduceRegion(ee.Reducer.count(),\n",
        "                                                scale = scale,\n",
        "                                                maxPixels = maxPixels).get(B1))\n",
        "        sumWeights = ee.Number(weights.reduceRegion(ee.Reducer.sum(),\n",
        "                                                    geometry = geometry,\n",
        "                                                    scale = scale,\n",
        "                                                    maxPixels = maxPixels).get(b1))\n",
        "        covw = centered.multiply(weights.sqrt()) \\\n",
        "                    .toArray() \\\n",
        "                    .reduceRegion(ee.Reducer.centeredCovariance(),\n",
        "                                    geometry = geometry,\n",
        "                                    scale = scale,\n",
        "                                    maxPixels = maxPixels) \\\n",
        "                    .get('array')\n",
        "        covw = ee.Array(covw).multiply(nPixels).divide(sumWeights)\n",
        "        return (centered.arrayFlatten([bandNames]), covw)\n",
        "    except Exception as e:\n",
        "        print('Error: %s'%e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEtccjvpsShA"
      },
      "source": [
        "The second routine, called _corr()_, transforms a covariance matrix to the equivalent correlation matrix by dividing by the square roots of the variances, see Equation (3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olCi6RvJsShA"
      },
      "outputs": [],
      "source": [
        "def corr(cov):\n",
        "    '''Transform covariance matrix to correlation matrix'''\n",
        "    # diagonal matrix of inverse sigmas\n",
        "    sInv = cov.matrixDiagonal().sqrt().matrixToDiag().matrixInverse()\n",
        "    # transform\n",
        "    corr = sInv.matrixMultiply(cov).matrixMultiply(sInv).getInfo()\n",
        "    # truncate\n",
        "    return [list(map(trunc, corr[i])) for i in range(len(corr))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G34S_Z2gQAwp"
      },
      "source": [
        "For example the code below calculates the (unweighted) covariance matrix of the visual and infrared bands of the image _im1_ and prints out the corresponding correlation matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmFXfPhYDSNi"
      },
      "outputs": [],
      "source": [
        "_, cov = covarw(im1.select(visirbands))\n",
        "pprint(corr(cov))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWhTaIRXsShB"
      },
      "source": [
        "Note that the spectral bands of the image are generally highly and positively correlated with one another.\n",
        "\n",
        "By stacking the images one atop the other we can use the helper functions to display the between image correlations\n",
        "\n",
        "$$\n",
        "\\rm{corr}(X_i,Y_i), \\quad i= 1\\dots N.\n",
        "$$\n",
        "\n",
        "They can be found in the diagonal of the upper left $6\\times 6$ submatrix of the correlation matrix for the stacked image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXJkxrZmsShB"
      },
      "outputs": [],
      "source": [
        "im12 = im1.select(visirbands).addBands(im2.select(visirbands))\n",
        "_, covar = covarw(im12)\n",
        "correl = np.array(corr(covar))\n",
        "print(np.diag(correl[:6, 6:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJC-EC_PAXo9"
      },
      "source": [
        "It is these between image correlations that the MAD algorithm tries to maximize.\n",
        "\n",
        "The third and last auxiliary routine, *geneiv()*, is the core of CCA and the MAD transformation. It solves the *generalized eigenproblem*,\n",
        "\n",
        "$$\n",
        "CX = \\lambda BX\n",
        "$$\n",
        "\n",
        "for two $N\\times N$ matrices $C$ and $B$, returning the $N$ solutions, or eigenvectors, $X_i$ and the $N$ eigenvalues $\\lambda_i, \\ i=1\\dots N$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lK3AMH709Sg5"
      },
      "outputs": [],
      "source": [
        "def geneiv(C,B):\n",
        "    '''Return the eigenvalues and eigenvectors of the generalized eigenproblem\n",
        "                         C*X = lambda*B*X'''\n",
        "    try:\n",
        "        C = ee.Array(C)\n",
        "        B = ee.Array(B)\n",
        "        #  Li = choldc(B)^-1\n",
        "        Li = ee.Array(B.matrixCholeskyDecomposition().get('L')).matrixInverse()\n",
        "        #  solve symmetric, ordinary eigenproblem Li*C*Li^T*x = lambda*x\n",
        "        Xa = Li.matrixMultiply(C) \\\n",
        "            .matrixMultiply(Li.matrixTranspose()) \\\n",
        "            .eigen()\n",
        "        #  eigenvalues as a row vector\n",
        "        lambdas = Xa.slice(1, 0, 1).matrixTranspose()\n",
        "        #  eigenvectors as columns\n",
        "        X = Xa.slice(1, 1).matrixTranspose()\n",
        "        #  generalized eigenvectors as columns, Li^T*X\n",
        "        eigenvecs = Li.matrixTranspose().matrixMultiply(X)\n",
        "        return (lambdas, eigenvecs)\n",
        "    except Exception as e:\n",
        "        print('Error: %s'%e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qksWAxsrIV4g"
      },
      "source": [
        "The next cell codes the MAD transformation itself in the funcion *mad_run()*, taking as input two multiband images and returning the _canonical variates_\n",
        "\n",
        "$$\n",
        "U_i, \\ V_i, \\quad i=1\\dots N,\n",
        "$$\n",
        "\n",
        "the _MAD variates_\n",
        "\n",
        "$$\n",
        "M_i = U_i - V_i, \\quad i=1\\dots N,\n",
        "$$\n",
        "\n",
        "as well as the sum of the squares of the standardized MAD variates,\n",
        "\n",
        "$$\n",
        "Z = \\sum_{i=1}^N\\left({M_i\\over \\sigma_{M_i}}\\right)^2.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4I6Ig_zFICj"
      },
      "outputs": [],
      "source": [
        "#@title The MAD transformation\n",
        "def mad_run(image1, image2, scale = 20):\n",
        "    '''The MAD transformation of two multiband images'''\n",
        "    try:\n",
        "        image = image1.addBands(image2)\n",
        "        region = image.geometry()\n",
        "        nBands = image.bandNames().length().divide(2)\n",
        "        centeredImage,covarArray = covarw(image,scale=scale)\n",
        "        bNames = centeredImage.bandNames()\n",
        "        bNames1 = bNames.slice(0,nBands)\n",
        "        bNames2 = bNames.slice(nBands)\n",
        "        centeredImage1 = centeredImage.select(bNames1)\n",
        "        centeredImage2 = centeredImage.select(bNames2)\n",
        "        s11 = covarArray.slice(0,0,nBands).slice(1,0,nBands)\n",
        "        s22 = covarArray.slice(0,nBands).slice(1,nBands)\n",
        "        s12 = covarArray.slice(0,0,nBands).slice(1,nBands)\n",
        "        s21 = covarArray.slice(0,nBands).slice(1,0,nBands)\n",
        "        c1 = s12.matrixMultiply(s22.matrixInverse()).matrixMultiply(s21)\n",
        "        b1 = s11\n",
        "        c2 = s21.matrixMultiply(s11.matrixInverse()).matrixMultiply(s12)\n",
        "        b2 = s22\n",
        "        # solution of generalized eigenproblems\n",
        "        lambdas, A = geneiv(c1,b1)\n",
        "        _,       B = geneiv(c2,b2)\n",
        "        rhos = lambdas.sqrt().project(ee.List([1]))\n",
        "        # MAD variances\n",
        "        sigma2s = rhos.subtract(1).multiply(-2).toList()\n",
        "        sigma2s = ee.Image.constant(sigma2s)\n",
        "        # ensure sum of correlations between X and U is positive\n",
        "        tmp = s11.matrixDiagonal().sqrt()\n",
        "        ones = tmp.multiply(0).add(1)\n",
        "        tmp = ones.divide(tmp).matrixToDiag()\n",
        "        s = tmp.matrixMultiply(s11).matrixMultiply(A).reduce(ee.Reducer.sum(),[0]).transpose()\n",
        "        A = A.matrixMultiply(s.divide(s.abs()).matrixToDiag())\n",
        "        # ensure positive correlation between U and V\n",
        "        tmp = A.transpose().matrixMultiply(s12).matrixMultiply(B).matrixDiagonal()\n",
        "        tmp = tmp.divide(tmp.abs()).matrixToDiag()\n",
        "        B = B.matrixMultiply(tmp)\n",
        "        # canonical and MAD variates\n",
        "        centeredImage1Array = centeredImage1.toArray().toArray(1)\n",
        "        centeredImage2Array = centeredImage2.toArray().toArray(1)\n",
        "        U = ee.Image(A.transpose()).matrixMultiply(centeredImage1Array) \\\n",
        "                    .arrayProject([0]) \\\n",
        "                    .arrayFlatten([bNames2])\n",
        "        V = ee.Image(B.transpose()).matrixMultiply(centeredImage2Array) \\\n",
        "                    .arrayProject([0]) \\\n",
        "                    .arrayFlatten([bNames2])\n",
        "        MAD = U.subtract(V)\n",
        "        #  chi square image\n",
        "        Z = MAD.pow(2) \\\n",
        "                .divide(sigma2s) \\\n",
        "                .reduce(ee.Reducer.sum()) \\\n",
        "                .clip(region)\n",
        "        return (U, V, MAD, Z)\n",
        "    except Exception as e:\n",
        "        print('Error: %s'%e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLuzqZ4lR1Ck"
      },
      "source": [
        "After setting up the MAD transformation on the six visual and infrared bands of the Landkreis Olpe images,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTK-8PV5NvtO",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "U, V, MAD, Z = mad_run(im1.select(visirbands), im2.select(visirbands))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbeJ6WF_sShC"
      },
      "source": [
        "we display an RGB composite of the first 3 MAD variates, together with the original images and their simple difference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNAGaOr8sShC"
      },
      "outputs": [],
      "source": [
        "M2 = geemap.Map()\n",
        "M2.centerObject(aoi, 11)\n",
        "display_ls(im1.select(visbands), M2, 'Image 1')\n",
        "display_ls(im2.select(visbands), M2, 'Image 2')\n",
        "display_ls(diff, M2, 'Difference')\n",
        "display_ls(MAD.select(0, 1, 2), M2, 'MAD Image', True)\n",
        "\n",
        "M2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALAZOS0EdM24"
      },
      "source": [
        "The richness in colors in the MAD image is a consequence of the decoupling (orthogonalization) of the MAD components, as will be explained below.\n",
        "\n",
        "Pretty colors notwithstanding, when compared with the simple difference image the result is _mittelpr√§chtig_ (German slang for \"not so hot\"), and certainly no easier to interpret. However we're not finished.\n",
        "\n",
        "The canonical variates have very nice properties indeed. They are _all mutually uncorrelated_ except for the pairs $(U_i,V_i)$, and these are ordered by decreasing positive correlation:\n",
        "\n",
        "$$\n",
        "\\rho_i = {\\rm cov}(U_i, V_i),\\quad i=1\\dots N, \\tag{6}\n",
        "$$\n",
        "\n",
        "with $\\rho_1 \\ge \\rho_2 \\ge\\dots \\ge\\rho_N$.\n",
        "\n",
        "Stacking the images $U$ and $V$, these facts can be read from the $12\\times 12$ correlation matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwQjSqbPdPUQ"
      },
      "outputs": [],
      "source": [
        "_, covar = covarw(U.addBands(V))\n",
        "correl = np.array(corr(covar))\n",
        "pprint(correl)\n",
        "print('rho =', np.diag(correl[:6,6:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIWj8a7j8MhO"
      },
      "source": [
        "The MAD variates themselves are consequently also mutually uncorrelated, their covariances being given by\n",
        "\n",
        "$$\n",
        "{\\rm cov}(M_i,M_j) = {\\rm cov}(U_i-V_i,U_j-V_j)= 0,\\quad i\\ne j=1\\dots N, \\tag{7}\n",
        "$$\n",
        "\n",
        "and their variances by\n",
        "\n",
        "$$\n",
        "\\sigma_{M_i}^2={\\rm var}(U_i-V_i)= {\\rm var}(U_i)+{\\rm var}(V_i) -2{\\rm cov}(U_i,V_i) = 2(1-\\rho_i),\\quad i=1\\dots N, \\tag{8}\n",
        "$$\n",
        "\n",
        "where the last equality follows from the constraint Equation (4). Let's check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXOtyyIThIOw"
      },
      "outputs": [],
      "source": [
        "# display MAD covariance matrix\n",
        "_, covar = covarw(MAD)\n",
        "covar = covar.getInfo()\n",
        "[list(map(trunc,covar[i])) for i in range(len(covar))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOCSrVuo6jcL"
      },
      "source": [
        "\n",
        "\n",
        "The first MAD variate has minimum variance in its pixel\n",
        "intensities. The second MAD variate has minimum variance subject to the condition that its pixel intensities are statistically\n",
        "uncorrelated with those in the first variate; the third has\n",
        "minimum spread subject to being uncorrelated with the first two,\n",
        "and so on. Depending on the type of change present, any of the\n",
        "components may exhibit significant change information, although it tends to be concentrated in the first few MAD variates. One of the nicest aspects of the MAD transformation is that it can sort different categories of change into different, uncorrelated image bands.\n",
        "\n",
        "However, the result, as we saw, needs some improvement. This is the subject of Part 2."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
