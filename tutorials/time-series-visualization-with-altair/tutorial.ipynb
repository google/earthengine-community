{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "time-series-visualization-with-altair.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kdsGkYJXXKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Copyright 2020 The Earth Engine Community Authors { display-mode: \"form\" }\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l18M9_r5XmAQ",
        "colab_type": "text"
      },
      "source": [
        "# Time Series Visualization with Altair \n",
        "Author: jdbcode\n",
        "\n",
        "This tutorial provides methods for generating time series data in Earth Engine and visualizing it with the [Altair](https://altair-viz.github.io/) library using drought and vegetation response as an example.\n",
        "\n",
        "Topics include:\n",
        "\n",
        "- Time series region reduction in Earth Engine\n",
        "- Formatting a table in Earth Engine\n",
        "- Transferring Earth Engine table to Colab Python kernel\n",
        "- Converting Earth Engine table to [pandas](https://pandas.pydata.org/) DataFrame\n",
        "- Data representation with various Altair chart types\n",
        "\n",
        "**Note** that this tutorial uses the [Earth Engine Python API](https://developers.google.com/earth-engine/python_install) in a [Colab notebook](https://developers.google.com/earth-engine/python_install-colab.html). For a review of Python syntax as it relates to the Earth Engine API, please see this [guide](https://developers.google.com/earth-engine/python_install#syntax).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0cJv0RW40rx",
        "colab_type": "text"
      },
      "source": [
        "## Context\n",
        "\n",
        "At the heart of this tutorial is the notion of data reduction and the need to transform data into insights to help inform our understanding of Earth processes and human's role in them. It combines a series of technologies, each best suited to a particular task in the data reduction process. **Earth Engine** is used to calculate spatiotemporal summary statistics, **pandas** is used to organize the results, and **Altair** is used to visualize the results (Figure 1).\n",
        "\n",
        "<img src='https://github.com/jdbcode/earthengine-community/blob/ipynb-ts-vis/tutorials/time-series-visualization-with-altair/data-reduction.png?raw=1' width='600px' alt='reduction.png'><br>\n",
        "_Figure 1. Conceptual diagram illustrating the use of Earth Engine, pandas, and Altair in a data reduction workflow._\n",
        "\n",
        "**Note:** there are limitations to interactive computation time and server-to-client data transfer size imposed by Colab and Earth Engine. Therefore, this notebook is intended only to demonstrate functionality and provide an analysis template. For actual applications, it is recommended that you [export](https://developers.google.com/earth-engine/exporting#exporting-tables-and-vector-data) `featureCollection` results from Earth Engine as assets and then import them into a separate notebook to perform the steps involving Earth Engine table formatting, conversion to pandas DataFrame, and charting with Altair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UmHTUZJUkUG",
        "colab_type": "text"
      },
      "source": [
        "## Materials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8joJ18b9LA3",
        "colab_type": "text"
      },
      "source": [
        "### Datasets\n",
        "\n",
        "Climate\n",
        "\n",
        "- Drought severity ([PDSI](https://developers.google.com/earth-engine/datasets/catalog/NASA_NEX-DCP30))\n",
        "\n",
        "- Historical climate ([PRISM](https://developers.google.com/earth-engine/datasets/catalog/OREGONSTATE_PRISM_AN81m))\n",
        "- Projected climate ([NEX-DCP30](https://developers.google.com/earth-engine/datasets/catalog/NASA_NEX-DCP30))\n",
        "\n",
        "Vegetation proxies\n",
        "\n",
        "- NDVI ([MODIS](https://developers.google.com/earth-engine/datasets/catalog/MODIS_006_MOD13A2))\n",
        "- NBR ([Landsat](https://developers.google.com/earth-engine/datasets/catalog/landsat/)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtiMHMPj5Uha",
        "colab_type": "text"
      },
      "source": [
        "### Region of interest\n",
        "\n",
        "The region of interest for these examples is the Sierra Nevada ecoregion of California (Figure 2a). The vegetation grades from mostly ponderosa pine and Douglas-fir at low elevations on the western side, to pines and Sierra juniper on the eastern side, and to fir and other conifers at higher elevations. A sample representation is provided by Figure 2b.\n",
        "\n",
        "| <img src='https://github.com/jdbcode/earthengine-community/blob/ipynb-ts-vis/tutorials/time-series-visualization-with-altair/aoi.jpg?raw=1' height='350px' alt='aoi.jpg'> | <img src='https://github.com/jdbcode/earthengine-community/blob/ipynb-ts-vis/tutorials/time-series-visualization-with-altair/sierra-nevada-representation.jpg?raw=1' height='350px' alt='sierra-nevada-representation.jpg'> |\n",
        "| :---: | :---: |\n",
        "| Figure 2a. Sierra Nevada ecoregion, CA.<br>_Source: Google Earth Engine_ | Figure 2b. Sample representation of<br>the Sierra Nevada ecoregion<br>_Source: HikingMike - Mineral King Valley, <br>CC BY 2.0, https://commons.wikimedia.org/w/index.php?curid=46093222_ |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLDFvqCczdVc",
        "colab_type": "text"
      },
      "source": [
        "## General workflow\n",
        "\n",
        "Preparation of every dataset for visualization follows the same basic steps:\n",
        "\n",
        "1. Filter the dataset (Earth Engine)\n",
        "2. Reduce data region by a statistic (Earth Engine)\n",
        "3. Format region reduction into a table (Earth Engine)\n",
        "4. Convert Earth Engine table to DataFrame (Earth Engine > Python)\n",
        "5. Alter the DataFrame (Python)\n",
        "6. Plot the DataFrame (Python)\n",
        "\n",
        "The first example will walk through each step in detail. Following examples will provide less description, unless there is variation that merits note."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9YDwlnVQU5w",
        "colab_type": "text"
      },
      "source": [
        "## Python setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCezQcI-Qd3o",
        "colab_type": "text"
      },
      "source": [
        "### Earth Engine API\n",
        "\n",
        "1. Import the Earth Engine library.\n",
        "2. Authenticate access (registration verification and Google account access).\n",
        "3. Initialize the API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUBNDgqyiPZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup Earth Engine API.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6rS3zTFht5P",
        "colab_type": "text"
      },
      "source": [
        "### Other libraries\n",
        "\n",
        "Import other libraries used in this notebook.\n",
        "\n",
        "- **pandas**: DataFrame object\n",
        "- **altair**: charting\n",
        "- **numpy**: linear regression\n",
        "- **folium**: web map\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo_AiuPLh0kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries.\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "import numpy as np\n",
        "import folium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxeeVDke3L8K",
        "colab_type": "text"
      },
      "source": [
        "## Region reduction function\n",
        "\n",
        "Reduction of pixels intersecting the region of interest to a statistic will be performed multiple times. Define a reusable function that can perform the task for each dataset. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MWbSSWjIRs1",
        "colab_type": "text"
      },
      "source": [
        "#### Arguments\n",
        "\n",
        "Define a global dictionary that provides arguments to the Earth Engine `reduceRegion` image method used in the following function. As a global variable, this dictionary will be available to functions without the need to pass it as an input, which is convenient for mapping a function over an image collection where an image is the only parameter allowed. Here, you are simply initializing the key-value pairs; values will be updated as needed in later steps. Take a minute to [review each parameter](https://developers.google.com/earth-engine/api_docs#ee.image.reduceregion) if you are unfamiliar with the `reduceRegion` image method.\n",
        " \n",
        "**Note:** most of the reduction operations in this tutorial use a large pixel scale so that operations complete quickly. In your own application, set the scale and other parameter arguments as you wish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGb4uoN23Kkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define region reduction arguments.\n",
        "reduce_args = {\n",
        "    'reducer': ee.Reducer.mean(),\n",
        "    'geometry': ee.Geometry.Point([0, 0]),\n",
        "    'scale': 1000,\n",
        "    'crs': 'EPSG:5070',\n",
        "    'bestEffort': True,\n",
        "    'maxPixels': 1e14,\n",
        "    'tileScale': 4\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWG8SqeU3amp",
        "colab_type": "text"
      },
      "source": [
        "#### Function\n",
        "\n",
        "This function reduces the pixels intersecting a region to a statistic. It operates on a single `ee.Image` and returns an `ee.Feature`. Each feature will have properties that represent the result of the region reduction and the observation timestamp. The plan is to map this over multiple `ee.ImageCollection` objects. Within the function:\n",
        "\n",
        "1. Apply the `reduceRegion` method to the input image using arguments provided by the global `reduce_args` dictionary defined previously.\n",
        "2. Return an `ee.Feature` that contains the region reduction statistic result (an `ee.Dictionary`) and the data's timestamp formatted as milliseconds from Unix epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mkEjSrFlkjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a reusable region reduction function.\n",
        "def reduce_region(img):\n",
        "  stat = img.reduceRegion(\n",
        "      reducer=reduce_args['reducer'],\n",
        "      geometry=reduce_args['geometry'],\n",
        "      scale=reduce_args['scale'],\n",
        "      crs=reduce_args['crs'],\n",
        "      bestEffort=reduce_args['bestEffort'],\n",
        "      maxPixels=reduce_args['maxPixels'],\n",
        "      tileScale=reduce_args['tileScale'])\n",
        "\n",
        "  return ee.Feature(reduce_args['geometry'],\n",
        "                    stat).set({'millis': img.date().millis()})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-k7xPg-3oog",
        "colab_type": "text"
      },
      "source": [
        "### Formatting\n",
        "\n",
        "The result of the region reduction function above applied to an `ee.ImageCollection` (as in this tutorial) produces an `ee.FeatureCollection`. This data needs to be transferred to the Python kernel, but serialized feature collections are large and awkward to deal with. This step defines a function to convert the feature collection to an `ee.Dictionary` where the keys are feature property names and values are corresponding lists of property values, which `pandas` can deal with handily.\n",
        "\n",
        "1. Extract the property values from the `ee.FeatureCollection` as a list of lists stored in an `ee.Dictionary` using `reduceColumns()`.\n",
        "2. Extract the list of lists from the dictionary.\n",
        "3. Add names to each list by converting to an `ee.Dictionary` where keys are property names and values are the corresponding value lists.\n",
        "\n",
        "The returned `ee.Dictionary` is essentially a table, where keys define columns and list elements define rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuiFsdRL-qpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function to transfer feature properties to a dictionary.\n",
        "def fc_to_dict(fc):\n",
        "  prop_names = fc.first().propertyNames()\n",
        "  prop_lists = fc.reduceColumns(\n",
        "      reducer=ee.Reducer.toList().repeat(prop_names.size()),\n",
        "      selectors=prop_names).get('list')\n",
        "\n",
        "  return ee.Dictionary.fromLists(prop_names, prop_lists)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYFE71iWobNp",
        "colab_type": "text"
      },
      "source": [
        "## Drought severity\n",
        "\n",
        "In this section we'll look at a time series of drought severity as a calendar heat map and a bar chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPqa1d4v2vkV",
        "colab_type": "text"
      },
      "source": [
        "###  Import data\n",
        "\n",
        "1. Load gridded Palmer Drought Severity Index (PDSI) data as an `ee.ImageCollection`.\n",
        "2. Load the EPA Level-3 ecoregion boundaries as an `ee.FeatureCollection` and filter it to include only the Sierra Nevada region, which defines the area of interest (AOI)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTl381vFnZs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PDSI image collection and define area of interest.\n",
        "pdsi = ee.ImageCollection('IDAHO_EPSCOR/PDSI')\n",
        "aoi = ee.FeatureCollection('EPA/Ecoregions/2013/L3').filter(\n",
        "    ee.Filter.eq('na_l3name', 'Sierra Nevada')).geometry()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bV4k9Z-Abxx",
        "colab_type": "text"
      },
      "source": [
        "**Note**: the `aoi` defined above will be used throughout this tutorial. In your own application, redefine it for your own area of interest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLJ_XNtEog_L",
        "colab_type": "text"
      },
      "source": [
        "### Reduce data\n",
        "\n",
        "1. Set the reduce region arguments.\n",
        "2. Map the `reduce_region` function over the `pdsi` image collection to reduce each image.\n",
        "3. Filter out any resulting features that have null computed values (occurs when all pixels in an AOI are masked)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_9MyvRrogoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set region reduction arguments and reduce the PDSI image collection.\n",
        "reduce_args['scale'] = 5000\n",
        "reduce_args['reducer'] = ee.Reducer.mean()\n",
        "reduce_args['geometry'] = aoi\n",
        "reduce_args['crs'] = 'EPSG:3310'\n",
        "\n",
        "pdsi_stat_fc = ee.FeatureCollection(pdsi.map(reduce_region)).filter(\n",
        "    ee.Filter.notNull(pdsi.first().bandNames()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7tN6uQ1-5Pw",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "**STOP**: \n",
        "\n",
        "### _Optional export_\n",
        "\n",
        "_If your process is long-running_, you'll want to export the `pdsi_stat_fc` variable as an asset using a batch task. Wait until the task finishes, import the asset, and continue on. Please see the Developer Guide section on [exporting with the Python API](https://developers.google.com/earth-engine/python_install#exporting-data).\n",
        "\n",
        "Export to asset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-WOTuEujehM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "task = ee.batch.Export.table.toAsset(\n",
        "    collection=pdsi_stat_fc,\n",
        "    description='pdsi_stat_fc export',\n",
        "    assetId='users/YOUR_USER_NAME/pdsi_stat_fc_ts_vis_with_altair')\n",
        "\n",
        "task.start()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgNv-hlNj0sO",
        "colab_type": "text"
      },
      "source": [
        "Import the asset after the export completes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3Dx_z5KkOeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "pdsi_stat_fc = ee.FeatureCollection('users/YOUR_USER_NAME/pdsi_stat_fc_ts_vis_with_altair')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H4246-UPGVX",
        "colab_type": "text"
      },
      "source": [
        "_\\* Remove triple quote comment fence to run the above cells._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOUWkPIRjnz5",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "**CONTINUE**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k71MycvMoUce",
        "colab_type": "text"
      },
      "source": [
        "### Server to client transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vmsFl28_O0x",
        "colab_type": "text"
      },
      "source": [
        "The `ee.FeatureCollection` needs to be converted to a dictionary and transferred to the Python kernel.\n",
        "\n",
        "1. Apply the `fc_to_dict` function to convert from `ee.FeatureCollection` to `ee.Dictionary`.\n",
        "2. Call `getInfo()` on the `ee.Dictionary` to transfer the data client-side. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgP2H-2f_bj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert feature collection to dictionary and transfer from server to client.\n",
        "pdsi_dict = fc_to_dict(pdsi_stat_fc).getInfo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-HLx08ynhV-",
        "colab_type": "text"
      },
      "source": [
        "The result is a Python dictionary. Print it to see how it is formatted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ_Nu0mZy5Wh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inspect the client-side dictionary.\n",
        "print(type(pdsi_dict), '\\n')\n",
        "for prop in pdsi_dict.keys():\n",
        "    print(prop + ':', pdsi_dict[prop][0:3] + ['...'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUEiS9qOHCDn",
        "colab_type": "text"
      },
      "source": [
        "Convert the Python dictionary to pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-5u7KQE2k5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the Python dictionary to pandas DataFrame.\n",
        "pdsi_df = pd.DataFrame(pdsi_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0he_CQbkAhh",
        "colab_type": "text"
      },
      "source": [
        "Preview the DataFrame and check the column data types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mpcb6s62pmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preview the DataFrame and check the column data types.\n",
        "print(pdsi_df, '\\n\\n')\n",
        "print(pdsi_df.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZZsiwC6DgvZ",
        "colab_type": "text"
      },
      "source": [
        "### Add date columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiqSmB_KlYt6",
        "colab_type": "text"
      },
      "source": [
        "Add date columns derived from the milliseconds from Unix epoch column. The pandas library provides functions and objects for timestamps and the Dataframe object allows for easy mutation.\n",
        "\n",
        "Define a function to add date variables to the DataFrame: year, month, day, and day-of-year (DOY)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SqbstkjlYA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to add date variables to DataFrame.\n",
        "def add_date_info(df):\n",
        "  df['Timestamp'] = pd.to_datetime(df['millis'], unit='ms')\n",
        "  df['Year'] = pd.DatetimeIndex(df['Timestamp']).year\n",
        "  df['Month'] = pd.DatetimeIndex(df['Timestamp']).month\n",
        "  df['Day'] = pd.DatetimeIndex(df['Timestamp']).day\n",
        "  df['DOY'] = pd.DatetimeIndex(df['Timestamp']).dayofyear\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL9T59AYDfpD",
        "colab_type": "text"
      },
      "source": [
        "**Note**: the above function for adding date information to a DataFrame will be used throughout this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqKafJMfpqbF",
        "colab_type": "text"
      },
      "source": [
        "Apply the `add_date_info` function to the PDSI DataFrame to add date attribute columns, preview the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSJDElx4NLxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add date columns and preview the DataFrame.\n",
        "pdsi_df = add_date_info(pdsi_df)\n",
        "pdsi_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHzIm-TrrRsf",
        "colab_type": "text"
      },
      "source": [
        "### Rename and drop columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtacuRy4QDwS",
        "colab_type": "text"
      },
      "source": [
        "Often it is desirable to rename columns and/or remove unnecessary columns. Do both here and preview the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ72vESQQDUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rename and drop columns.\n",
        "pdsi_df = pdsi_df.rename(columns={\n",
        "    'pdsi': 'PDSI'\n",
        "}).drop(columns=['millis', 'system:index'])\n",
        "pdsi_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg6XQWritHcs",
        "colab_type": "text"
      },
      "source": [
        "Check the data type of each column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsV2q0wvtPrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inspect column data types.\n",
        "pdsi_df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfVRIsBdtqPE",
        "colab_type": "text"
      },
      "source": [
        "At this point the DataFrame is in good shape for charting with Altair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbbOWe4OEWKz",
        "colab_type": "text"
      },
      "source": [
        "### Calendar heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnRr12VRIxqG",
        "colab_type": "text"
      },
      "source": [
        "Chart PDSI data as a calendar heatmap. Set observation year as the x-axis variable, month as y-axis, and PDSI value as color.\n",
        "\n",
        "Note that Altair features a convenient [method for aggregating values within groups](https://altair-viz.github.io/user_guide/transform/aggregate.html) while encoding the chart (i.e., no need to create a new DataFrame). The mean aggregate transform is applied here because each month has three PDSI observations (year and month are the grouping factors).\n",
        "\n",
        "Also note that a tooltip has been added to the chart; hovering over cells reveals the values of the selected variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY4FJBtNuLeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chart PDSI time series as a calendar heatmap.\n",
        "alt.Chart(pdsi_df).mark_rect().encode(\n",
        "    x='Year:O',\n",
        "    y='Month:O',\n",
        "    color=alt.Color(\n",
        "        'mean(PDSI):Q', scale=alt.Scale(scheme='redblue', domain=(-5, 5))),\n",
        "    tooltip=[\n",
        "        alt.Tooltip('Year:O', title='Year'),\n",
        "        alt.Tooltip('Month:O', title='Month'),\n",
        "        alt.Tooltip('mean(PDSI):Q', title='PDSI')\n",
        "    ]).properties(\n",
        "        width=600, height=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpDDqSMJQeUz",
        "colab_type": "text"
      },
      "source": [
        "The calendar heat map is good for interpretation of relative intra- and inter-annual differences in PDSI. However, since the PDSI variable is represented by color, estimating absolute values and magnitude of difference is difficult."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrNfrsd-ifa8",
        "colab_type": "text"
      },
      "source": [
        "### Bar chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSYnvI9pRUpz",
        "colab_type": "text"
      },
      "source": [
        "Chart PDSI time series as a bar chart to more easily interpret absolute values and compare them over time. Here, the observation timestamp is represented on the x-axis and PDSI is represented by both the y-axis and color. Since each PDSI observation has a unique timestamp that can be plotted to the x-axis, there is no need to aggregate PDSI values as in the above chart. A tooltip is added to the chart; hover over data to reveal variable values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58p76nLEwaEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chart PDSI time series as an Altair bar chart.\n",
        "alt.Chart(pdsi_df).mark_bar(size=1).encode(\n",
        "    x='Timestamp:T',\n",
        "    y='PDSI:Q',\n",
        "    color=alt.Color(\n",
        "        'PDSI:Q', scale=alt.Scale(scheme='redblue', domain=(-5, 5))),\n",
        "    tooltip=[\n",
        "        alt.Tooltip('Timestamp:T', title='Date'),\n",
        "        alt.Tooltip('PDSI:Q', title='PDSI')\n",
        "    ]).properties(\n",
        "        width=600, height=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU5FYG7bTaao",
        "colab_type": "text"
      },
      "source": [
        "This temporal bar chart makes it easier to interpret and compare absolute values of PDSI over time, but relative intra- and inter-annual variability are arguably harder to interpret because the division of year and month is not as distinct as in the calendar heatmap above.\n",
        "\n",
        "Take note of the extended and severe period of drought from 2012 through 2016. In the next section, we'll look for a vegetation response to this event."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWviMm-Q2VMF",
        "colab_type": "text"
      },
      "source": [
        "## Vegetation productivity\n",
        "\n",
        "NDVI is a proxy measure of photosynthetic capacity and is used in this tutorial to investigate vegetation response to the 2012-2016 drought identified in the PDSI bar chart above.\n",
        "\n",
        "MODIS provides an analysis-ready 16-day NDVI composite that is well suited for regional investigation of temporal dynamics. The following steps reduce and prepare this data for charting in the same manner as the PDSI data above; please refer to previous sections to review details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tigBp8Y9IWjC",
        "colab_type": "text"
      },
      "source": [
        "### Import and reduce\n",
        "\n",
        "1. Load MODIS NDVI data as an Earth Engine `ee.ImageCollection`.\n",
        "2. Set the region reduce `scale` argument to 1000 (meters).\n",
        "3. Apply region reduce function to all images in the time series.\n",
        "4. Filter out features with null computed values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfTfhCiX8Ew4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import and compute region reduction for MODIS NDVI image collection.\n",
        "ndvi = ee.ImageCollection('MODIS/006/MOD13A2').select('NDVI')\n",
        "reduce_args['scale'] = 1000\n",
        "ndvi_stat_fc = ee.FeatureCollection(ndvi.map(reduce_region)).filter(\n",
        "    ee.Filter.notNull(ndvi.first().bandNames()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtO27KMIemk",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "**STOP**: \n",
        "\n",
        "_If your process is long-running_, you'll want to export the `ndvi_stat_fc` variable as an asset using a batch task. Wait until the task finishes, import the asset, and continue on.\n",
        "\n",
        "Please see the above **_Optional export_** section for more details.\n",
        "\n",
        "**CONTINUE**:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7TI_Rd9xAqz",
        "colab_type": "text"
      },
      "source": [
        "### Prepare DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQM61YQFxKeb",
        "colab_type": "text"
      },
      "source": [
        "1. Transfer data from server to client.\n",
        "2. Convert Python dictionary to pandas DataFrame.\n",
        "3. Preview DataFrame and check data types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P-3PUnfIeEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare DataFrame.\n",
        "ndvi_dict = fc_to_dict(ndvi_stat_fc).getInfo()\n",
        "ndvi_df = pd.DataFrame(ndvi_dict)\n",
        "print(ndvi_df, '\\n\\n')\n",
        "print(ndvi_df.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIOh1UhGJvSz",
        "colab_type": "text"
      },
      "source": [
        "4. Remove the NDVI scaling.\n",
        "5. Add date attribute columns.\n",
        "6. Preview the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wIikfzeJ2Yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare DataFrame.\n",
        "ndvi_df['NDVI'] = ndvi_df['NDVI'] / 10000\n",
        "ndvi_df = add_date_info(ndvi_df)\n",
        "ndvi_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2zu-3XLxp3D",
        "colab_type": "text"
      },
      "source": [
        "These NDVI time series data are now ready for plotting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnYEVl-d3t63",
        "colab_type": "text"
      },
      "source": [
        "### DOY line chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uBkZEtyH_wJ",
        "colab_type": "text"
      },
      "source": [
        "Make day-of-year (DOY) line chart where each line represents a year of observations. This chart makes it possible to compare the same observation date among years. Use it to compare NDVI values for years during the drought and not.\n",
        "\n",
        "Day-of-year is represented on the x-axis and NDVI on the y-axis. Each line represents a year and is distinguished by color. Note that this plot includes a tooltip and has been made interactive so that the y-axis can be zoomed and panned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brVo8clu3v1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chart NDVI time series as an Altair line chart.\n",
        "alt.Chart(ndvi_df).mark_line().encode(\n",
        "    alt.X('DOY:O'),\n",
        "    alt.Y('NDVI:Q', scale=alt.Scale(domain=(0.1, 0.7))),\n",
        "    alt.Color('Year:O', scale=alt.Scale(scheme='magma')),\n",
        "    tooltip=[\n",
        "        alt.Tooltip('Year:O', title='Year'),\n",
        "        alt.Tooltip('DOY:O', title='DOY'),\n",
        "        alt.Tooltip('NDVI:Q', title='NDVI')\n",
        "    ]).interactive().properties(\n",
        "        width=600, height=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkDcTpYRJKef",
        "colab_type": "text"
      },
      "source": [
        "The first thing to note is that winter dates (when there is snow in the Sierra Nevada ecoregion) exhibit highly variable inter-annual NDVI, but spring, summer, and fall dates are more consistent. With regard to drought effects on vegetation, summer and fall dates are the most sensitive time. Zooming into observations for the summer/fall days (224-272), you'll notice that many years have a u-shaped pattern where NDVI values decrease and then rise, though it's a bit of a spaghetti mess to interpret."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAaoCg7fmcPc",
        "colab_type": "text"
      },
      "source": [
        "Another way to view these data is to plot the distribution of NDVI by DOY represented as an interquartile range envelope and median line. Here these two charts are defined and then combined in the following snippet.\n",
        "\n",
        "1. Define a line chart for median NDVI (note the use of aggregate median transform grouping by DOY).\n",
        "2. Define a band chart using `'iqr'` (interquartile range) to represent NDVI distribution grouping on DOY.\n",
        "3. Combine the two plots by simply adding them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbMOGmqs0Zsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chart NDVI time series as an Altair band and line chart.\n",
        "x_scale = alt.Scale(domain=(150, 340))\n",
        "y_scale = alt.Scale(domain=(0.47, 0.53))\n",
        "\n",
        "line = alt.Chart(ndvi_df).mark_line().encode(\n",
        "    x=alt.X('DOY:Q', scale=x_scale),\n",
        "    y=alt.Y('median(NDVI):Q', scale=y_scale),\n",
        ").interactive()\n",
        "\n",
        "band = alt.Chart(ndvi_df).mark_errorband(extent='iqr').encode(\n",
        "    x=alt.X('DOY:Q', scale=x_scale),\n",
        "    y=alt.Y('NDVI:Q', scale=y_scale),\n",
        ")\n",
        "\n",
        "(band + line).properties(width=600, height=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaC4Sdj_VUGv",
        "colab_type": "text"
      },
      "source": [
        "The summary statistics for the summer/fall days (224-272) certainly show an NDVI reduction, but there is also variability; some years exhibit greater NDVI reduction than others as suggested by the wide interquartile range during the middle of the summer. Assuming that NDVI reduction is due to water and heat limiting photosynthesis, we can hypothesize that during years of drought, photosynthesis (NDVI) will be lower than non-drought years. We can investigate the relationship between photosynthesis (NDVI) and drought (PDSI) using a scatter plot and linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG_muCDTKmFP",
        "colab_type": "text"
      },
      "source": [
        "## Dought and productivity relationship\n",
        "\n",
        "A scatterplot is a good way to visualize the relationship between two variables. Here, PDSI (drought indicator) will be plotted on the x-axis and NDVI (vegetation productivity) on the y-axis. To achieve this, both variables must exist in the same DataFrame. Each row will be an observation in time and columns will correspond to PDSI and NDVI values. Currently, PDSI and NDVI are in two different DataFrames and need to be merged."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3jRaH-mWqxd",
        "colab_type": "text"
      },
      "source": [
        "### Prepare DataFrames\n",
        "\n",
        "Before they can be merged, each variable must be reduced to a common temporal observation unit to define correspondence. There are a number of ways to do this and each will define the relationship between PDSI and NDVI differently. Here, our temporal unit will be an annual observation set where NDVI is reduced to the intra-annual minimum from DOY 224 to 272 and PDSI will be the mean from DOY 1 to 272. We are proposing that average drought severity for the first three quarters of a year are related to minimum summer NDVI for a given year.\n",
        "\n",
        "1. Filter the NDVI DataFrame to observations that occur between DOY 224 and 272.\n",
        "2. Reduce the DOY-filtered subset to intra-annual minimum NDVI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMRpUId911I2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filter the NDVI DataFrame to include only observations within a defined DOY range.\n",
        "ndvi_doy_range = [224, 272]\n",
        "\n",
        "ndvi_df_sub = ndvi_df[(ndvi_df['DOY'] >= ndvi_doy_range[0])\n",
        "                      & (ndvi_df['DOY'] <= ndvi_doy_range[1])]\n",
        "\n",
        "ndvi_df_sub = ndvi_df_sub.groupby('Year').agg('min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV0G7eQhR9Dp",
        "colab_type": "text"
      },
      "source": [
        "**Note**: in your own application you may find that a different DOY range is more suitable, change the `ndvi_doy_range` as needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF3Y76torIae",
        "colab_type": "text"
      },
      "source": [
        "3. Filter the PDSI DataFrame to observations that occur between DOY 1 and 272.\n",
        "4. Reduce the values within a given year to the mean of the observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge9xtTU5ayeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filter the PDSI DataFrame to include only observations within a defined DOY range.\n",
        "pdsi_doy_range = [1, 272]\n",
        "\n",
        "pdsi_df_sub = pdsi_df[(pdsi_df['DOY'] >= pdsi_doy_range[0])\n",
        "                      & (pdsi_df['DOY'] <= pdsi_doy_range[1])]\n",
        "\n",
        "pdsi_df_sub = pdsi_df_sub.groupby('Year').agg('mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6_AUSguSLDI",
        "colab_type": "text"
      },
      "source": [
        "**Note**: in your own application you may find that a different DOY range is more suitable, change the `pdsi_doy_range` as needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdBx1sLsbIrB",
        "colab_type": "text"
      },
      "source": [
        "5. Perform a join on 'Year' to combine the two reduced DataFrames.\n",
        "6. Select only the columns of interest: 'Year', 'NDVI', 'PDSI'.\n",
        "7. Preview the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myucC6MxbHxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join the NDVI and PDSI DataFrames on the common 'Year' property.\n",
        "ndvi_pdsi_df = pd.merge(\n",
        "    ndvi_df_sub, pdsi_df_sub, how='left', on='Year').reset_index()\n",
        "\n",
        "ndvi_pdsi_df = ndvi_pdsi_df[['Year', 'NDVI', 'PDSI']]\n",
        "\n",
        "ndvi_pdsi_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfHPBeA0Nf8v",
        "colab_type": "text"
      },
      "source": [
        "NDVI and PDSI are now included in the same DataFrame linked by Year. This format is suitable for determining a linear relationship and drawing a line of best fit through the data.\n",
        "\n",
        "Including a line of best fit can be a helpful visual aid. Here, a 1D polynomial is fit through the xy point cloud defined by corresponding NDVI and PDSI observations. The resulting fit is added to the DataFrame as a new column 'Fit'.\n",
        "\n",
        "8. Add a line of best fit between PDSI and NDVI by determining the linear relationship and predicting NDVI based on PDSI for each year."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6w8Kqj5Wk34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add column to DataFrame that represents regression-fitted NDVI.\n",
        "ndvi_pdsi_df['Fit'] = np.poly1d(\n",
        "    np.polyfit(ndvi_pdsi_df['PDSI'], ndvi_pdsi_df['NDVI'], 1))(\n",
        "        ndvi_pdsi_df['PDSI'])\n",
        "\n",
        "ndvi_pdsi_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSpWaIhwN6k9",
        "colab_type": "text"
      },
      "source": [
        "### Scatter plot\n",
        "\n",
        "The DataFrame is ready for plotting. Since this chart is to include points and a line of best fit, two charts need to be created, one for the points and one for the line. The results are combined into the final plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCBImjDdCuZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chart NDVI/PDSI scatter plot with line of best fit.\n",
        "x_scale = alt.Scale(domain=(-5, 5))\n",
        "y_scale = alt.Scale(domain=(0.4, 0.6))\n",
        "\n",
        "points = alt.Chart(ndvi_pdsi_df).mark_circle(size=60).encode(\n",
        "    x=alt.X('PDSI:Q', scale=x_scale),\n",
        "    y=alt.Y('NDVI:Q', scale=y_scale),\n",
        "    color=alt.Color('Year:O', scale=alt.Scale(scheme='magma')),\n",
        "    tooltip=['Year', 'PDSI', 'NDVI']).interactive()\n",
        "\n",
        "fit = alt.Chart(ndvi_pdsi_df).mark_line().encode(\n",
        "    x=alt.X('PDSI:Q', scale=x_scale),\n",
        "    y=alt.Y('Fit:Q', scale=y_scale),\n",
        "    color=alt.value('#808080'))\n",
        "\n",
        "(fit + points).properties(width=600, height=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DucYGPjXONyg",
        "colab_type": "text"
      },
      "source": [
        "As you can see, there seems to be some degree of positive correlation between PDSI and NDVI (i.e., as wetness increases, vegetation productivity increases; as wetness decreases, vegetation productivity decreases). Note that some of the greatest outliers are 2016, 2017, 2018 - the three years following recovery from the long drought. It is also important to note that there are many other factors that may influence the NDVI signal that are not being considered here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb5kyCMmcJYQ",
        "colab_type": "text"
      },
      "source": [
        "## Patch-level vegetation mortality\n",
        "\n",
        "At a regional scale there appears to be a relationship between drought and vegetation productivity. This section will look more closely at effects of drought on vegetation at a patch level, with a specific focus on mortality. Here, a Landsat time series collection is created for the period 1984-present to provide greater temporal context for change at a relatively precise spatial resolution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPSzZWf_uMzE",
        "colab_type": "text"
      },
      "source": [
        "### Find a point of interest\n",
        "\n",
        "Use [aerial imagery](https://developers.google.com/earth-engine/datasets/catalog/USDA_NAIP_DOQQ) from the National Agriculture Imagery Program (NAIP) in an interactive [Folium](https://python-visualization.github.io/folium/) map to identify a location in the Sierra Nevada ecoregion that appears to have patches of dead trees.\n",
        "\n",
        "1. Run the following code block to render an interactive Folium map for a selected NAIP image.\n",
        "2. Zoom and pan around the image to identify a region of recently dead trees (standing silver snags with no fine branches or brown/grey snags with fine branches).\n",
        "3. Click the map to list the latitude and longitude for a patch of interest. Record these values for use in the following section (the example location used in the following section is presented as a red point)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeYO1qmfb3jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display an interactive aerial image map with Folium. Use it to identify\n",
        "# coordinates for a patch of dead trees.\n",
        "\n",
        "# Define a method for displaying Earth Engine image tiles to folium map.\n",
        "def add_ee_layer(self, ee_image_object, vis_params, name):\n",
        "  map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n",
        "  folium.raster_layers.TileLayer(\n",
        "      tiles=map_id_dict['tile_fetcher'].url_format,\n",
        "      attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine, USDA National Agriculture Imagery Program</a>',\n",
        "      name=name,\n",
        "      overlay=True,\n",
        "      control=True).add_to(self)\n",
        "\n",
        "# Add Earth Engine layer drawing method to folium.\n",
        "folium.Map.add_ee_layer = add_ee_layer\n",
        "\n",
        "# Import a NAIP image for the area and date of interest.\n",
        "naip_img = ee.ImageCollection('USDA/NAIP/DOQQ').filterDate(\n",
        "    '2016-01-01',\n",
        "    '2017-01-01').filterBounds(ee.Geometry.Point([-118.6407, 35.9665])).first()\n",
        "\n",
        "# Display to folium map.\n",
        "m = folium.Map(location=[35.9665, -118.6407], zoom_start=16, height=500)\n",
        "m.add_ee_layer(naip_img, None, 'NAIP image, 2016')\n",
        "\n",
        "# Add point of interest to map.\n",
        "folium.Circle(\n",
        "    radius=15,\n",
        "    location=[35.9665, -118.6407],\n",
        "    color='yellow',\n",
        "    fill=False,\n",
        ").add_to(m)\n",
        "\n",
        "# Add a lat lon popup.\n",
        "folium.LatLngPopup().add_to(m)\n",
        "\n",
        "# Display the map.\n",
        "display(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpiOjZwEX4BI",
        "colab_type": "text"
      },
      "source": [
        "### Prepare Landsat collection\n",
        "\n",
        "Landsat surface reflectance data need to be prepared before being reduced. The steps below will organize data from multiple sensors into congruent collections where band names are consistent, cloud and cloud shadows have been masked out, and the normalized burn ratio (NBR) transformation is calculated and returned as the image representative (NBR is a good indicator of forest disturbance). Finally, all sensor collections will be merged into a single collection and annual composites calculated based on mean annual NBR using a join. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKsbAOU5YA8_",
        "colab_type": "text"
      },
      "source": [
        "1. Define Landsat observation date window inputs based on NDVI curve plotted previously and set latitude and longitude variables from the map above.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud1vjM8zHpdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start and end dates.\n",
        "start_day = 224\n",
        "end_day = 272\n",
        "\n",
        "# Lat and lon from above point selection.\n",
        "latitude = 35.9665\n",
        "longitude = -118.6407"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMuPRxnPvIz6",
        "colab_type": "text"
      },
      "source": [
        "**Note**: in your own application it may be necessary to change these values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOvKDV_A14QM",
        "colab_type": "text"
      },
      "source": [
        "2. Prepare a Landsat surface reflectance collection 1984-present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DOwu0zt-UcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make lat and lon an `ee.Geometry.Point`.\n",
        "point = ee.Geometry.Point([longitude, latitude])\n",
        "\n",
        "# Define function to get and rename bands of interest from OLI.\n",
        "def rename_oli(img):\n",
        "  return (img.select(\n",
        "      ee.List(['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'pixel_qa']),\n",
        "      ee.List(['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2', 'pixel_qa'])))\n",
        "\n",
        "# Define function to get and rename bands of interest from ETM+.\n",
        "def rename_etm(img):\n",
        "  return (img.select(\n",
        "      ee.List(['B1', 'B2', 'B3', 'B4', 'B5', 'B7', 'pixel_qa']),\n",
        "      ee.List(['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2', 'pixel_qa'])))\n",
        "\n",
        "# Define function to mask out clouds and cloud shadows.\n",
        "def cfmask(img):\n",
        "  cloud_shadow_bi_mask = 1 << 3\n",
        "  cloud_bit_mask = 1 << 5\n",
        "  qa = img.select('pixel_qa')\n",
        "  mask = qa.bitwiseAnd(cloud_shadow_bi_mask).eq(0).And(\n",
        "      qa.bitwiseAnd(cloud_bit_mask).eq(0))\n",
        "  return img.updateMask(mask)\n",
        "\n",
        "# Define function to add year as an image property.\n",
        "def set_year(img):\n",
        "  year = ee.Image(img).date().get('year')\n",
        "  return img.set('Year', year)\n",
        "\n",
        "# Define function to calculate NBR.\n",
        "def calc_nbr(img):\n",
        "  return img.normalizedDifference(ee.List(['NIR', 'SWIR2'])).rename('NBR')\n",
        "\n",
        "# Define function to prepare OLI images.\n",
        "def prep_oli(img):\n",
        "  orig = img\n",
        "  img = rename_oli(img)\n",
        "  img = cfmask(img)\n",
        "  img = calc_nbr(img)\n",
        "  img = img.copyProperties(orig, orig.propertyNames())\n",
        "  return set_year(img)\n",
        "\n",
        "# Define function to prepare TM/ETM+ images.\n",
        "def prep_etm(img):\n",
        "  orig = img\n",
        "  img = rename_etm(img)\n",
        "  img = cfmask(img)\n",
        "  img = calc_nbr(img)\n",
        "  img = img.copyProperties(orig, orig.propertyNames())\n",
        "  return set_year(img)\n",
        "\n",
        "# Import image collections for each Landsat sensor (surface reflectance).\n",
        "tm_col = ee.ImageCollection('LANDSAT/LT05/C01/T1_SR')\n",
        "etm_col = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\n",
        "oli_col = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
        "\n",
        "# Filter collections and prepare them for merging.\n",
        "oli_col = oli_col.filterBounds(point).filter(\n",
        "    ee.Filter.calendarRange(start_day, end_day, 'day_of_year')).map(prep_oli)\n",
        "\n",
        "etm_col = etm_col.filterBounds(point).filter(\n",
        "    ee.Filter.calendarRange(start_day, end_day, 'day_of_year')).map(prep_etm)\n",
        "\n",
        "tm_col = tm_col.filterBounds(point).filter(\n",
        "    ee.Filter.calendarRange(start_day, end_day, 'day_of_year')).map(prep_etm)\n",
        "\n",
        "# Merge the collections.\n",
        "landsat_col = oli_col.merge(etm_col).merge(tm_col)\n",
        "\n",
        "# Get distinct year collection.\n",
        "distinct_year_col = landsat_col.distinct('Year')\n",
        "\n",
        "# Define a filter that identifies which images from the complete collection\n",
        "# match the year from the distinct year collection.\n",
        "join_filter = ee.Filter.equals(leftField='Year', rightField='Year')\n",
        "\n",
        "# Define a join.\n",
        "join = ee.Join.saveAll('year_matches')\n",
        "\n",
        "# Apply the join and convert the resulting FeatureCollection to an\n",
        "# ImageCollection.\n",
        "join_col = ee.ImageCollection(\n",
        "    join.apply(distinct_year_col, landsat_col, join_filter))\n",
        "\n",
        "# Define function to apply mean reduction among matching year collections.\n",
        "def reduce_by_join(img):\n",
        "  year_col = ee.ImageCollection.fromImages(ee.Image(img).get('year_matches'))\n",
        "  return year_col.reduce(ee.Reducer.mean()).rename('NBR').set(\n",
        "      'system:time_start',\n",
        "      ee.Image(img).date().update(month=8, day=1).millis())\n",
        "\n",
        "\n",
        "# Apply the `reduce_by_join` function to the list of annual images in the\n",
        "# properties of the join collection.\n",
        "landsat_col = join_col.map(reduce_by_join)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp0HPyK8wsk7",
        "colab_type": "text"
      },
      "source": [
        "The result of the above code block is an image collection with as many images as there are years present in the merged Landsat collection. Each image represents the annual mean NBR constrained to observations within the given date window. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pCpfWiUXxri",
        "colab_type": "text"
      },
      "source": [
        "### Prepare DataFrame\n",
        "\n",
        "1. Set the region reduce arguments. Use `ee.Reducer.first()` as the reducer since no spatial aggregation is needed, as we are setting the `scale` argument to 30m (native Landsat resolution). Set the region as the geometry defined by the lat and long coordinates identified in the above map.\n",
        "2. Apply the `reduce_region` function to all images in the time series.\n",
        "3. Filter out features with null computed values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx0Vk49_duQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Update region reduction arguments and reduce the Landsat collection.\n",
        "reduce_args['reducer'] = ee.Reducer.first()\n",
        "reduce_args['scale'] = 30\n",
        "reduce_args['geometry'] = point\n",
        "\n",
        "nbr_stat_fc = ee.FeatureCollection(landsat_col.map(reduce_region)).filter(\n",
        "    ee.Filter.notNull(landsat_col.first().bandNames()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uCkzwsWxdgfN"
      },
      "source": [
        "4. Transfer data from server to client.<br>\n",
        "*Note: if the process times out, you'll need to export/import the `nbr_stat_fc` feature collection as described in the **Optional export** section*.\n",
        "5. Convert Python dictionary to pandas DataFrame.\n",
        "6. Preview DataFrame and check data types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySnV4y-6cDR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare DataFrame.\n",
        "nbr_dict = fc_to_dict(nbr_stat_fc).getInfo()\n",
        "nbr_df = pd.DataFrame(nbr_dict)\n",
        "print(nbr_df, '\\n\\n')\n",
        "print(nbr_df.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wTGhOqymdrjy"
      },
      "source": [
        "7. Add date attribute columns.\n",
        "8. Preview the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "068PiyGVdwrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare DataFrame.\n",
        "nbr_df = add_date_info(nbr_df)\n",
        "nbr_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZCj3bwZkOru",
        "colab_type": "text"
      },
      "source": [
        "### Line chart\n",
        "\n",
        "Display Landsat NBR time series for the point of interest as a line plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx8iDh4HEeUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display Landsat NBR time series for the point of interest as a line plot.\n",
        "alt.Chart(nbr_df).mark_line().encode(\n",
        "    x=alt.X('Timestamp:T', title='Date'),\n",
        "    y=alt.Y('NBR:Q'),\n",
        "    tooltip=[alt.Tooltip('Timestamp:T', title='Date'),\n",
        "             alt.Tooltip('NBR:Q')]).interactive().properties(\n",
        "                 width=600, height=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHpgJVUiiCsH",
        "colab_type": "text"
      },
      "source": [
        "As you can see from the above time series of NBR observations, a dramatic decrease in NBR was detected in 2015, a year after the severe and extended drought began. The decline continued through 2017, when a minor recovery began. Within the context of the entire time series, it is apparent that the decline is outside of normal inter-annual variability and that the reduction in NBR for this site is quite severe. The lack of major recovery response in NBR in 2017-2019 indicates that the event was not ephemeral; the loss of vegetation will have a lasting impact on this site. The corresponding onset of drought and reduction in NBR provides further evidence that there is a relationship between drought and vegetation response in the Sierra Nevada ecoregion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJqMUKIyWeIx",
        "colab_type": "text"
      },
      "source": [
        "## Past and future climate\n",
        "The previous data visualizations suggest there is a relationship between drought and vegetation stress and mortality in the Sierra Nevada ecoregion. \n",
        "\n",
        "This section will look at how climate is projected to change in the future, which can give us a sense for what to expect with regard to drought conditions and speculate about its impact on vegetation.\n",
        "\n",
        "We'll look at historical and projected temperature and precipitation. Projected data are represented by NEX-DCP30, and historical observations by PRISM. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wpu02REWI6u",
        "colab_type": "text"
      },
      "source": [
        "### Future climate\n",
        "\n",
        "NEX-DCP30 data contain 33 climate models projected to the year 2100 using several scenarios of greenhouse gas concentration pathways (RCP). Here, we'll use the median of all models for RCP 8.5 (the worst case scenario) to look at potential future temperature and precipitation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBgoSEEB_U91",
        "colab_type": "text"
      },
      "source": [
        "#### Import and prepare collection\n",
        "\n",
        "1. Filter collection by date and scenario.\n",
        "2. Calculate 'mean' temperature from median min and max among 33 models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBD6cDZSWdRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import and prepare collection.\n",
        "dcp_col = (ee.ImageCollection('NASA/NEX-DCP30_ENSEMBLE_STATS')\n",
        "           .select(['tasmax_median', 'tasmin_median', 'pr_median'])\n",
        "           .filter(\n",
        "               ee.Filter.And(ee.Filter.eq('scenario', 'rcp85'),\n",
        "                             ee.Filter.date('2019-01-01', '2070-01-01'))))\n",
        "\n",
        "def calc_mean_temp(img):\n",
        "  return (img.select('tasmax_median')\n",
        "          .add(img.select('tasmin_median'))\n",
        "          .divide(ee.Image.constant(2.0))\n",
        "          .addBands(img.select('pr_median'))\n",
        "          .rename(['Temp-mean', 'Precip-rate'])\n",
        "          .copyProperties(img, img.propertyNames()))\n",
        "\n",
        "dcp_col = dcp_col.map(calc_mean_temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EJzPaDN9qIHc"
      },
      "source": [
        "#### Prepare DataFrame\n",
        "\n",
        "1. Set the scale argument of the `reduce_args` variable.\n",
        "2. Apply the `reduce_region` function to all images in the time series.\n",
        "3. Filter out features with null computed values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LqsXbDWnOxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare DataFrame.\n",
        "reduce_args['scale'] = 5000\n",
        "\n",
        "dcp_stat_fc = ee.FeatureCollection(dcp_col.map(reduce_region)).filter(\n",
        "    ee.Filter.notNull(dcp_col.first().bandNames()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-g71SoO8INL",
        "colab_type": "text"
      },
      "source": [
        "4. Transfer data from server to client.<br>\n",
        "*Note: if the process times out, you'll need to export/import the `dcp_stat_fc` feature collection as described in the **Optional export** section*.\n",
        "5. Convert Python dictionary to pandas DataFrame.\n",
        "6. Preview DataFrame and check data types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiA8t7HMBGIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare DataFrame.\n",
        "dcp_dict = fc_to_dict(dcp_stat_fc).getInfo()\n",
        "dcp_df = pd.DataFrame(dcp_dict)\n",
        "print(dcp_df, '\\n\\n')\n",
        "print(dcp_df.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AXlv_fqtr4tL"
      },
      "source": [
        "7. Add date attribute columns.\n",
        "8. Preview the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzpDZFfqr53G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare DataFrame.\n",
        "dcp_df = add_date_info(dcp_df)\n",
        "dcp_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpKd9JNs_BLR",
        "colab_type": "text"
      },
      "source": [
        "9. Convert precipitation rate to mm.\n",
        "10. Convert Kelvin to celsius.\n",
        "11. Add the model name as a column.\n",
        "12. Remove the 'Precip-rate' column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mHupoR_bIEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare DataFrame.\n",
        "dcp_df['Precip-mm'] = dcp_df['Precip-rate'] * 86400 * 30\n",
        "dcp_df['Temp-mean'] = dcp_df['Temp-mean'] - 273.15\n",
        "dcp_df['Model'] = 'NEX-DCP30'\n",
        "dcp_df = dcp_df.drop('Precip-rate', 1)\n",
        "dcp_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSxhiDhs0p4E",
        "colab_type": "text"
      },
      "source": [
        "### Past climate\n",
        "\n",
        "PRISM data are climate datasets for the conterminous United States. Grid cells are interpolated based on station data assimilated from many networks across the country. The datasets used here are monthly averages for precipitation and temperature. They provide a record of historical climate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nfQ3XFdb0mMz"
      },
      "source": [
        "#### Reduce collection and prepare DataFrame\n",
        "\n",
        "1. Import collection and filter by date.\n",
        "2. Reduce collection images by region and filter null computed value.\n",
        "3. Convert feature collection to dictionary and transfer client-side.<br>\n",
        "*Note: if the process times out, you'll need to export/import the `prism_stat_fc` feature collection as described in the **Optional export** section*.\n",
        "4. Convert dictionary to DataFrame.\n",
        "5. Preview DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI-SWEQp7Ylh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import, select, and filter PRISM precipitation and temperature.\n",
        "prism_col = (ee.ImageCollection('OREGONSTATE/PRISM/AN81m')\n",
        "             .select(['ppt', 'tmean'])\n",
        "             .filter(ee.Filter.date('1979-01-01', '2019-12-31')))\n",
        "\n",
        "prism_stat_fc = (ee.FeatureCollection(prism_col.map(reduce_region))\n",
        "                 .filter(ee.Filter.notNull(prism_col.first().bandNames())))\n",
        "\n",
        "prism_dict = fc_to_dict(prism_stat_fc).getInfo()\n",
        "prism_df = pd.DataFrame(prism_dict)\n",
        "\n",
        "print(prism_df, '\\n\\n')\n",
        "print(prism_df.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5b9CEeFA_gd",
        "colab_type": "text"
      },
      "source": [
        "6. Add date attribute columns.\n",
        "7. Add model name.\n",
        "8. Rename columns to be consistent with the NEX-DCP30 DataFrame.\n",
        "9. Preview the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iASS9-kQ1tEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare DataFrame.\n",
        "prism_df = add_date_info(prism_df)\n",
        "prism_df['Model'] = 'PRISM'\n",
        "prism_df = prism_df.rename(columns={'ppt': 'Precip-mm', 'tmean': 'Temp-mean'})\n",
        "prism_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2ub1LvfAos4",
        "colab_type": "text"
      },
      "source": [
        "### Combine DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTvmvSKTBJX8",
        "colab_type": "text"
      },
      "source": [
        "At this point PRISM and NEX-DCP30 DataFrames have the same columns, the same units, and are distinguished by unique entries in the 'Model' column. Use the `concat` function to concatenate these DataFrames into a single DataFrame for plotting together in the same chart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCZE_7S_9XJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Concatenate PRISM and DCP DataFrames.\n",
        "climate_df = pd.concat([prism_df, dcp_df], sort=True)\n",
        "climate_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KbnhXleCk5U",
        "colab_type": "text"
      },
      "source": [
        "### Charts\n",
        "\n",
        "Chart the past and future precipitation and temperature together to get a sense for where climate has been and where it is projected to go under RCP 8.5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDnkRaXWAzTX",
        "colab_type": "text"
      },
      "source": [
        "#### Precipitation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgtpmiuskuFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a line plot by reducing intra-annual observations to median.\n",
        "line = alt.Chart(climate_df).mark_line().encode(\n",
        "    x='Year:O',\n",
        "    y=alt.Y('median(Precip-mm):Q', title='Precipitation (mm/month)'),\n",
        "    color='Model')\n",
        "\n",
        "# Define an interquartile range plot to describe the intra-annual distribution.\n",
        "band = alt.Chart(climate_df).mark_errorband(extent='iqr').encode(\n",
        "    x='Year:O',\n",
        "    y=alt.Y('Precip-mm:Q', title='Precipitation (mm/month)'),\n",
        "    color='Model')\n",
        "\n",
        "# Combine the band and line plots; display it.\n",
        "(band + line).properties(width=600, height=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLdKpyzIA_7S",
        "colab_type": "text"
      },
      "source": [
        "#### Temperature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7T9n3y10MLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a line plot by reducing intra-annual observations to median.\n",
        "line = alt.Chart(climate_df).mark_line().encode(\n",
        "    x='Year:O', y='median(Temp-mean):Q', color='Model')\n",
        "\n",
        "# Define an interquartile range band to describe the intra-annual distribution.\n",
        "band = alt.Chart(climate_df).mark_errorband(extent='iqr').encode(\n",
        "    x='Year:O', y=alt.Y('Temp-mean:Q', title='Temperature (C)'), color='Model')\n",
        "\n",
        "# Combine the band and line plots; display it.\n",
        "(band + line).properties(width=600, height=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RuZV6h7BH22",
        "colab_type": "text"
      },
      "source": [
        "Future climate projections suggest that precipitation will decrease and temperature will increase for the selected point of interest. We can hypothesize, given the RCP 8.5 trajectory, that future conditions will more regularly resemble the 2012-2016 drought, which could lead to the same vegetation reduction response documented here and that more frequent drought events could lead to development of plant communities that are better adapted to low precipitation, high temperature conditions."
      ]
    }
  ]
}
